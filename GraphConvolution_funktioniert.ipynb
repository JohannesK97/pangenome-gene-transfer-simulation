{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "02383f1c-2d68-48e6-a07c-8dcc412d11a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uhewm\\AppData\\Local\\Temp\\ipykernel_21540\\2309232405.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edges = torch.tensor(graph_properties[1], dtype=torch.long)  # [2, num_edges]\n",
      "C:\\Users\\uhewm\\AppData\\Local\\Temp\\ipykernel_21540\\2309232405.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coords = torch.tensor(graph_properties[2].T)           # [2, num_nodes]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, DirGNNConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "import random, h5py, pickle, glob, os\n",
    "import networkx as nx\n",
    "from copy import deepcopy\n",
    "\n",
    "output_dir = r\"C:\\Users\\uhewm\\Desktop\\ProjectHGT\\simulation_chunks(4)\"\n",
    "all_files = sorted(glob.glob(os.path.join(output_dir, \"*.h5\")))\n",
    "\n",
    "graphs = []\n",
    "\n",
    "for file in random.sample(all_files, 10000):\n",
    "    with h5py.File(file, \"r\") as f:\n",
    "        grp = f[\"results\"]\n",
    "        graph_properties = pickle.loads(grp[\"graph_properties\"][()])\n",
    "        \n",
    "        nodes = torch.tensor(graph_properties[0])              # [num_nodes]\n",
    "        edges = torch.tensor(graph_properties[1], dtype=torch.long)  # [2, num_edges]\n",
    "        coords = torch.tensor(graph_properties[2].T)           # [2, num_nodes]\n",
    "        #edges_reversed = edges.flip(0)  # Tauscht Zeile 0 und 1\n",
    "\n",
    "        # Erstelle einen gerichteten Graphen\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        # Füge Knoten hinzu (optional mit Koordinaten als Attribut)\n",
    "        for i, node_id in enumerate(nodes.tolist()):\n",
    "            G.add_node(node_id, core_distance = coords[:, i].tolist()[0], allele_distance = coords[:, i].tolist()[1])\n",
    "        \n",
    "        # Füge Kanten hinzu\n",
    "        edge_list = edges.tolist()\n",
    "        for src, dst in zip(edge_list[0], edge_list[1]):\n",
    "            G.add_edge(src, dst)\n",
    "\n",
    "        H = deepcopy(G)\n",
    "            \n",
    "        for node in G.nodes():\n",
    "            children = list(G.predecessors(node))\n",
    "            if children:\n",
    "                core_sum = sum(G.nodes[child]['core_distance'] for child in children)\n",
    "                allele_sum = sum(G.nodes[child]['allele_distance'] for child in children)\n",
    "                H.nodes[node]['core_distance'] = G.nodes[node]['core_distance'] - core_sum\n",
    "                H.nodes[node]['allele_distance'] = G.nodes[node]['allele_distance'] - allele_sum\n",
    "            else:\n",
    "                H.nodes[node]['core_distance'] = G.nodes[node]['core_distance']\n",
    "                H.nodes[node]['allele_distance'] = G.nodes[node]['allele_distance']\n",
    "\n",
    "\n",
    "        node_features = []\n",
    "        for node in list(H.nodes):\n",
    "            core = H.nodes[node].get(\"core_distance\", 0.0)\n",
    "            allele = H.nodes[node].get(\"allele_distance\", 0.0)\n",
    "            node_features.append([core, allele])\n",
    "        coords = torch.tensor(node_features, dtype=torch.float32).T\n",
    "         \n",
    "        # Node features: nur die zwei Werte pro Knoten (coords)\n",
    "        x_node_features = coords.float().T  # Shape [num_nodes, 2]\n",
    "\n",
    "        # Labels\n",
    "        theta_gains = torch.tensor(\n",
    "            [1 if node in grp.attrs[\"parental_nodes_hgt_events_corrected\"] else 0 \n",
    "             for node in graph_properties[0]],\n",
    "            dtype=torch.long\n",
    "        )\n",
    "\n",
    "        # PyG-Graph erstellen\n",
    "        data = Data(\n",
    "            x=x_node_features,       # Node Features [num_nodes, 2]\n",
    "            edge_index=edges,        # Edge Index [2, num_edges]\n",
    "            y=theta_gains            # Labels [num_nodes]\n",
    "        )\n",
    "        graphs.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6e7da6c6-2c14-4bf8-8161-070f4b47a53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uhewm\\AppData\\Local\\Temp\\ipykernel_21540\\3794257270.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos_weight = torch.tensor((ratio**0.5), dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos Weight: 6.65\n",
      "Epoch 01 | Loss: 2.5878 | Acc: 0.921 | Prec: 0.197 | Rec: 0.929 | F1: 0.325\n",
      "Epoch 02 | Loss: 0.8332 | Acc: 0.940 | Prec: 0.246 | Rec: 0.942 | F1: 0.390\n",
      "Epoch 03 | Loss: 0.6011 | Acc: 0.957 | Prec: 0.313 | Rec: 0.931 | F1: 0.468\n",
      "Epoch 04 | Loss: 0.4696 | Acc: 0.967 | Prec: 0.377 | Rec: 0.919 | F1: 0.534\n",
      "Epoch 05 | Loss: 0.3687 | Acc: 0.974 | Prec: 0.440 | Rec: 0.920 | F1: 0.595\n",
      "Epoch 06 | Loss: 0.3162 | Acc: 0.978 | Prec: 0.481 | Rec: 0.931 | F1: 0.634\n",
      "Epoch 07 | Loss: 0.2549 | Acc: 0.983 | Prec: 0.555 | Rec: 0.913 | F1: 0.690\n",
      "Epoch 08 | Loss: 0.2091 | Acc: 0.985 | Prec: 0.578 | Rec: 0.904 | F1: 0.705\n",
      "Epoch 09 | Loss: 0.1818 | Acc: 0.986 | Prec: 0.601 | Rec: 0.898 | F1: 0.720\n",
      "Epoch 10 | Loss: 0.1575 | Acc: 0.985 | Prec: 0.595 | Rec: 0.898 | F1: 0.716\n",
      "Epoch 11 | Loss: 0.1459 | Acc: 0.987 | Prec: 0.623 | Rec: 0.885 | F1: 0.731\n",
      "Epoch 12 | Loss: 0.1324 | Acc: 0.986 | Prec: 0.611 | Rec: 0.891 | F1: 0.725\n",
      "Epoch 13 | Loss: 0.1234 | Acc: 0.987 | Prec: 0.636 | Rec: 0.855 | F1: 0.729\n",
      "Epoch 14 | Loss: 0.1198 | Acc: 0.986 | Prec: 0.608 | Rec: 0.887 | F1: 0.721\n",
      "Epoch 15 | Loss: 0.1131 | Acc: 0.985 | Prec: 0.599 | Rec: 0.880 | F1: 0.712\n",
      "Epoch 16 | Loss: 0.1098 | Acc: 0.987 | Prec: 0.622 | Rec: 0.873 | F1: 0.726\n",
      "Epoch 17 | Loss: 0.1087 | Acc: 0.986 | Prec: 0.613 | Rec: 0.872 | F1: 0.720\n",
      "Epoch 18 | Loss: 0.1076 | Acc: 0.986 | Prec: 0.611 | Rec: 0.880 | F1: 0.721\n",
      "Epoch 19 | Loss: 0.1050 | Acc: 0.987 | Prec: 0.641 | Rec: 0.860 | F1: 0.735\n",
      "Epoch 20 | Loss: 0.1055 | Acc: 0.987 | Prec: 0.632 | Rec: 0.872 | F1: 0.733\n",
      "Epoch 21 | Loss: 0.1035 | Acc: 0.986 | Prec: 0.615 | Rec: 0.888 | F1: 0.727\n",
      "Epoch 22 | Loss: 0.1026 | Acc: 0.987 | Prec: 0.625 | Rec: 0.875 | F1: 0.729\n",
      "Epoch 23 | Loss: 0.0993 | Acc: 0.986 | Prec: 0.607 | Rec: 0.899 | F1: 0.725\n",
      "Epoch 24 | Loss: 0.1020 | Acc: 0.986 | Prec: 0.605 | Rec: 0.907 | F1: 0.726\n",
      "Epoch 25 | Loss: 0.1027 | Acc: 0.987 | Prec: 0.620 | Rec: 0.881 | F1: 0.728\n",
      "Epoch 26 | Loss: 0.0998 | Acc: 0.987 | Prec: 0.635 | Rec: 0.885 | F1: 0.740\n",
      "Epoch 27 | Loss: 0.0991 | Acc: 0.988 | Prec: 0.648 | Rec: 0.885 | F1: 0.748\n",
      "Epoch 28 | Loss: 0.0992 | Acc: 0.988 | Prec: 0.654 | Rec: 0.876 | F1: 0.749\n",
      "Epoch 29 | Loss: 0.0973 | Acc: 0.987 | Prec: 0.634 | Rec: 0.896 | F1: 0.742\n",
      "Epoch 30 | Loss: 0.0987 | Acc: 0.988 | Prec: 0.653 | Rec: 0.882 | F1: 0.751\n",
      "Epoch 31 | Loss: 0.0979 | Acc: 0.988 | Prec: 0.647 | Rec: 0.882 | F1: 0.746\n",
      "Epoch 32 | Loss: 0.0984 | Acc: 0.988 | Prec: 0.664 | Rec: 0.854 | F1: 0.747\n",
      "Epoch 33 | Loss: 0.0974 | Acc: 0.988 | Prec: 0.642 | Rec: 0.882 | F1: 0.743\n",
      "Epoch 34 | Loss: 0.0982 | Acc: 0.988 | Prec: 0.652 | Rec: 0.873 | F1: 0.747\n",
      "Epoch 35 | Loss: 0.0966 | Acc: 0.986 | Prec: 0.615 | Rec: 0.905 | F1: 0.733\n",
      "Epoch 36 | Loss: 0.0968 | Acc: 0.988 | Prec: 0.645 | Rec: 0.882 | F1: 0.745\n",
      "Epoch 37 | Loss: 0.0956 | Acc: 0.987 | Prec: 0.634 | Rec: 0.891 | F1: 0.741\n",
      "Epoch 38 | Loss: 0.0978 | Acc: 0.988 | Prec: 0.649 | Rec: 0.861 | F1: 0.740\n",
      "Epoch 39 | Loss: 0.0965 | Acc: 0.987 | Prec: 0.624 | Rec: 0.910 | F1: 0.740\n",
      "Epoch 40 | Loss: 0.0961 | Acc: 0.988 | Prec: 0.659 | Rec: 0.842 | F1: 0.739\n",
      "Epoch 41 | Loss: 0.0971 | Acc: 0.987 | Prec: 0.639 | Rec: 0.871 | F1: 0.737\n",
      "Epoch 42 | Loss: 0.0948 | Acc: 0.988 | Prec: 0.644 | Rec: 0.889 | F1: 0.747\n",
      "Epoch 43 | Loss: 0.0972 | Acc: 0.989 | Prec: 0.698 | Rec: 0.833 | F1: 0.759\n",
      "Epoch 44 | Loss: 0.0953 | Acc: 0.988 | Prec: 0.645 | Rec: 0.896 | F1: 0.750\n",
      "Epoch 45 | Loss: 0.0951 | Acc: 0.989 | Prec: 0.691 | Rec: 0.842 | F1: 0.759\n",
      "Epoch 46 | Loss: 0.0951 | Acc: 0.988 | Prec: 0.661 | Rec: 0.873 | F1: 0.752\n",
      "Epoch 47 | Loss: 0.0951 | Acc: 0.988 | Prec: 0.643 | Rec: 0.893 | F1: 0.748\n",
      "Epoch 48 | Loss: 0.0951 | Acc: 0.986 | Prec: 0.614 | Rec: 0.908 | F1: 0.732\n",
      "Epoch 49 | Loss: 0.0940 | Acc: 0.988 | Prec: 0.666 | Rec: 0.878 | F1: 0.757\n",
      "Epoch 50 | Loss: 0.0962 | Acc: 0.987 | Prec: 0.629 | Rec: 0.892 | F1: 0.737\n"
     ]
    }
   ],
   "source": [
    "#### FUNKTIONIERT!\n",
    "\n",
    "# Train/Test Split\n",
    "random.shuffle(graphs)\n",
    "split_idx = int(0.8 * len(graphs))\n",
    "train_graphs = graphs[:split_idx]\n",
    "test_graphs = graphs[split_idx:]\n",
    "\n",
    "train_loader = DataLoader(train_graphs, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_graphs, batch_size=8)\n",
    "\n",
    "# === 2. Modell definieren ===\n",
    "class GCNClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, dropout=0.3):\n",
    "        super().__init__()\n",
    "        # Innerer conv wird an DirGNNConv übergeben\n",
    "        self.conv1 = DirGNNConv(GCNConv(in_channels, hidden_channels))\n",
    "        self.conv2 = DirGNNConv(GCNConv(hidden_channels, hidden_channels))\n",
    "        self.lin = nn.Linear(hidden_channels, 1)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        #x = self.conv2(x, edge_index)\n",
    "        #x = F.relu(x)\n",
    "        x = self.lin(x)\n",
    "        return x.view(-1)\n",
    "\n",
    "\n",
    "# === 3. Modell, Optimizer, Loss ===\n",
    "model = GCNClassifier(in_channels=2, hidden_channels=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Klassengewichte berechnen (gegen Ungleichgewicht)\n",
    "all_labels = torch.cat([g.y for g in train_graphs])\n",
    "ratio = (len(all_labels) - all_labels.sum()) / all_labels.sum()\n",
    "pos_weight = torch.tensor((ratio**0.5), dtype=torch.float)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "print(f\"Pos Weight: {pos_weight.item():.2f}\")\n",
    "\n",
    "# === 4. Training & Evaluation ===\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = criterion(out, batch.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_nodes = 0\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "\n",
    "    for batch in loader:\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        preds = torch.sigmoid(out) > 0.5\n",
    "        total_correct += (preds == batch.y.bool()).sum().item()\n",
    "        total_nodes += batch.y.size(0)\n",
    "\n",
    "        # Metriken für Klasse 1\n",
    "        tp += ((preds == 1) & (batch.y == 1)).sum().item()\n",
    "        fp += ((preds == 1) & (batch.y == 0)).sum().item()\n",
    "        fn += ((preds == 0) & (batch.y == 1)).sum().item()\n",
    "\n",
    "    acc = total_correct / total_nodes\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "# === 5. Training starten ===\n",
    "for epoch in range(1, 51):\n",
    "    loss = train()\n",
    "    acc, prec, rec, f1 = evaluate(test_loader)\n",
    "    print(f\"Epoch {epoch:02d} | Loss: {loss:.4f} | Acc: {acc:.3f} | Prec: {prec:.3f} | Rec: {rec:.3f} | F1: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e50017b-3bfc-4827-b4e1-8ae528fbd0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos Weight: 6.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uhewm\\AppData\\Local\\Temp\\ipykernel_21540\\1273750112.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos_weight = torch.tensor((ratio**0.5), dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | Loss: 0.6432 | Acc: 0.938 | Prec: 0.246 | Rec: 0.884 | F1: 0.385\n",
      "Epoch 02 | Loss: 0.2845 | Acc: 0.958 | Prec: 0.328 | Rec: 0.870 | F1: 0.476\n",
      "Epoch 03 | Loss: 0.1933 | Acc: 0.960 | Prec: 0.342 | Rec: 0.873 | F1: 0.492\n",
      "Epoch 04 | Loss: 0.1717 | Acc: 0.967 | Prec: 0.389 | Rec: 0.858 | F1: 0.535\n",
      "Epoch 05 | Loss: 0.1613 | Acc: 0.970 | Prec: 0.411 | Rec: 0.835 | F1: 0.551\n",
      "Epoch 06 | Loss: 0.1503 | Acc: 0.973 | Prec: 0.437 | Rec: 0.846 | F1: 0.576\n",
      "Epoch 07 | Loss: 0.1467 | Acc: 0.971 | Prec: 0.428 | Rec: 0.888 | F1: 0.577\n",
      "Epoch 08 | Loss: 0.1397 | Acc: 0.970 | Prec: 0.419 | Rec: 0.903 | F1: 0.572\n",
      "Epoch 09 | Loss: 0.1383 | Acc: 0.972 | Prec: 0.434 | Rec: 0.895 | F1: 0.584\n",
      "Epoch 10 | Loss: 0.1366 | Acc: 0.972 | Prec: 0.437 | Rec: 0.899 | F1: 0.589\n",
      "Epoch 11 | Loss: 0.1314 | Acc: 0.972 | Prec: 0.430 | Rec: 0.913 | F1: 0.585\n",
      "Epoch 12 | Loss: 0.1301 | Acc: 0.972 | Prec: 0.433 | Rec: 0.916 | F1: 0.588\n",
      "Epoch 13 | Loss: 0.1286 | Acc: 0.972 | Prec: 0.435 | Rec: 0.915 | F1: 0.590\n",
      "Epoch 14 | Loss: 0.1255 | Acc: 0.970 | Prec: 0.414 | Rec: 0.930 | F1: 0.573\n",
      "Epoch 15 | Loss: 0.1239 | Acc: 0.973 | Prec: 0.440 | Rec: 0.914 | F1: 0.594\n",
      "Epoch 16 | Loss: 0.1211 | Acc: 0.972 | Prec: 0.435 | Rec: 0.926 | F1: 0.592\n",
      "Epoch 17 | Loss: 0.1205 | Acc: 0.973 | Prec: 0.444 | Rec: 0.916 | F1: 0.598\n",
      "Epoch 18 | Loss: 0.1198 | Acc: 0.973 | Prec: 0.443 | Rec: 0.927 | F1: 0.599\n",
      "Epoch 19 | Loss: 0.1197 | Acc: 0.972 | Prec: 0.436 | Rec: 0.930 | F1: 0.593\n",
      "Epoch 20 | Loss: 0.1162 | Acc: 0.971 | Prec: 0.426 | Rec: 0.936 | F1: 0.586\n",
      "Epoch 21 | Loss: 0.1186 | Acc: 0.972 | Prec: 0.439 | Rec: 0.930 | F1: 0.596\n",
      "Epoch 22 | Loss: 0.1177 | Acc: 0.971 | Prec: 0.430 | Rec: 0.942 | F1: 0.591\n",
      "Epoch 23 | Loss: 0.1152 | Acc: 0.970 | Prec: 0.421 | Rec: 0.945 | F1: 0.583\n",
      "Epoch 24 | Loss: 0.1143 | Acc: 0.972 | Prec: 0.433 | Rec: 0.938 | F1: 0.592\n",
      "Epoch 25 | Loss: 0.1099 | Acc: 0.972 | Prec: 0.436 | Rec: 0.942 | F1: 0.596\n",
      "Epoch 26 | Loss: 0.1122 | Acc: 0.973 | Prec: 0.444 | Rec: 0.934 | F1: 0.602\n",
      "Epoch 27 | Loss: 0.1100 | Acc: 0.973 | Prec: 0.444 | Rec: 0.936 | F1: 0.602\n",
      "Epoch 28 | Loss: 0.1090 | Acc: 0.973 | Prec: 0.448 | Rec: 0.942 | F1: 0.607\n",
      "Epoch 29 | Loss: 0.1116 | Acc: 0.971 | Prec: 0.427 | Rec: 0.950 | F1: 0.589\n",
      "Epoch 30 | Loss: 0.1087 | Acc: 0.972 | Prec: 0.437 | Rec: 0.941 | F1: 0.597\n",
      "Epoch 31 | Loss: 0.1058 | Acc: 0.972 | Prec: 0.441 | Rec: 0.942 | F1: 0.601\n",
      "Epoch 32 | Loss: 0.1067 | Acc: 0.972 | Prec: 0.432 | Rec: 0.944 | F1: 0.593\n",
      "Epoch 33 | Loss: 0.1057 | Acc: 0.973 | Prec: 0.443 | Rec: 0.935 | F1: 0.601\n",
      "Epoch 34 | Loss: 0.1043 | Acc: 0.973 | Prec: 0.442 | Rec: 0.939 | F1: 0.601\n",
      "Epoch 35 | Loss: 0.1052 | Acc: 0.973 | Prec: 0.442 | Rec: 0.938 | F1: 0.601\n",
      "Epoch 36 | Loss: 0.1049 | Acc: 0.973 | Prec: 0.449 | Rec: 0.941 | F1: 0.608\n",
      "Epoch 37 | Loss: 0.1049 | Acc: 0.974 | Prec: 0.458 | Rec: 0.927 | F1: 0.613\n",
      "Epoch 38 | Loss: 0.1060 | Acc: 0.972 | Prec: 0.440 | Rec: 0.939 | F1: 0.599\n",
      "Epoch 39 | Loss: 0.1047 | Acc: 0.974 | Prec: 0.456 | Rec: 0.921 | F1: 0.610\n",
      "Epoch 40 | Loss: 0.1052 | Acc: 0.971 | Prec: 0.425 | Rec: 0.944 | F1: 0.586\n",
      "Epoch 41 | Loss: 0.1027 | Acc: 0.972 | Prec: 0.441 | Rec: 0.943 | F1: 0.601\n",
      "Epoch 42 | Loss: 0.1012 | Acc: 0.973 | Prec: 0.446 | Rec: 0.941 | F1: 0.605\n",
      "Epoch 43 | Loss: 0.1026 | Acc: 0.973 | Prec: 0.442 | Rec: 0.944 | F1: 0.602\n",
      "Epoch 44 | Loss: 0.1019 | Acc: 0.972 | Prec: 0.438 | Rec: 0.947 | F1: 0.599\n",
      "Epoch 45 | Loss: 0.1014 | Acc: 0.972 | Prec: 0.439 | Rec: 0.945 | F1: 0.600\n",
      "Epoch 46 | Loss: 0.1019 | Acc: 0.973 | Prec: 0.449 | Rec: 0.942 | F1: 0.608\n",
      "Epoch 47 | Loss: 0.0999 | Acc: 0.972 | Prec: 0.437 | Rec: 0.944 | F1: 0.598\n",
      "Epoch 48 | Loss: 0.0989 | Acc: 0.973 | Prec: 0.449 | Rec: 0.941 | F1: 0.608\n",
      "Epoch 49 | Loss: 0.1001 | Acc: 0.973 | Prec: 0.448 | Rec: 0.938 | F1: 0.607\n",
      "Epoch 50 | Loss: 0.1001 | Acc: 0.974 | Prec: 0.452 | Rec: 0.930 | F1: 0.608\n"
     ]
    }
   ],
   "source": [
    "# Train/Test Split\n",
    "random.shuffle(graphs)\n",
    "split_idx = int(0.8 * len(graphs))\n",
    "train_graphs = graphs[:split_idx]\n",
    "test_graphs = graphs[split_idx:]\n",
    "\n",
    "train_loader = DataLoader(train_graphs, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_graphs, batch_size=8)\n",
    "\n",
    "# === 2. Modell definieren ===\n",
    "class GCNClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = DirGNNConv(GCNConv(in_channels, hidden_channels))\n",
    "        self.conv2 = DirGNNConv(GCNConv(hidden_channels, hidden_channels))\n",
    "        self.conv3 = DirGNNConv(GCNConv(hidden_channels, hidden_channels))\n",
    "        self.conv4 = DirGNNConv(GCNConv(hidden_channels, hidden_channels))\n",
    "        self.lin = nn.Linear(hidden_channels, 1)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.lin(x)\n",
    "        return x.view(-1)\n",
    "\n",
    "# === 3. Modell, Optimizer, Loss ===\n",
    "model = GCNClassifier(in_channels=2, hidden_channels=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Klassengewichte berechnen (gegen Ungleichgewicht)\n",
    "all_labels = torch.cat([g.y for g in train_graphs])\n",
    "ratio = (len(all_labels) - all_labels.sum()) / all_labels.sum()\n",
    "pos_weight = torch.tensor((ratio**0.5), dtype=torch.float)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "print(f\"Pos Weight: {pos_weight.item():.2f}\")\n",
    "\n",
    "# === 4. Training & Evaluation ===\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = criterion(out, batch.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_nodes = 0\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "\n",
    "    for batch in loader:\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        preds = torch.sigmoid(out) > 0.5\n",
    "        total_correct += (preds == batch.y.bool()).sum().item()\n",
    "        total_nodes += batch.y.size(0)\n",
    "\n",
    "        # Metriken für Klasse 1\n",
    "        tp += ((preds == 1) & (batch.y == 1)).sum().item()\n",
    "        fp += ((preds == 1) & (batch.y == 0)).sum().item()\n",
    "        fn += ((preds == 0) & (batch.y == 1)).sum().item()\n",
    "\n",
    "    acc = total_correct / total_nodes\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "# === 5. Training starten ===\n",
    "for epoch in range(1, 51):\n",
    "    loss = train()\n",
    "    acc, prec, rec, f1 = evaluate(test_loader)\n",
    "    print(f\"Epoch {epoch:02d} | Loss: {loss:.4f} | Acc: {acc:.3f} | Prec: {prec:.3f} | Rec: {rec:.3f} | F1: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ab43e559-f703-41e9-bb71-d86a18c967cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uhewm\\AppData\\Local\\Temp\\ipykernel_21540\\1012065659.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos_weight = torch.tensor((ratio**0.5), dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos Weight: 6.65\n",
      "Epoch 01 | Loss: 0.6265 | Acc: 0.977 | Prec: 0.468 | Rec: 0.845 | F1: 0.603\n",
      "Epoch 02 | Loss: 0.2420 | Acc: 0.966 | Prec: 0.367 | Rec: 0.936 | F1: 0.527\n",
      "Epoch 03 | Loss: 0.1602 | Acc: 0.982 | Prec: 0.530 | Rec: 0.867 | F1: 0.658\n",
      "Epoch 04 | Loss: 0.1433 | Acc: 0.978 | Prec: 0.483 | Rec: 0.920 | F1: 0.633\n",
      "Epoch 05 | Loss: 0.1350 | Acc: 0.982 | Prec: 0.532 | Rec: 0.889 | F1: 0.666\n",
      "Epoch 06 | Loss: 0.1287 | Acc: 0.983 | Prec: 0.552 | Rec: 0.907 | F1: 0.686\n",
      "Epoch 07 | Loss: 0.1222 | Acc: 0.983 | Prec: 0.548 | Rec: 0.910 | F1: 0.684\n",
      "Epoch 08 | Loss: 0.1185 | Acc: 0.984 | Prec: 0.560 | Rec: 0.909 | F1: 0.693\n",
      "Epoch 09 | Loss: 0.1180 | Acc: 0.984 | Prec: 0.567 | Rec: 0.912 | F1: 0.699\n",
      "Epoch 10 | Loss: 0.1102 | Acc: 0.986 | Prec: 0.605 | Rec: 0.889 | F1: 0.720\n",
      "Epoch 11 | Loss: 0.1104 | Acc: 0.986 | Prec: 0.601 | Rec: 0.909 | F1: 0.723\n",
      "Epoch 12 | Loss: 0.1072 | Acc: 0.986 | Prec: 0.601 | Rec: 0.899 | F1: 0.720\n",
      "Epoch 13 | Loss: 0.1078 | Acc: 0.985 | Prec: 0.595 | Rec: 0.902 | F1: 0.717\n",
      "Epoch 14 | Loss: 0.1037 | Acc: 0.986 | Prec: 0.599 | Rec: 0.891 | F1: 0.716\n",
      "Epoch 15 | Loss: 0.1022 | Acc: 0.986 | Prec: 0.600 | Rec: 0.905 | F1: 0.721\n",
      "Epoch 16 | Loss: 0.1008 | Acc: 0.986 | Prec: 0.600 | Rec: 0.914 | F1: 0.725\n",
      "Epoch 17 | Loss: 0.1004 | Acc: 0.987 | Prec: 0.632 | Rec: 0.899 | F1: 0.742\n",
      "Epoch 18 | Loss: 0.0988 | Acc: 0.985 | Prec: 0.585 | Rec: 0.918 | F1: 0.714\n",
      "Epoch 19 | Loss: 0.0973 | Acc: 0.987 | Prec: 0.631 | Rec: 0.894 | F1: 0.740\n",
      "Epoch 20 | Loss: 0.0974 | Acc: 0.986 | Prec: 0.609 | Rec: 0.912 | F1: 0.730\n",
      "Epoch 21 | Loss: 0.0938 | Acc: 0.987 | Prec: 0.625 | Rec: 0.899 | F1: 0.738\n",
      "Epoch 22 | Loss: 0.0930 | Acc: 0.987 | Prec: 0.630 | Rec: 0.903 | F1: 0.742\n",
      "Epoch 23 | Loss: 0.0921 | Acc: 0.986 | Prec: 0.594 | Rec: 0.924 | F1: 0.723\n",
      "Epoch 24 | Loss: 0.0928 | Acc: 0.985 | Prec: 0.591 | Rec: 0.932 | F1: 0.723\n",
      "Epoch 25 | Loss: 0.0909 | Acc: 0.986 | Prec: 0.600 | Rec: 0.929 | F1: 0.729\n",
      "Epoch 26 | Loss: 0.0899 | Acc: 0.987 | Prec: 0.620 | Rec: 0.914 | F1: 0.739\n",
      "Epoch 27 | Loss: 0.0882 | Acc: 0.987 | Prec: 0.616 | Rec: 0.923 | F1: 0.738\n",
      "Epoch 28 | Loss: 0.0875 | Acc: 0.986 | Prec: 0.605 | Rec: 0.916 | F1: 0.729\n",
      "Epoch 29 | Loss: 0.0872 | Acc: 0.987 | Prec: 0.617 | Rec: 0.910 | F1: 0.735\n",
      "Epoch 30 | Loss: 0.0860 | Acc: 0.985 | Prec: 0.590 | Rec: 0.926 | F1: 0.720\n",
      "Epoch 31 | Loss: 0.0840 | Acc: 0.985 | Prec: 0.579 | Rec: 0.935 | F1: 0.715\n",
      "Epoch 32 | Loss: 0.0892 | Acc: 0.986 | Prec: 0.600 | Rec: 0.928 | F1: 0.728\n",
      "Epoch 33 | Loss: 0.0853 | Acc: 0.985 | Prec: 0.592 | Rec: 0.932 | F1: 0.724\n",
      "Epoch 34 | Loss: 0.0843 | Acc: 0.986 | Prec: 0.599 | Rec: 0.926 | F1: 0.727\n",
      "Epoch 35 | Loss: 0.0867 | Acc: 0.986 | Prec: 0.595 | Rec: 0.931 | F1: 0.726\n",
      "Epoch 36 | Loss: 0.0841 | Acc: 0.985 | Prec: 0.591 | Rec: 0.932 | F1: 0.724\n",
      "Epoch 37 | Loss: 0.0824 | Acc: 0.985 | Prec: 0.588 | Rec: 0.935 | F1: 0.722\n",
      "Epoch 38 | Loss: 0.0825 | Acc: 0.985 | Prec: 0.588 | Rec: 0.934 | F1: 0.721\n",
      "Epoch 39 | Loss: 0.0835 | Acc: 0.987 | Prec: 0.612 | Rec: 0.929 | F1: 0.738\n",
      "Epoch 40 | Loss: 0.0810 | Acc: 0.987 | Prec: 0.615 | Rec: 0.926 | F1: 0.739\n",
      "Epoch 41 | Loss: 0.0812 | Acc: 0.985 | Prec: 0.592 | Rec: 0.929 | F1: 0.723\n",
      "Epoch 42 | Loss: 0.0822 | Acc: 0.986 | Prec: 0.597 | Rec: 0.930 | F1: 0.728\n",
      "Epoch 43 | Loss: 0.0812 | Acc: 0.986 | Prec: 0.611 | Rec: 0.928 | F1: 0.737\n",
      "Epoch 44 | Loss: 0.0804 | Acc: 0.987 | Prec: 0.617 | Rec: 0.920 | F1: 0.739\n",
      "Epoch 45 | Loss: 0.0785 | Acc: 0.986 | Prec: 0.593 | Rec: 0.932 | F1: 0.725\n",
      "Epoch 46 | Loss: 0.0799 | Acc: 0.986 | Prec: 0.609 | Rec: 0.930 | F1: 0.736\n",
      "Epoch 47 | Loss: 0.0786 | Acc: 0.987 | Prec: 0.613 | Rec: 0.930 | F1: 0.739\n",
      "Epoch 48 | Loss: 0.0807 | Acc: 0.987 | Prec: 0.619 | Rec: 0.924 | F1: 0.741\n",
      "Epoch 49 | Loss: 0.0792 | Acc: 0.987 | Prec: 0.618 | Rec: 0.924 | F1: 0.741\n",
      "Epoch 50 | Loss: 0.0783 | Acc: 0.986 | Prec: 0.609 | Rec: 0.929 | F1: 0.736\n",
      "\n",
      "Bester Threshold: 0.760 mit F1-Score: 0.789\n",
      "Evaluation mit bestem Threshold:\n",
      "Acc: 0.991 | Prec: 0.780 | Rec: 0.797 | F1: 0.789\n",
      "\n",
      "Ein paar Beispielvorhersagen auf Trainingsdaten mit bestem Threshold:\n",
      "Sample 1:\n",
      "  True label:      0\n",
      "  Predicted prob:  0.0001\n",
      "  Predicted label: 0\n",
      "Sample 2:\n",
      "  True label:      0\n",
      "  Predicted prob:  0.0000\n",
      "  Predicted label: 0\n",
      "Sample 3:\n",
      "  True label:      0\n",
      "  Predicted prob:  0.0003\n",
      "  Predicted label: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# === 2. Modell definieren ===\n",
    "class GCNClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = DirGNNConv(GCNConv(in_channels, hidden_channels))\n",
    "        self.conv2 = DirGNNConv(GCNConv(hidden_channels, hidden_channels))\n",
    "        self.conv3 = DirGNNConv(GCNConv(hidden_channels, hidden_channels))\n",
    "        self.conv4 = DirGNNConv(GCNConv(hidden_channels, hidden_channels))\n",
    "        self.lin = nn.Linear(hidden_channels, 1)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        edge_index = edge_index[[1, 0], :]\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.lin(x)\n",
    "        return x.view(-1)\n",
    "\n",
    "\n",
    "# === 3. Modell, Optimizer, Loss ===\n",
    "model = GCNClassifier(in_channels=2, hidden_channels=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Klassengewichte berechnen (gegen Ungleichgewicht)\n",
    "all_labels = torch.cat([g.y for g in train_graphs])\n",
    "ratio = (len(all_labels) - all_labels.sum()) / all_labels.sum()\n",
    "pos_weight = torch.tensor((ratio**0.5), dtype=torch.float)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "print(f\"Pos Weight: {pos_weight.item():.2f}\")\n",
    "\n",
    "# === 4. Training & Evaluation ===\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = criterion(out, batch.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_nodes = 0\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "\n",
    "    for batch in loader:\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        preds = torch.sigmoid(out) > threshold\n",
    "        total_correct += (preds == batch.y.bool()).sum().item()\n",
    "        total_nodes += batch.y.size(0)\n",
    "\n",
    "        tp += ((preds == 1) & (batch.y == 1)).sum().item()\n",
    "        fp += ((preds == 1) & (batch.y == 0)).sum().item()\n",
    "        fn += ((preds == 0) & (batch.y == 1)).sum().item()\n",
    "\n",
    "    acc = total_correct / total_nodes\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "@torch.no_grad()\n",
    "def find_best_threshold(loader, thresholds=np.linspace(0, 1, 101)):\n",
    "    model.eval()\n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    # Alle Outputs und Labels sammeln, damit man nicht für jeden Threshold neu durch die Daten geht\n",
    "    all_outs = []\n",
    "    all_labels = []\n",
    "    for batch in loader:\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        all_outs.append(torch.sigmoid(out))\n",
    "        all_labels.append(batch.y)\n",
    "    all_outs = torch.cat(all_outs)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        preds = all_outs > threshold\n",
    "        tp = ((preds == 1) & (all_labels == 1)).sum().item()\n",
    "        fp = ((preds == 1) & (all_labels == 0)).sum().item()\n",
    "        fn = ((preds == 0) & (all_labels == 1)).sum().item()\n",
    "\n",
    "        precision = tp / (tp + fp + 1e-8)\n",
    "        recall = tp / (tp + fn + 1e-8)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "@torch.no_grad()\n",
    "def show_some_predictions(loader, n_samples=3, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in loader:\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        probs = torch.sigmoid(out)\n",
    "        preds = (probs > threshold).long()\n",
    "        all_probs.append(probs)\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(batch.y)\n",
    "\n",
    "    all_probs = torch.cat(all_probs)\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    total_samples = len(all_labels)\n",
    "    indices = random.sample(range(total_samples), k=min(n_samples, total_samples))\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        print(f\"Sample {i + 1}:\")\n",
    "        print(f\"  True label:      {all_labels[idx].item()}\")\n",
    "        print(f\"  Predicted prob:  {all_probs[idx].item():.4f}\")\n",
    "        print(f\"  Predicted label: {all_preds[idx].item()}\")\n",
    "\n",
    "# === 5. Training starten ===\n",
    "for epoch in range(1, 51):\n",
    "    loss = train()\n",
    "    acc, prec, rec, f1 = evaluate(test_loader, threshold=0.5)\n",
    "    print(f\"Epoch {epoch:02d} | Loss: {loss:.4f} | Acc: {acc:.3f} | Prec: {prec:.3f} | Rec: {rec:.3f} | F1: {f1:.3f}\")\n",
    "\n",
    "# Nach Training besten Threshold bestimmen\n",
    "best_threshold, best_f1 = find_best_threshold(test_loader)\n",
    "print(f\"\\nBester Threshold: {best_threshold:.3f} mit F1-Score: {best_f1:.3f}\")\n",
    "\n",
    "# Evaluation mit bestem Threshold\n",
    "acc, prec, rec, f1 = evaluate(test_loader, threshold=best_threshold)\n",
    "print(f\"Evaluation mit bestem Threshold:\")\n",
    "print(f\"Acc: {acc:.3f} | Prec: {prec:.3f} | Rec: {rec:.3f} | F1: {f1:.3f}\")\n",
    "\n",
    "print(\"\\nEin paar Beispielvorhersagen auf Trainingsdaten mit bestem Threshold:\")\n",
    "show_some_predictions(train_loader, n_samples=3, threshold=best_threshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "194888e8-2677-4dcc-8aa4-2db342a94655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ein paar Beispielvorhersagen auf Trainingsdaten mit bestem Threshold:\n",
      "Graph Sample 1 (Index 18):\n",
      "Node-wise True Labels:      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "Node-wise Predicted Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "--------------------------------------------------\n",
      "Graph Sample 2 (Index 48):\n",
      "Node-wise True Labels:      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Node-wise Predicted Labels: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def show_some_graph_predictions(loader, n_samples=3, threshold=0.5):\n",
    "    model.eval()\n",
    "    graphs = []\n",
    "\n",
    "    # Alle Graphen und deren Vorhersagen sammeln\n",
    "    for batch in loader:\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        probs = torch.sigmoid(out)\n",
    "        preds = (probs > threshold).long()\n",
    "        graphs.append({\n",
    "            \"probs\": probs,\n",
    "            \"preds\": preds,\n",
    "            \"labels\": batch.y\n",
    "        })\n",
    "\n",
    "    total_graphs = len(graphs)\n",
    "    indices = random.sample(range(total_graphs), k=min(n_samples, total_graphs))\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        graph = graphs[idx]\n",
    "        print(f\"Graph Sample {i + 1} (Index {idx}):\")\n",
    "        print(\"Node-wise True Labels:     \", graph[\"labels\"].tolist())\n",
    "        #print(\"Node-wise Predicted Probs: \", [f\"{p:.4f}\" for p in graph[\"probs\"].tolist()])\n",
    "        print(\"Node-wise Predicted Labels:\", graph[\"preds\"].tolist())\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "print(\"\\nEin paar Beispielvorhersagen auf Trainingsdaten mit bestem Threshold:\")\n",
    "show_some_graph_predictions(train_loader, n_samples=2, threshold=best_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0001f72b-0b17-4c79-b932-a31b90d531c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "553.5075376884422"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "110148/199"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangenome-gene-transfer-simulation",
   "language": "python",
   "name": "pangenome-gene-transfer-simulation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
