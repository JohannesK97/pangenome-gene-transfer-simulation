{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22f95433-12c3-4bdf-9df9-0ef687c71134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uhewm\\AppData\\Local\\Temp\\ipykernel_308\\2556439182.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edges = torch.tensor(graph_properties[1])          # Shape [2, 198]\n",
      "C:\\Users\\uhewm\\AppData\\Local\\Temp\\ipykernel_308\\2556439182.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coords = torch.tensor(graph_properties[2].T)       # Shape [2, 199]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "\n",
    "output_dir = r\"C:\\Users\\uhewm\\Desktop\\ProjectHGT\\simulation_chunks\"\n",
    "all_files = sorted(glob.glob(os.path.join(output_dir, \"*.h5\")))\n",
    "\n",
    "x = []\n",
    "theta = []\n",
    "\n",
    "for file in random.sample(all_files, 100):\n",
    "#for file in all_files:\n",
    "    with h5py.File(file, \"r\") as f:\n",
    "        grp = f[\"results\"]\n",
    "        \n",
    "        graph_properties = pickle.loads(grp[\"graph_properties\"][()])\n",
    "        # Tensors erstellen\n",
    "        nodes = torch.tensor(graph_properties[0])          # Shape [199]\n",
    "        edges = torch.tensor(graph_properties[1])          # Shape [2, 198]\n",
    "        coords = torch.tensor(graph_properties[2].T)       # Shape [2, 199]\n",
    "\n",
    "        #coords[0] = coords[0] / max(coords[0]) * max(coords[1])\n",
    "        \n",
    "        # Falls nötig, auffüllen\n",
    "        if edges.shape[1] < nodes.shape[0]:\n",
    "            padding = torch.full((2, nodes.shape[0] - edges.shape[1]), -1, dtype=edges.dtype)\n",
    "            edges = torch.cat([edges, padding], dim=1)     # Jetzt Shape [2, 199]\n",
    "        \n",
    "        # Alles zu einem Tensor kombinieren: z. B. pro Knoten eine Zeile mit:\n",
    "        # [node_id, coord_x, coord_y, edge_from, edge_to]\n",
    "        combined_x = torch.stack([nodes, coords[0], coords[1], edges[0], edges[1]], dim=1)  # Shape [199, 5]\n",
    "        x.append(combined_x)\n",
    "\n",
    "        theta_gains = torch.tensor([1 if node in grp.attrs[\"parental_nodes_hgt_events_corrected\"] else 0 for node in graph_properties[0]])\n",
    "        theta_losses = torch.tensor([1 if node in grp.attrs[\"children_gene_nodes_loss_events\"] else 0 for node in graph_properties[0]])\n",
    "        #combined_theta = torch.stack([theta_gains, theta_losses], dim = 1)\n",
    "        combined_theta = torch.stack([theta_gains], dim = 1)\n",
    "        theta.append(combined_theta)\n",
    "\n",
    "# Für SNPE vorbereiten\n",
    "x_all = torch.stack(x).float()\n",
    "theta_all = torch.stack(theta).float()\n",
    "\n",
    "x_all_flat = x_all.view(x_all.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb0ece9e-bbae-48a4-be82-2c1b3220e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sbi import utils\n",
    "from sbi.inference import SNPE\n",
    "from sbi.neural_nets import posterior_nn\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCNEmbedding(torch.nn.Module):\n",
    "    def __init__(self, in_channels=2, hidden_channels=32, gcn_out_channels=32, final_out_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_nodes = 199         # feste Anzahl Knoten (z.B.)\n",
    "        self.features_per_node = 5   # wie bei combined tensor\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, gcn_out_channels)\n",
    "        \n",
    "        self.fc_out = torch.nn.Linear(gcn_out_channels, final_out_channels)  # 2 binäre Outputs pro Knoten\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)  # Batchgröße\n",
    "        outputs = []\n",
    "\n",
    "        for i in range(B):\n",
    "            sample_flat = x[i]  # [num_nodes * features_per_node]\n",
    "            sample = sample_flat.view(self.num_nodes, self.features_per_node)\n",
    "            \n",
    "            nodes = sample[:, 0].long()\n",
    "            node_id_to_idx = {nid.item(): idx for idx, nid in enumerate(nodes)}\n",
    "            \n",
    "            raw_edges = sample[:, 3:5].T.long()\n",
    "            #edges_mapped = torch.tensor([\n",
    "            #    [node_id_to_idx.get(n.item(), -1) for n in raw_edges[0]],\n",
    "            #    [node_id_to_idx.get(n.item(), -1) for n in raw_edges[1]],\n",
    "            #])\n",
    "\n",
    "            mask = (raw_edges[0] >= 0) & (raw_edges[1] >= 0)\n",
    "            edge_index = raw_edges[:, mask]\n",
    "\n",
    "            # Optional: ungerichtet machen\n",
    "            #edge_index = torch.cat([edge_index.flip(0)], dim=1)\n",
    "            edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)\n",
    "\n",
    "            node_features = sample[:, 1:3]  # z. B. Koordinaten\n",
    "\n",
    "            z = F.relu(self.conv1(node_features, edge_index))\n",
    "            #z = F.relu(self.conv2(z, edge_index))  # Optional: weitere Aktivierung\n",
    "\n",
    "            logits = self.fc_out(z)               # [num_nodes, 2]\n",
    "            probs = torch.sigmoid(logits)         # Binäre Wahrscheinlichkeiten für beide Labels\n",
    "            \n",
    "            outputs.append(probs)\n",
    "\n",
    "        return torch.stack(outputs, dim=0)  # [B, num_nodes, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "615da4f2-b6ea-4701-9ae4-ff5d53621670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_weight used for BCEWithLogitsLoss: tensor([7.3988])\n",
      "Epoch 01, Loss: 0.9767, Acc: 0.042, Prec: tensor([0.0108]), Recall: tensor([0.5798])\n",
      "Epoch 02, Loss: 0.9872, Acc: 0.042, Prec: tensor([0.0106]), Recall: tensor([0.5686])\n",
      "Epoch 03, Loss: 0.9777, Acc: 0.043, Prec: tensor([0.0105]), Recall: tensor([0.5602])\n",
      "Epoch 04, Loss: 0.9789, Acc: 0.044, Prec: tensor([0.0105]), Recall: tensor([0.5602])\n",
      "Epoch 05, Loss: 0.9738, Acc: 0.044, Prec: tensor([0.0104]), Recall: tensor([0.5574])\n",
      "Epoch 06, Loss: 0.9797, Acc: 0.045, Prec: tensor([0.0102]), Recall: tensor([0.5434])\n",
      "Epoch 07, Loss: 0.9893, Acc: 0.045, Prec: tensor([0.0101]), Recall: tensor([0.5406])\n",
      "Epoch 08, Loss: 0.9941, Acc: 0.046, Prec: tensor([0.0099]), Recall: tensor([0.5294])\n",
      "Epoch 09, Loss: 0.9945, Acc: 0.046, Prec: tensor([0.0098]), Recall: tensor([0.5238])\n",
      "Epoch 10, Loss: 0.9800, Acc: 0.047, Prec: tensor([0.0098]), Recall: tensor([0.5210])\n",
      "Epoch 11, Loss: 0.9733, Acc: 0.047, Prec: tensor([0.0098]), Recall: tensor([0.5210])\n",
      "Epoch 12, Loss: 0.9766, Acc: 0.048, Prec: tensor([0.0097]), Recall: tensor([0.5126])\n",
      "Epoch 13, Loss: 0.9814, Acc: 0.048, Prec: tensor([0.0095]), Recall: tensor([0.5042])\n",
      "Epoch 14, Loss: 0.9749, Acc: 0.048, Prec: tensor([0.0094]), Recall: tensor([0.4986])\n",
      "Epoch 15, Loss: 0.9745, Acc: 0.049, Prec: tensor([0.0094]), Recall: tensor([0.4958])\n",
      "Epoch 16, Loss: 0.9896, Acc: 0.050, Prec: tensor([0.0093]), Recall: tensor([0.4902])\n",
      "Epoch 17, Loss: 0.9659, Acc: 0.050, Prec: tensor([0.0090]), Recall: tensor([0.4790])\n",
      "Epoch 18, Loss: 0.9836, Acc: 0.050, Prec: tensor([0.0089]), Recall: tensor([0.4734])\n",
      "Epoch 19, Loss: 0.9733, Acc: 0.050, Prec: tensor([0.0088]), Recall: tensor([0.4650])\n",
      "Epoch 20, Loss: 0.9655, Acc: 0.051, Prec: tensor([0.0088]), Recall: tensor([0.4650])\n",
      "Epoch 21, Loss: 0.9793, Acc: 0.052, Prec: tensor([0.0088]), Recall: tensor([0.4650])\n",
      "Epoch 22, Loss: 0.9712, Acc: 0.052, Prec: tensor([0.0086]), Recall: tensor([0.4538])\n",
      "Epoch 23, Loss: 0.9807, Acc: 0.052, Prec: tensor([0.0084]), Recall: tensor([0.4454])\n",
      "Epoch 24, Loss: 0.9677, Acc: 0.053, Prec: tensor([0.0085]), Recall: tensor([0.4454])\n",
      "Epoch 25, Loss: 0.9889, Acc: 0.053, Prec: tensor([0.0085]), Recall: tensor([0.4454])\n",
      "Epoch 26, Loss: 0.9674, Acc: 0.053, Prec: tensor([0.0083]), Recall: tensor([0.4370])\n",
      "Epoch 27, Loss: 0.9762, Acc: 0.053, Prec: tensor([0.0080]), Recall: tensor([0.4230])\n",
      "Epoch 28, Loss: 0.9749, Acc: 0.054, Prec: tensor([0.0080]), Recall: tensor([0.4230])\n",
      "Epoch 29, Loss: 0.9692, Acc: 0.054, Prec: tensor([0.0080]), Recall: tensor([0.4202])\n",
      "Epoch 30, Loss: 0.9656, Acc: 0.054, Prec: tensor([0.0080]), Recall: tensor([0.4202])\n",
      "Epoch 31, Loss: 0.9728, Acc: 0.054, Prec: tensor([0.0078]), Recall: tensor([0.4118])\n",
      "Epoch 32, Loss: 0.9681, Acc: 0.055, Prec: tensor([0.0078]), Recall: tensor([0.4118])\n",
      "Epoch 33, Loss: 0.9617, Acc: 0.055, Prec: tensor([0.0078]), Recall: tensor([0.4090])\n",
      "Epoch 34, Loss: 0.9661, Acc: 0.056, Prec: tensor([0.0077]), Recall: tensor([0.4062])\n",
      "Epoch 35, Loss: 0.9716, Acc: 0.056, Prec: tensor([0.0077]), Recall: tensor([0.4062])\n",
      "Epoch 36, Loss: 0.9748, Acc: 0.056, Prec: tensor([0.0074]), Recall: tensor([0.3894])\n",
      "Epoch 37, Loss: 0.9597, Acc: 0.057, Prec: tensor([0.0074]), Recall: tensor([0.3894])\n",
      "Epoch 38, Loss: 0.9670, Acc: 0.057, Prec: tensor([0.0074]), Recall: tensor([0.3894])\n",
      "Epoch 39, Loss: 0.9684, Acc: 0.058, Prec: tensor([0.0074]), Recall: tensor([0.3894])\n",
      "Epoch 40, Loss: 0.9624, Acc: 0.058, Prec: tensor([0.0074]), Recall: tensor([0.3866])\n",
      "Epoch 41, Loss: 0.9676, Acc: 0.058, Prec: tensor([0.0073]), Recall: tensor([0.3810])\n",
      "Epoch 42, Loss: 0.9710, Acc: 0.059, Prec: tensor([0.0072]), Recall: tensor([0.3754])\n",
      "Epoch 43, Loss: 0.9556, Acc: 0.059, Prec: tensor([0.0072]), Recall: tensor([0.3754])\n",
      "Epoch 44, Loss: 0.9679, Acc: 0.059, Prec: tensor([0.0072]), Recall: tensor([0.3754])\n",
      "Epoch 45, Loss: 0.9665, Acc: 0.059, Prec: tensor([0.0072]), Recall: tensor([0.3754])\n",
      "Epoch 46, Loss: 0.9696, Acc: 0.060, Prec: tensor([0.0072]), Recall: tensor([0.3754])\n",
      "Epoch 47, Loss: 0.9663, Acc: 0.060, Prec: tensor([0.0070]), Recall: tensor([0.3669])\n",
      "Epoch 48, Loss: 0.9717, Acc: 0.061, Prec: tensor([0.0070]), Recall: tensor([0.3641])\n",
      "Epoch 49, Loss: 0.9793, Acc: 0.061, Prec: tensor([0.0070]), Recall: tensor([0.3641])\n",
      "Epoch 50, Loss: 0.9567, Acc: 0.062, Prec: tensor([0.0070]), Recall: tensor([0.3641])\n",
      "Epoch 51, Loss: 0.9755, Acc: 0.062, Prec: tensor([0.0069]), Recall: tensor([0.3613])\n",
      "Epoch 52, Loss: 0.9609, Acc: 0.062, Prec: tensor([0.0069]), Recall: tensor([0.3585])\n",
      "Epoch 53, Loss: 0.9896, Acc: 0.063, Prec: tensor([0.0068]), Recall: tensor([0.3557])\n",
      "Epoch 54, Loss: 0.9714, Acc: 0.063, Prec: tensor([0.0069]), Recall: tensor([0.3557])\n",
      "Epoch 55, Loss: 0.9606, Acc: 0.064, Prec: tensor([0.0069]), Recall: tensor([0.3557])\n",
      "Epoch 56, Loss: 0.9611, Acc: 0.064, Prec: tensor([0.0069]), Recall: tensor([0.3557])\n",
      "Epoch 57, Loss: 0.9639, Acc: 0.064, Prec: tensor([0.0068]), Recall: tensor([0.3529])\n",
      "Epoch 58, Loss: 0.9656, Acc: 0.065, Prec: tensor([0.0068]), Recall: tensor([0.3529])\n",
      "Epoch 59, Loss: 0.9665, Acc: 0.065, Prec: tensor([0.0068]), Recall: tensor([0.3501])\n",
      "Epoch 60, Loss: 0.9574, Acc: 0.065, Prec: tensor([0.0067]), Recall: tensor([0.3473])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def train_gcn_model(targets, inputs, model = None, num_epochs=30, batch_size=1, lr=1e-4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    dataset = TensorDataset(inputs, targets)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    if model == None:\n",
    "        model = GCNEmbedding()\n",
    "    model.to(device)\n",
    "\n",
    "    # pos_weight berechnen für jede Klasse (Shape: [2])\n",
    "    total_positives = targets.sum(dim=(0, 1))  # shape: [2]\n",
    "    total_negatives = targets.shape[0] * targets.shape[1] - total_positives\n",
    "    pos_weight = total_negatives / (total_positives + 1e-6)  # Numerische Stabilität\n",
    "    pos_weight = torch.sqrt(total_negatives / (total_positives + 1e-8))\n",
    "    pos_weight = pos_weight.clamp(min=1.0, max=100.0).to(device)\n",
    "\n",
    "    print(f\"pos_weight used for BCEWithLogitsLoss: {pos_weight}\")\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for xb, yb in dataloader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)  # [B, N, 2]\n",
    "            loss = loss_fn(logits, yb.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Zum Debuggen: Prediction & Label sammeln\n",
    "            probs = torch.sigmoid(logits.detach())\n",
    "            preds = (probs > 0.5).float()\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(yb.cpu())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        # Accuracy & co. berechnen\n",
    "        all_preds = torch.cat(all_preds, dim=0).view(-1, 1)\n",
    "        all_labels = torch.cat(all_labels, dim=0).view(-1, 1)\n",
    "        correct = (all_preds == all_labels).float()\n",
    "        acc = correct.mean().item()\n",
    "        precision = (all_preds * all_labels).sum(dim=0) / (all_preds.sum(dim=0) + 1e-6)\n",
    "        recall = (all_preds * all_labels).sum(dim=0) / (all_labels.sum(dim=0) + 1e-6)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d}, Loss: {avg_loss:.4f}, Acc: {acc:.3f}, Prec: {precision}, Recall: {recall}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = train_gcn_model(theta_all, x_all_flat, model = None, num_epochs=60, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d9768933-2bd8-4f58-bb89-0a3dc1347c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_weight used for BCEWithLogitsLoss: tensor([20.])\n",
      "Initialized last layer bias to tensor([-3.8742])\n",
      "Epoch 01, Loss: 0.8949, Thresholds: [0.7310557961463928], Prec: [0.24109460413455963], Rec: [0.47344741225242615]\n",
      "Epoch 02, Loss: 0.8851, Thresholds: [0.7310581207275391], Prec: [0.24108068645000458], Rec: [0.4719095230102539]\n",
      "Epoch 03, Loss: 0.8814, Thresholds: [0.7310484051704407], Prec: [0.20925560593605042], Rec: [0.6170238852500916]\n",
      "Epoch 04, Loss: 0.8790, Thresholds: [0.7310568690299988], Prec: [0.20811203122138977], Rec: [0.6132714748382568]\n",
      "Epoch 05, Loss: 0.8778, Thresholds: [0.7310584783554077], Prec: [0.20907334983348846], Rec: [0.6066365838050842]\n",
      "Epoch 06, Loss: 0.8773, Thresholds: [0.7310585379600525], Prec: [0.20876014232635498], Rec: [0.5981914401054382]\n",
      "Epoch 07, Loss: 0.8769, Thresholds: [0.7310248613357544], Prec: [0.19025234878063202], Rec: [0.7479634284973145]\n",
      "Epoch 08, Loss: 0.8767, Thresholds: [0.731031596660614], Prec: [0.19019316136837006], Rec: [0.7477173209190369]\n",
      "Epoch 09, Loss: 0.8766, Thresholds: [0.7310375571250916], Prec: [0.18984967470169067], Rec: [0.7463904023170471]\n",
      "Epoch 10, Loss: 0.8765, Thresholds: [0.7310466170310974], Prec: [0.18986541032791138], Rec: [0.7464255094528198]\n",
      "Epoch 11, Loss: 0.8764, Thresholds: [0.7310489416122437], Prec: [0.18979418277740479], Rec: [0.7461355328559875]\n",
      "Epoch 12, Loss: 0.8764, Thresholds: [0.7310514450073242], Prec: [0.18985310196876526], Rec: [0.7464255094528198]\n",
      "Epoch 13, Loss: 0.8763, Thresholds: [0.7310532927513123], Prec: [0.18973708152770996], Rec: [0.7458543181419373]\n",
      "Epoch 14, Loss: 0.8763, Thresholds: [0.7310551404953003], Prec: [0.18978393077850342], Rec: [0.7457049489021301]\n",
      "Epoch 15, Loss: 0.8762, Thresholds: [0.7310541868209839], Prec: [0.189501091837883], Rec: [0.7450282573699951]\n",
      "Epoch 16, Loss: 0.8762, Thresholds: [0.731057345867157], Prec: [0.1899527907371521], Rec: [0.7467506527900696]\n",
      "Epoch 17, Loss: 0.8762, Thresholds: [0.731057345867157], Prec: [0.1898479163646698], Rec: [0.7463200688362122]\n",
      "Epoch 18, Loss: 0.8761, Thresholds: [0.7310574650764465], Prec: [0.1895543485879898], Rec: [0.7451776266098022]\n",
      "Epoch 19, Loss: 0.8762, Thresholds: [0.7310575842857361], Prec: [0.18946020305156708], Rec: [0.7448524832725525]\n",
      "Epoch 20, Loss: 0.8761, Thresholds: [0.7310575842857361], Prec: [0.18931163847446442], Rec: [0.7442900538444519]\n",
      "Epoch 21, Loss: 0.8761, Thresholds: [0.7310579419136047], Prec: [0.18928422033786774], Rec: [0.7436749339103699]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 138\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThresholds: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthresholds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Prec: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecisions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Rec: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecalls\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m--> 138\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_gcn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_all_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[66], line 80\u001b[0m, in \u001b[0;36mtrain_gcn_model\u001b[1;34m(targets, inputs, model, num_epochs, batch_size, lr, use_temperature, temperature, min_threshold)\u001b[0m\n\u001b[0;32m     78\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(xb)  \u001b[38;5;66;03m# [B, N, C]\u001b[39;00m\n\u001b[0;32m     79\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, yb\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m---> 80\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     83\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pangenome-hgt-sim\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pangenome-hgt-sim\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pangenome-hgt-sim\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ===================================\n",
    "# Hilfsfunktion: Bias-Initialisierung\n",
    "# ===================================\n",
    "def init_last_bias_to_prior(model, prior_pos):\n",
    "    \"\"\"\n",
    "    Setzt den Bias der letzten Linearschicht auf log(p/(1-p)).\n",
    "    prior_pos: Tensor [C], P(y=1) pro Klasse.\n",
    "    \"\"\"\n",
    "    p = torch.clamp(prior_pos, 1e-6, 1 - 1e-6)\n",
    "    bias = torch.log(p / (1 - p))\n",
    "    last_linear = None\n",
    "    for m in reversed(list(model.modules())):\n",
    "        if isinstance(m, nn.Linear) and m.out_features == prior_pos.numel():\n",
    "            last_linear = m\n",
    "            break\n",
    "    if last_linear is not None:\n",
    "        with torch.no_grad():\n",
    "            last_linear.bias.copy_(bias)\n",
    "        print(f\"Initialized last layer bias to {bias}\")\n",
    "    else:\n",
    "        print(\"WARNING: Could not find last linear layer for bias init.\")\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# Training\n",
    "# ===================================\n",
    "def train_gcn_model(\n",
    "    targets,\n",
    "    inputs,\n",
    "    model=None,\n",
    "    num_epochs=30,\n",
    "    batch_size=16,\n",
    "    lr=1e-4,\n",
    "    use_temperature=False,\n",
    "    temperature=1.5,  # Skaliert Logits beim Eval\n",
    "    min_threshold=0.05\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    dataset = TensorDataset(inputs, targets)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    if model is None:\n",
    "        model = GCNEmbedding()\n",
    "    model.to(device)\n",
    "\n",
    "    # ==========================\n",
    "    # Klassenprävalenz & pos_weight\n",
    "    # ==========================\n",
    "    with torch.no_grad():\n",
    "        total = targets.shape[0] * targets.shape[1]\n",
    "        total_positives = targets.sum(dim=(0, 1))  # [C]\n",
    "        total_negatives = total - total_positives\n",
    "        pos_weight = (total_negatives / (total_positives + 1e-6)).clamp(1.0, 20.0)\n",
    "        pos_weight = pos_weight.to(device)\n",
    "        prior_pos = total_positives / total\n",
    "        print(f\"pos_weight used for BCEWithLogitsLoss: {pos_weight}\")\n",
    "\n",
    "    # Bias-Init\n",
    "    init_last_bias_to_prior(model, prior_pos.to(device))\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for xb, yb in dataloader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)  # [B, N, C]\n",
    "            loss = loss_fn(logits, yb.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        # ==========================\n",
    "        # Evaluation (Train-Set)\n",
    "        # ==========================\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            xb = inputs.to(device)\n",
    "            yb = targets.to(device)\n",
    "\n",
    "            logits = model(xb)\n",
    "            if use_temperature:\n",
    "                logits = logits * temperature\n",
    "\n",
    "            probs = torch.sigmoid(logits).cpu()\n",
    "            labels = yb.cpu()\n",
    "\n",
    "            # Metrics\n",
    "            flat_probs = probs.view(-1, probs.shape[-1])\n",
    "            flat_labels = labels.view(-1, labels.shape[-1])\n",
    "\n",
    "            thresholds, precisions, recalls = [], [], []\n",
    "\n",
    "            for k in range(flat_probs.shape[1]):\n",
    "                values = flat_probs[:, k]\n",
    "                ts = torch.quantile(values, torch.linspace(0, 1, 51))\n",
    "                best_f1, best_t = 0.0, 0.5\n",
    "\n",
    "                for t in ts:\n",
    "                    t = max(min_threshold, float(t))\n",
    "                    preds = (values > t).float()\n",
    "                    tp = (preds * flat_labels[:, k]).sum()\n",
    "                    fp = (preds * (1 - flat_labels[:, k])).sum()\n",
    "                    fn = ((1 - preds) * flat_labels[:, k]).sum()\n",
    "                    precision = tp / (tp + fp + 1e-8)\n",
    "                    recall = tp / (tp + fn + 1e-8)\n",
    "                    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "                    if f1 > best_f1:\n",
    "                        best_f1, best_t = f1.item(), t\n",
    "\n",
    "                thresholds.append(best_t)\n",
    "                preds_best = (values > best_t).float()\n",
    "                tp = (preds_best * flat_labels[:, k]).sum()\n",
    "                fp = (preds_best * (1 - flat_labels[:, k])).sum()\n",
    "                fn = ((1 - preds_best) * flat_labels[:, k]).sum()\n",
    "                precisions.append((tp / (tp + fp + 1e-8)).item())\n",
    "                recalls.append((tp / (tp + fn + 1e-8)).item())\n",
    "\n",
    "            print(f\"Epoch {epoch+1:02d}, Loss: {avg_loss:.4f}, \"\n",
    "                  f\"Thresholds: {thresholds}, Prec: {precisions}, Rec: {recalls}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_gcn_model(theta_all, x_all_flat, model = None, num_epochs=60, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c33becf8-22f9-4695-ad0a-fe23544cbc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beide Features = 1 (gesamt): 4210\n"
     ]
    }
   ],
   "source": [
    "# theta_all: [11328, 199, 2], binär (0/1)\n",
    "\n",
    "# Maske: beide Features = 1  -> Summe entlang der letzten Achse == 2\n",
    "both_one_mask = (theta_all.sum(dim=-1) == 2)   # [11328, 199], bool\n",
    "\n",
    "# Gesamtzahl\n",
    "count_total = both_one_mask.sum().item()\n",
    "print(\"Beide Features = 1 (gesamt):\", count_total)\n",
    "\n",
    "# Optional: pro Node (über alle Datenpunkte)\n",
    "count_per_node = both_one_mask.sum(dim=0)      # [199]\n",
    "# Optional: pro Datenpunkt (über alle Nodes)\n",
    "count_per_datapoint = both_one_mask.sum(dim=1) # [11328]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangenome-gene-transfer-simulation",
   "language": "python",
   "name": "pangenome-gene-transfer-simulation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
