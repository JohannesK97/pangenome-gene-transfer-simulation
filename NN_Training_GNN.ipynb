{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22f95433-12c3-4bdf-9df9-0ef687c71134",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uhewm\\AppData\\Local\\Temp\\ipykernel_21512\\1768722487.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edges = torch.tensor(graph_properties[1])          # Shape [2, 198]\n",
      "C:\\Users\\uhewm\\AppData\\Local\\Temp\\ipykernel_21512\\1768722487.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coords = torch.tensor(graph_properties[2].T)       # Shape [2, 199]\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pickle\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "\n",
    "output_dir = r\"C:\\Users\\uhewm\\Desktop\\ProjectHGT\\simulation_chunks\"\n",
    "all_files = sorted(glob.glob(os.path.join(output_dir, \"*.h5\")))\n",
    "\n",
    "x = []\n",
    "theta = []\n",
    "\n",
    "for file in random.sample(all_files, 100):\n",
    "#for file in all_files:\n",
    "    with h5py.File(file, \"r\") as f:\n",
    "        grp = f[\"results\"]\n",
    "        \n",
    "        graph_properties = pickle.loads(grp[\"graph_properties\"][()])\n",
    "        # Tensors erstellen\n",
    "        nodes = torch.tensor(graph_properties[0])          # Shape [199]\n",
    "        edges = torch.tensor(graph_properties[1])          # Shape [2, 198]\n",
    "        coords = torch.tensor(graph_properties[2].T)       # Shape [2, 199]\n",
    "\n",
    "        #coords[0] = coords[0] / max(coords[0]) * max(coords[1])\n",
    "        \n",
    "        # Falls nötig, auffüllen\n",
    "        if edges.shape[1] < nodes.shape[0]:\n",
    "            padding = torch.full((2, nodes.shape[0] - edges.shape[1]), -1, dtype=edges.dtype)\n",
    "            edges = torch.cat([edges, padding], dim=1)     # Jetzt Shape [2, 199]\n",
    "        \n",
    "        # Alles zu einem Tensor kombinieren: z. B. pro Knoten eine Zeile mit:\n",
    "        # [node_id, coord_x, coord_y, edge_from, edge_to]\n",
    "        combined_x = torch.stack([nodes, coords[0], coords[1], edges[0], edges[1]], dim=1)  # Shape [199, 5]\n",
    "        x.append(combined_x)\n",
    "\n",
    "        theta_gains = torch.tensor([1 if node in grp.attrs[\"parental_nodes_hgt_events_corrected\"] else 0 for node in graph_properties[0]])\n",
    "        theta_losses = torch.tensor([1 if node in grp.attrs[\"children_gene_nodes_loss_events\"] else 0 for node in graph_properties[0]])\n",
    "        #combined_theta = torch.stack([theta_gains, theta_losses], dim = 1)\n",
    "        combined_theta = torch.stack([theta_gains], dim = 1)\n",
    "        theta.append(combined_theta)\n",
    "\n",
    "# Für SNPE vorbereiten\n",
    "x_all = torch.stack(x).float()\n",
    "theta_all = torch.stack(theta).float()\n",
    "\n",
    "x_all_flat = x_all.view(x_all.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb0ece9e-bbae-48a4-be82-2c1b3220e5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "from sbi import utils\n",
    "from sbi.inference import SNPE\n",
    "from sbi.neural_nets import posterior_nn\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCNEmbedding(torch.nn.Module):\n",
    "    def __init__(self, in_channels=2, hidden_channels=32, gcn_out_channels=32, final_out_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_nodes = 199         # feste Anzahl Knoten (z.B.)\n",
    "        self.features_per_node = 5   # wie bei combined tensor\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, gcn_out_channels)\n",
    "        \n",
    "        self.fc_out = torch.nn.Linear(gcn_out_channels, final_out_channels)  # 2 binäre Outputs pro Knoten\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)  # Batchgröße\n",
    "        outputs = []\n",
    "\n",
    "        for i in range(B):\n",
    "            sample_flat = x[i]  # [num_nodes * features_per_node]\n",
    "            sample = sample_flat.view(self.num_nodes, self.features_per_node)\n",
    "            \n",
    "            nodes = sample[:, 0].long()\n",
    "            node_id_to_idx = {nid.item(): idx for idx, nid in enumerate(nodes)}\n",
    "            \n",
    "            raw_edges = sample[:, 3:5].T.long()\n",
    "            #edges_mapped = torch.tensor([\n",
    "            #    [node_id_to_idx.get(n.item(), -1) for n in raw_edges[0]],\n",
    "            #    [node_id_to_idx.get(n.item(), -1) for n in raw_edges[1]],\n",
    "            #])\n",
    "\n",
    "            mask = (raw_edges[0] >= 0) & (raw_edges[1] >= 0)\n",
    "            edge_index = raw_edges[:, mask]\n",
    "\n",
    "            # Optional: ungerichtet machen\n",
    "            #edge_index = torch.cat([edge_index.flip(0)], dim=1)\n",
    "            edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)\n",
    "\n",
    "            node_features = sample[:, 1:3]  # z. B. Koordinaten\n",
    "\n",
    "            z = F.relu(self.conv1(node_features, edge_index))\n",
    "            #z = F.relu(self.conv2(z, edge_index))  # Optional: weitere Aktivierung\n",
    "\n",
    "            logits = self.fc_out(z)               # [num_nodes, 2]\n",
    "            probs = torch.sigmoid(logits)         # Binäre Wahrscheinlichkeiten für beide Labels\n",
    "            \n",
    "            outputs.append(probs)\n",
    "\n",
    "        return torch.stack(outputs, dim=0)  # [B, num_nodes, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "615da4f2-b6ea-4701-9ae4-ff5d53621670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_weight used for BCEWithLogitsLoss: tensor([14.0712])\n",
      "Epoch 01, Loss: 0.9890, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 02, Loss: 0.9890, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 03, Loss: 0.9889, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 04, Loss: 0.9889, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 05, Loss: 0.9889, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 06, Loss: 0.9888, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 07, Loss: 0.9888, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 08, Loss: 0.9887, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 09, Loss: 0.9887, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 10, Loss: 0.9886, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 11, Loss: 0.9886, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 12, Loss: 0.9885, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 13, Loss: 0.9885, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 14, Loss: 0.9884, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 15, Loss: 0.9884, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 16, Loss: 0.9883, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 17, Loss: 0.9883, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 18, Loss: 0.9882, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 19, Loss: 0.9882, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 20, Loss: 0.9881, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 21, Loss: 0.9881, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 22, Loss: 0.9881, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 23, Loss: 0.9880, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 24, Loss: 0.9880, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 25, Loss: 0.9879, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 26, Loss: 0.9879, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 27, Loss: 0.9878, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 28, Loss: 0.9878, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 29, Loss: 0.9877, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 30, Loss: 0.9877, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 31, Loss: 0.9876, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 32, Loss: 0.9876, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 33, Loss: 0.9875, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 34, Loss: 0.9875, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 35, Loss: 0.9874, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 36, Loss: 0.9874, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 37, Loss: 0.9873, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 38, Loss: 0.9873, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 39, Loss: 0.9872, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 40, Loss: 0.9872, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 41, Loss: 0.9871, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 42, Loss: 0.9871, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 43, Loss: 0.9870, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 44, Loss: 0.9870, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 45, Loss: 0.9869, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 46, Loss: 0.9869, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 47, Loss: 0.9868, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 48, Loss: 0.9868, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 49, Loss: 0.9867, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 50, Loss: 0.9867, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 51, Loss: 0.9866, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 52, Loss: 0.9865, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 53, Loss: 0.9865, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 54, Loss: 0.9864, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 55, Loss: 0.9864, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 56, Loss: 0.9863, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 57, Loss: 0.9863, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 58, Loss: 0.9862, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 59, Loss: 0.9862, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n",
      "Epoch 60, Loss: 0.9861, Acc: 0.005, Prec: tensor([0.0050]), Recall: tensor([1.0000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def train_gcn_model(targets, inputs, model = None, num_epochs=30, batch_size=1, lr=1e-4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    dataset = TensorDataset(inputs, targets)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    if model == None:\n",
    "        model = GCNEmbedding()\n",
    "    model.to(device)\n",
    "\n",
    "    # pos_weight berechnen für jede Klasse (Shape: [2])\n",
    "    total_positives = targets.sum(dim=(0, 1))  # shape: [2]\n",
    "    total_negatives = targets.shape[0] * targets.shape[1] - total_positives\n",
    "    pos_weight = total_negatives / (total_positives + 1e-6)  # Numerische Stabilität\n",
    "    pos_weight = torch.sqrt(total_negatives / (total_positives + 1e-8))\n",
    "    pos_weight = pos_weight.clamp(min=1.0, max=100.0).to(device)\n",
    "\n",
    "    print(f\"pos_weight used for BCEWithLogitsLoss: {pos_weight}\")\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        for xb, yb in dataloader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)  # [B, N, 2]\n",
    "            loss = loss_fn(logits, yb.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Zum Debuggen: Prediction & Label sammeln\n",
    "            probs = torch.sigmoid(logits.detach())\n",
    "            preds = (probs > 0.5).float()\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(yb.cpu())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        # Accuracy & co. berechnen\n",
    "        all_preds = torch.cat(all_preds, dim=0).view(-1, 1)\n",
    "        all_labels = torch.cat(all_labels, dim=0).view(-1, 1)\n",
    "        correct = (all_preds == all_labels).float()\n",
    "        acc = correct.mean().item()\n",
    "        precision = (all_preds * all_labels).sum(dim=0) / (all_preds.sum(dim=0) + 1e-6)\n",
    "        recall = (all_preds * all_labels).sum(dim=0) / (all_labels.sum(dim=0) + 1e-6)\n",
    "\n",
    "        print(f\"Epoch {epoch+1:02d}, Loss: {avg_loss:.4f}, Acc: {acc:.3f}, Prec: {precision}, Recall: {recall}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = train_gcn_model(theta_all, x_all_flat, model = None, num_epochs=60, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9768933-2bd8-4f58-bb89-0a3dc1347c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_weight used for BCEWithLogitsLoss: tensor([20.])\n",
      "Initialized last layer bias to tensor([-5.2883])\n",
      "Epoch 01, Loss: 0.7617, Thresholds: [0.5], Prec: [0.005025125574320555], Rec: [1.0]\n",
      "Epoch 02, Loss: 0.7617, Thresholds: [0.5], Prec: [0.005025125574320555], Rec: [1.0]\n",
      "Epoch 03, Loss: 0.7617, Thresholds: [0.5], Prec: [0.005025125574320555], Rec: [1.0]\n",
      "Epoch 04, Loss: 0.7617, Thresholds: [0.5], Prec: [0.005025125574320555], Rec: [1.0]\n",
      "Epoch 05, Loss: 0.7617, Thresholds: [0.5], Prec: [0.005025125574320555], Rec: [1.0]\n",
      "Epoch 06, Loss: 0.7617, Thresholds: [0.5], Prec: [0.005025125574320555], Rec: [1.0]\n",
      "Epoch 07, Loss: 0.7617, Thresholds: [0.5002151727676392], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 08, Loss: 0.7617, Thresholds: [0.5002182126045227], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 09, Loss: 0.7617, Thresholds: [0.500221312046051], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 10, Loss: 0.7617, Thresholds: [0.5002245306968689], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 11, Loss: 0.7617, Thresholds: [0.5002278685569763], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 12, Loss: 0.7617, Thresholds: [0.5002312660217285], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 13, Loss: 0.7617, Thresholds: [0.5002347826957703], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 14, Loss: 0.7617, Thresholds: [0.5002384781837463], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 15, Loss: 0.7617, Thresholds: [0.500242292881012], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 16, Loss: 0.7617, Thresholds: [0.5002462267875671], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 17, Loss: 0.7617, Thresholds: [0.5002503395080566], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 18, Loss: 0.7617, Thresholds: [0.5002545714378357], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 19, Loss: 0.7617, Thresholds: [0.5002589821815491], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 20, Loss: 0.7617, Thresholds: [0.5002635717391968], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 21, Loss: 0.7617, Thresholds: [0.5002683997154236], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 22, Loss: 0.7617, Thresholds: [0.5002733469009399], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 23, Loss: 0.7617, Thresholds: [0.5002785325050354], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 24, Loss: 0.7617, Thresholds: [0.5002838373184204], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 25, Loss: 0.7617, Thresholds: [0.5002893805503845], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 26, Loss: 0.7617, Thresholds: [0.5002950429916382], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 27, Loss: 0.7617, Thresholds: [0.5003010034561157], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 28, Loss: 0.7617, Thresholds: [0.5003070831298828], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 29, Loss: 0.7617, Thresholds: [0.500313401222229], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 30, Loss: 0.7617, Thresholds: [0.5003198981285095], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 31, Loss: 0.7617, Thresholds: [0.5003266334533691], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 32, Loss: 0.7617, Thresholds: [0.5003336071968079], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 33, Loss: 0.7617, Thresholds: [0.5003407597541809], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 34, Loss: 0.7617, Thresholds: [0.5003481507301331], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 35, Loss: 0.7617, Thresholds: [0.5003557801246643], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 36, Loss: 0.7617, Thresholds: [0.5003636479377747], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 37, Loss: 0.7617, Thresholds: [0.5003718733787537], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 38, Loss: 0.7617, Thresholds: [0.5003802180290222], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 39, Loss: 0.7617, Thresholds: [0.5003889203071594], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 40, Loss: 0.7617, Thresholds: [0.5003978610038757], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 41, Loss: 0.7617, Thresholds: [0.5004071593284607], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 42, Loss: 0.7617, Thresholds: [0.5004166960716248], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 43, Loss: 0.7617, Thresholds: [0.5004265904426575], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 44, Loss: 0.7616, Thresholds: [0.5004367232322693], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 45, Loss: 0.7616, Thresholds: [0.5004472732543945], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 46, Loss: 0.7616, Thresholds: [0.5004581212997437], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 47, Loss: 0.7616, Thresholds: [0.5004693269729614], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 48, Loss: 0.7616, Thresholds: [0.5004809498786926], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 49, Loss: 0.7616, Thresholds: [0.5004929304122925], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 50, Loss: 0.7616, Thresholds: [0.5005053281784058], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 51, Loss: 0.7616, Thresholds: [0.5005180239677429], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 52, Loss: 0.7616, Thresholds: [0.5005311965942383], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 53, Loss: 0.7616, Thresholds: [0.5005448460578918], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 54, Loss: 0.7616, Thresholds: [0.5005589127540588], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 55, Loss: 0.7616, Thresholds: [0.5005735158920288], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 56, Loss: 0.7616, Thresholds: [0.500588595867157], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 57, Loss: 0.7616, Thresholds: [0.5006040930747986], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 58, Loss: 0.7616, Thresholds: [0.5006201863288879], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 59, Loss: 0.7616, Thresholds: [0.5006368160247803], Prec: [0.005050505045801401], Rec: [1.0]\n",
      "Epoch 60, Loss: 0.7616, Thresholds: [0.5006539225578308], Prec: [0.005050505045801401], Rec: [1.0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ===================================\n",
    "# Hilfsfunktion: Bias-Initialisierung\n",
    "# ===================================\n",
    "def init_last_bias_to_prior(model, prior_pos):\n",
    "    \"\"\"\n",
    "    Setzt den Bias der letzten Linearschicht auf log(p/(1-p)).\n",
    "    prior_pos: Tensor [C], P(y=1) pro Klasse.\n",
    "    \"\"\"\n",
    "    p = torch.clamp(prior_pos, 1e-6, 1 - 1e-6)\n",
    "    bias = torch.log(p / (1 - p))\n",
    "    last_linear = None\n",
    "    for m in reversed(list(model.modules())):\n",
    "        if isinstance(m, nn.Linear) and m.out_features == prior_pos.numel():\n",
    "            last_linear = m\n",
    "            break\n",
    "    if last_linear is not None:\n",
    "        with torch.no_grad():\n",
    "            last_linear.bias.copy_(bias)\n",
    "        print(f\"Initialized last layer bias to {bias}\")\n",
    "    else:\n",
    "        print(\"WARNING: Could not find last linear layer for bias init.\")\n",
    "\n",
    "\n",
    "# ===================================\n",
    "# Training\n",
    "# ===================================\n",
    "def train_gcn_model(\n",
    "    targets,\n",
    "    inputs,\n",
    "    model=None,\n",
    "    num_epochs=30,\n",
    "    batch_size=16,\n",
    "    lr=1e-4,\n",
    "    use_temperature=False,\n",
    "    temperature=1.5,  # Skaliert Logits beim Eval\n",
    "    min_threshold=0.05\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    dataset = TensorDataset(inputs, targets)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    if model is None:\n",
    "        model = GCNEmbedding()\n",
    "    model.to(device)\n",
    "\n",
    "    # ==========================\n",
    "    # Klassenprävalenz & pos_weight\n",
    "    # ==========================\n",
    "    with torch.no_grad():\n",
    "        total = targets.shape[0] * targets.shape[1]\n",
    "        total_positives = targets.sum(dim=(0, 1))  # [C]\n",
    "        total_negatives = total - total_positives\n",
    "        pos_weight = (total_negatives / (total_positives + 1e-6)).clamp(1.0, 20.0)\n",
    "        pos_weight = pos_weight.to(device)\n",
    "        prior_pos = total_positives / total\n",
    "        print(f\"pos_weight used for BCEWithLogitsLoss: {pos_weight}\")\n",
    "\n",
    "    # Bias-Init\n",
    "    init_last_bias_to_prior(model, prior_pos.to(device))\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for xb, yb in dataloader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)  # [B, N, C]\n",
    "            loss = loss_fn(logits, yb.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "\n",
    "        # ==========================\n",
    "        # Evaluation (Train-Set)\n",
    "        # ==========================\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            xb = inputs.to(device)\n",
    "            yb = targets.to(device)\n",
    "\n",
    "            logits = model(xb)\n",
    "            if use_temperature:\n",
    "                logits = logits * temperature\n",
    "\n",
    "            probs = torch.sigmoid(logits).cpu()\n",
    "            labels = yb.cpu()\n",
    "\n",
    "            # Metrics\n",
    "            flat_probs = probs.view(-1, probs.shape[-1])\n",
    "            flat_labels = labels.view(-1, labels.shape[-1])\n",
    "\n",
    "            thresholds, precisions, recalls = [], [], []\n",
    "\n",
    "            for k in range(flat_probs.shape[1]):\n",
    "                values = flat_probs[:, k]\n",
    "                ts = torch.quantile(values, torch.linspace(0, 1, 51))\n",
    "                best_f1, best_t = 0.0, 0.5\n",
    "\n",
    "                for t in ts:\n",
    "                    t = max(min_threshold, float(t))\n",
    "                    preds = (values > t).float()\n",
    "                    tp = (preds * flat_labels[:, k]).sum()\n",
    "                    fp = (preds * (1 - flat_labels[:, k])).sum()\n",
    "                    fn = ((1 - preds) * flat_labels[:, k]).sum()\n",
    "                    precision = tp / (tp + fp + 1e-8)\n",
    "                    recall = tp / (tp + fn + 1e-8)\n",
    "                    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "                    if f1 > best_f1:\n",
    "                        best_f1, best_t = f1.item(), t\n",
    "\n",
    "                thresholds.append(best_t)\n",
    "                preds_best = (values > best_t).float()\n",
    "                tp = (preds_best * flat_labels[:, k]).sum()\n",
    "                fp = (preds_best * (1 - flat_labels[:, k])).sum()\n",
    "                fn = ((1 - preds_best) * flat_labels[:, k]).sum()\n",
    "                precisions.append((tp / (tp + fp + 1e-8)).item())\n",
    "                recalls.append((tp / (tp + fn + 1e-8)).item())\n",
    "\n",
    "            print(f\"Epoch {epoch+1:02d}, Loss: {avg_loss:.4f}, \"\n",
    "                  f\"Thresholds: {thresholds}, Prec: {precisions}, Rec: {recalls}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "model = train_gcn_model(theta_all, x_all_flat, model = None, num_epochs=60, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3987ed44-c241-470c-b15d-2a38ee425551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCNEmbedding(\n",
      "  (conv1): GCNConv(2, 32)\n",
      "  (conv2): GCNConv(32, 32)\n",
      "  (fc_out): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangenome-gene-transfer-simulation",
   "language": "python",
   "name": "pangenome-gene-transfer-simulation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
