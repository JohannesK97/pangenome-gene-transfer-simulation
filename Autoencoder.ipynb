{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1247564-6fb9-4f8f-a96d-2fd44b1899cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "def one_hot_pad(sequences, max_len=1000, alphabet=\"ACGT-\"):\n",
    "    char_to_index = {c: i for i, c in enumerate(alphabet)}\n",
    "    X = np.zeros((len(sequences), max_len, len(alphabet)), dtype=np.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        for j, c in enumerate(seq[:max_len]):\n",
    "            X[i, j, char_to_index[c]] = 1.0\n",
    "    return X\n",
    "\n",
    "def generate_random_sequences(n_sequences=10000, max_len=1000, alphabet=\"ACGT-\"):\n",
    "    sequences = []\n",
    "    for _ in range(n_sequences):\n",
    "        L = random.randint(1, max_len)\n",
    "        seq = ''.join(random.choices(alphabet, k=L))\n",
    "        sequences.append(seq)\n",
    "    return sequences\n",
    "    \n",
    "def one_hot_encode(sequences, alphabet=\"ACGT-\"):\n",
    "    char_to_index = {c: i for i, c in enumerate(alphabet)}\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    X = np.zeros((len(sequences), max_len, len(alphabet)), dtype=np.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        for j, c in enumerate(seq):\n",
    "            X[i, j, char_to_index[c]] = 1.0\n",
    "    return X\n",
    "\n",
    "class ConvSeqAutoencoder(nn.Module):\n",
    "    def __init__(self, seq_len=1000, alphabet_size=5, latent_dim=32):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(alphabet_size, 32, kernel_size=9, stride=3, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=9, stride=3, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size=9, stride=3, padding=4),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),   # komprimiert auf Länge 1\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(128, latent_dim)\n",
    "        \n",
    "        # Decoder (vereinfacht)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, seq_len * alphabet_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.seq_len = seq_len\n",
    "        self.alphabet_size = alphabet_size\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Eingabe: (B, L, A)\n",
    "        x = x.permute(0, 2, 1)       # -> (B, A, L)\n",
    "        encoded = self.encoder(x).squeeze(-1)\n",
    "        z = self.fc_mu(encoded)\n",
    "        decoded = self.decoder(z)\n",
    "        decoded = decoded.view(-1, self.seq_len, self.alphabet_size)\n",
    "        return decoded, z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46484c74-160e-4071-968c-901ea21876e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss=0.2492\n",
      "Epoch 100, loss=0.0000\n",
      "Epoch 200, loss=0.0000\n",
      "Epoch 300, loss=0.0000\n",
      "Epoch 400, loss=0.0000\n",
      "tensor([[ 4.9679, -3.5180,  0.2629, -4.2421, -3.0753,  3.1379,  3.5786,  4.1005],\n",
      "        [ 1.6593, -4.9951, -0.7777, -1.4683,  2.7091,  0.4280,  4.6903,  5.1249],\n",
      "        [ 5.5396, -0.0456,  2.2221, -4.3101, -6.6020,  6.5573,  2.0966,  2.3053],\n",
      "        [ 5.5396, -0.0456,  2.2221, -4.3101, -6.6020,  6.5573,  2.0966,  2.3053],\n",
      "        [-0.7830, -4.2424, -5.3825,  0.0084,  5.2634, -4.0651,  3.3325,  6.6650]])\n",
      "torch.Size([5, 8])\n"
     ]
    }
   ],
   "source": [
    "# Beispiel-Daten\n",
    "seqs = ['TTGT', 'ATC-', 'TTGC', 'TTGC', '----']\n",
    "X = one_hot_encode(seqs)\n",
    "X = torch.tensor(X.reshape(len(seqs), -1))  # Flatten\n",
    "\n",
    "model = SequenceAutoencoder(seq_len=X.shape[1] // 5, latent_dim=8)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    recon, z = model(X)\n",
    "    loss = criterion(recon, X)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, loss={loss.item():.4f}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings = model.encoder(X)\n",
    "print(embeddings)\n",
    "print(embeddings.shape)  # (Anzahl Sequenzen, latent_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de2e02f0-abc9-4730-bcc6-c056830e1477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss=0.1052\n",
      "Epoch 2, loss=0.0845\n",
      "Epoch 3, loss=0.0814\n",
      "Epoch 4, loss=0.0811\n",
      "Epoch 5, loss=0.0810\n",
      "Epoch 6, loss=0.0809\n",
      "Epoch 7, loss=0.0808\n",
      "Epoch 8, loss=0.0808\n",
      "Epoch 9, loss=0.0807\n",
      "Epoch 10, loss=0.0807\n"
     ]
    }
   ],
   "source": [
    "# Daten erzeugen\n",
    "seqs = generate_random_sequences(n_sequences=20000, max_len=1000)\n",
    "X = one_hot_pad(seqs)\n",
    "X = torch.tensor(X)\n",
    "\n",
    "# Modell, Optimizer, Loss\n",
    "model = ConvSeqAutoencoder(seq_len=1000, latent_dim=32)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training\n",
    "batch_size = 64\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    perm = torch.randperm(X.size(0))\n",
    "    total_loss = 0\n",
    "    for i in range(0, X.size(0), batch_size):\n",
    "        batch = X[perm[i:i+batch_size]]\n",
    "        recon, z = model(batch)\n",
    "        loss = criterion(recon, batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, loss={total_loss / (X.size(0)/batch_size):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aade137-29cb-48c3-ad80-d131b7806390",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# x_a, x_p, x_n: Batches (B, L, A) tensors\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m _, z_a \u001b[38;5;241m=\u001b[39m model(\u001b[43mx_a\u001b[49m)\n\u001b[1;32m     46\u001b[0m _, z_p \u001b[38;5;241m=\u001b[39m model(x_p)\n\u001b[1;32m     47\u001b[0m _, z_n \u001b[38;5;241m=\u001b[39m model(x_n)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_a' is not defined"
     ]
    }
   ],
   "source": [
    "# Auswertung: Korrelation und Scatterplot\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from random import sample\n",
    "\n",
    "def hamming_seq(a, b):\n",
    "    # a,b als strings gleicher Länge (hier padded)\n",
    "    return sum(x != y for x, y in zip(a, b))\n",
    "\n",
    "def evaluate_embeddings(seqs, embeddings, n_pairs=1000):\n",
    "    # seqs: list of strings (alle gleiche Länge)\n",
    "    # embeddings: numpy array (N, D)\n",
    "    N = len(seqs)\n",
    "    pairs = [tuple(sample(range(N), 2)) for _ in range(n_pairs)]\n",
    "    hammings = []\n",
    "    edists = []\n",
    "    for i, j in pairs:\n",
    "        hamm = hamming_seq(seqs[i], seqs[j])\n",
    "        ed = np.linalg.norm(embeddings[i] - embeddings[j])\n",
    "        hammings.append(hamm)\n",
    "        edists.append(ed)\n",
    "    hammings = np.array(hammings)\n",
    "    edists = np.array(edists)\n",
    "    print(\"Pearson r:\", pearsonr(hammings, edists)[0])\n",
    "    print(\"Spearman rho:\", spearmanr(hammings, edists)[0])\n",
    "    # Scatter\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(hammings, edists, s=8, alpha=0.6)\n",
    "    plt.xlabel(\"Hamming-Distanz\")\n",
    "    plt.ylabel(\"Embedding (Euklid) Distanz\")\n",
    "    plt.title(\"Hamming vs. Embedding Distanz\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return hammings, edists\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Beispiel Triplet-Loss-Trainingsstep (pseudocode)\n",
    "margin = 1.0\n",
    "optimizer.zero_grad()\n",
    "# x_a, x_p, x_n: Batches (B, L, A) tensors\n",
    "_, z_a = model(x_a)\n",
    "_, z_p = model(x_p)\n",
    "_, z_n = model(x_n)\n",
    "loss_triplet = F.triplet_margin_loss(z_a, z_p, z_n, margin=margin)\n",
    "# optional: kombiniere mit Rekonstruktionsloss\n",
    "loss_recon = criterion(model(x_a)[0], x_a)\n",
    "loss = loss_recon + 0.5 * loss_triplet\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
