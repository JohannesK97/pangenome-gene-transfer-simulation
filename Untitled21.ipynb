{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3a42a9b-a94b-45d3-a8e8-ffa0d85eb238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kippnich/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/numpy/core/getlimits.py:542: UserWarning: Signature b'\\x00\\xd0\\xcc\\xcc\\xcc\\xcc\\xcc\\xcc\\xfb\\xbf\\x00\\x00\\x00\\x00\\x00\\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.\n",
      "This warnings indicates broken support for the dtype!\n",
      "  machar = _get_machar(dtype)\n",
      "/tmp/ipykernel_16427/4233334138.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edges = torch.tensor(graph_properties[1], dtype=torch.long)  # [2, num_edges]\n",
      "/tmp/ipykernel_16427/4233334138.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coords = torch.tensor(graph_properties[2].T)             # [2, num_nodes]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import h5py\n",
    "import pickle\n",
    "import torch\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "\n",
    "def one_hot_encode(sequences, gene_present, gene_length, alphabet=['A','C','T','G','-']):\n",
    "    \"\"\"\n",
    "    sequences: List of strings (DNA sequences)\n",
    "    gene_present: np.array(bool) oder Torch Tensor, gleiche Länge wie sequences\n",
    "    gene_length: int, fixe Länge für das Hot-Encoding\n",
    "    alphabet: list, Zeichenalphabet\n",
    "    \"\"\"\n",
    "    num_samples = len(sequences)\n",
    "    num_chars = len(alphabet)\n",
    "    char_to_idx = {c:i for i,c in enumerate(alphabet)}\n",
    "    sequences_str = [s.decode('utf-8') for s in sequences]\n",
    "    gene_present = np.array(gene_present, dtype=bool)\n",
    "    \n",
    "    # 1️⃣ Leere Batch-Matrix vorbereiten: (num_samples, gene_length, num_chars)\n",
    "    batch = np.zeros((num_samples, gene_length, num_chars), dtype=np.float32)\n",
    "    \n",
    "    # 2️⃣ Hot-Encode alle Sequenzen\n",
    "    for i, seq in enumerate(sequences_str):\n",
    "        if gene_present[i]:\n",
    "            L = min(len(seq), gene_length)  # abschneiden\n",
    "            for j, c in enumerate(seq[:L]):\n",
    "                if c in char_to_idx:\n",
    "                    batch[i, j, char_to_idx[c]] = 1.0\n",
    "        elif gene_present[i] == 0:\n",
    "            batch[i, :, :] = -1.0\n",
    "            \n",
    "    # 3️⃣ Zufällige, aber konsistente Spaltenpermutation\n",
    "    perm = np.random.permutation(gene_length)\n",
    "    batch = batch[:, perm, :]\n",
    "    \n",
    "    # 4️⃣ Optional: Flatten zu Vektor (num_samples, gene_length*num_chars)\n",
    "    batch_flat = batch.reshape(num_samples, -1)\n",
    "    \n",
    "    return torch.tensor(batch_flat)  # shape: (num_samples, gene_length*num_chars)\n",
    "\n",
    "def load_file(file):\n",
    "\n",
    "    gene_length = 300\n",
    "    #nucleotide_mutation_rate = 0.1\n",
    "    \n",
    "    with h5py.File(file, \"r\") as f:\n",
    "            grp = f[\"results\"]\n",
    "            # Load graph_properties (pickle stored in dataset)\n",
    "            graph_properties = pickle.loads(grp[\"graph_properties\"][()])\n",
    "    \n",
    "            # Unpack graph properties\n",
    "            nodes = torch.tensor(graph_properties[0])                # [num_nodes]\n",
    "            edges = torch.tensor(graph_properties[1], dtype=torch.long)  # [2, num_edges]\n",
    "            coords = torch.tensor(graph_properties[2].T)             # [2, num_nodes]\n",
    "    \n",
    "            # Load datasets instead of attrs\n",
    "            gene_absence_presence_matrix = grp[\"gene_absence_presence_matrix\"][()]\n",
    "            nucleotide_sequences = grp[\"nucleotide_sequences\"][()]\n",
    "            #children_gene_nodes_loss_events = grp[\"children_gene_nodes_loss_events\"][()]\n",
    "        \n",
    "            # Load HGT events (simplified)\n",
    "            hgt_events = {}\n",
    "            hgt_grp_simpl = grp[\"nodes_hgt_events_simplified\"]\n",
    "            for site_id in hgt_grp_simpl.keys():\n",
    "                hgt_events[int(site_id)] = hgt_grp_simpl[site_id][()]\n",
    "\n",
    "            hot_encoded_nucleotide_sequences = one_hot_encode(nucleotide_sequences, gene_absence_presence_matrix, gene_length)\n",
    "\n",
    "            # Fill the remaining nodes with zeros.\n",
    "            pad_rows = len(nodes) - len(nucleotide_sequences)\n",
    "            pad = torch.zeros((pad_rows, hot_encoded_nucleotide_sequences.shape[1]), dtype=hot_encoded_nucleotide_sequences.dtype)\n",
    "            hot_encoded_nucleotide_sequences = torch.cat([hot_encoded_nucleotide_sequences, pad], dim=0)\n",
    "        \n",
    "            G = nx.DiGraph()\n",
    "            \n",
    "            # Füge Knoten hinzu (optional mit Koordinaten als Attribut)\n",
    "            for i, node_id in enumerate(nodes.tolist()):\n",
    "                G.add_node(node_id, node_time = coords[:, i].tolist()[5])\n",
    "            \n",
    "            # Füge Kanten hinzu\n",
    "            edge_list = edges.tolist()\n",
    "            for src, dst in zip(edge_list[0], edge_list[1]):\n",
    "                G.add_edge(src, dst)\n",
    "            \n",
    "            # Collect all recipient_parent_nodes from all sites\n",
    "            recipient_parent_nodes = set()\n",
    "            for site_id in hgt_grp_simpl.keys():\n",
    "                arr = hgt_grp_simpl[site_id][()]  # load dataset as numpy structured array\n",
    "                recipient_parent_nodes.update(arr[\"recipient_child_node\"].tolist())\n",
    "            \n",
    "            # Build theta_gains: 1 if node is in recipient_parent_nodes, else 0\n",
    "            theta_gains = torch.tensor(\n",
    "                [1 if node in recipient_parent_nodes else 0 for node in range(len(G.nodes))],\n",
    "                dtype=torch.long\n",
    "            )\n",
    "\n",
    "            level = {n: 0 for n in G.nodes}  # Leaves haben Level 0\n",
    "            \n",
    "            # 3. Topologische Sortierung (damit Kinder vor Eltern behandelt werden)\n",
    "            for node in reversed(list(nx.topological_sort(G))):\n",
    "                successors = list(G.successors(node))\n",
    "                if successors:\n",
    "                    level[node] = 1 + max(level[s] for s in successors)\n",
    "            \n",
    "            # 4. Level als Attribut setzen\n",
    "            nx.set_node_attributes(G, level, \"level\")\n",
    "\n",
    "            ### Add candidate egdes, i.e. potential hgt edges:\n",
    "        \n",
    "            # Hole alle Knoten und ihre Zeiten\n",
    "            node_times = {n: G.nodes[n]['node_time'] for n in G.nodes}\n",
    "            sorted_nodes = sorted(node_times.keys(), key=lambda n: node_times[n])\n",
    "\n",
    "            # Sort the level and node_times from 0 to max_node_id and not in G.nodes order:\n",
    "            node_times = torch.tensor([node_times[n] for n in sorted_nodes], dtype=torch.float)\n",
    "            node_levels = torch.tensor([level[n] for n in sorted_nodes], dtype=torch.float)\n",
    "\n",
    "            # Füge Kanten hinzu\n",
    "            existing_edges = set(zip(edges[0].tolist(), edges[1].tolist()))\n",
    "            candidate_edges = []\n",
    "            for i, src in enumerate(sorted_nodes):\n",
    "                t_src = node_times[src]\n",
    "                for dst in sorted_nodes[i+1:]:  # nur spätere Knoten\n",
    "                    t_dst = node_times[dst]\n",
    "                    if t_dst > t_src:\n",
    "                        if (dst, src) not in existing_edges:  # vermeidet doppelte\n",
    "                            #G.add_edge(src, dst)\n",
    "                            candidate_edges.append((dst, src))\n",
    "            if candidate_edges:  \n",
    "                candidate_edges = torch.tensor(candidate_edges, dtype=torch.long).t()\n",
    "            else:\n",
    "                candidate_edges = torch.empty((2,0), dtype=torch.long)\n",
    "                    \n",
    "            data = Data(\n",
    "                nucleotide_sequences = nucleotide_sequences,\n",
    "                hot_encoded_nucleotide_sequences = hot_encoded_nucleotide_sequences,       # Node Features [num_nodes, 2]\n",
    "                edge_index = edges[[1, 0], :],        # Edge Index [2, num_edges]\n",
    "                candidate_edges = candidate_edges[[1, 0], :],\n",
    "                y = theta_gains,            # Labels [num_nodes]\n",
    "                file = file,\n",
    "                G = G,\n",
    "                recipient_parent_nodes = recipient_parent_nodes,\n",
    "                gene_absence_presence_matrix = gene_absence_presence_matrix,\n",
    "                node_times = node_times,\n",
    "                node_levels = node_levels\n",
    "            )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "folder = \"/mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks\"\n",
    "\n",
    "files = [os.path.join(folder, f) for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "\n",
    "examples = load_file(random.choice(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46630505-d7da-4783-bc43-c5c53d2d1077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Avg Loss: 0.5649\n",
      "Epoch 001 | Avg Loss: 0.5201\n",
      "Epoch 002 | Avg Loss: 0.5178\n",
      "Epoch 003 | Avg Loss: 0.5166\n",
      "Epoch 004 | Avg Loss: 0.5240\n",
      "Epoch 005 | Avg Loss: 0.5170\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 186\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03mlist_of_Data = []\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03mfor f in files:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03mprint(f\"{len(list_of_Data)} Dateien erfolgreich geladen.\")\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m model \u001b[38;5;241m=\u001b[39m GraphHGTModel(in_dim\u001b[38;5;241m=\u001b[39mlist_of_Data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mhot_encoded_nucleotide_sequences\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 186\u001b[0m \u001b[43mtrain_hgt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_of_Data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 149\u001b[0m, in \u001b[0;36mtrain_hgt\u001b[0;34m(model, data_list, device, epochs, lr)\u001b[0m\n\u001b[1;32m    146\u001b[0m     loss \u001b[38;5;241m=\u001b[39m bce(out_nodes, y_true)\n\u001b[1;32m    148\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 149\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m optim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    151\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_geometric.data import Data\n",
    "import math\n",
    "\n",
    "# ==========================================================\n",
    "\n",
    "# 1️⃣ Attention-Layer mit lernbaren Kantenwahrscheinlichkeiten\n",
    "\n",
    "# ==========================================================\n",
    "\n",
    "class HGTAttentionLayer(MessagePassing):\n",
    "    def __init__(self, in_dim, out_dim, heads=2, dropout=0.1):\n",
    "        super().__init__(aggr='add')\n",
    "        self.heads = heads\n",
    "        self.out_dim = out_dim\n",
    "        self.W = nn.Linear(in_dim, heads * out_dim, bias=False)\n",
    "        self.att_src = nn.Parameter(torch.Tensor(heads, out_dim))\n",
    "        self.att_dst = nn.Parameter(torch.Tensor(heads, out_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.W.weight)\n",
    "        nn.init.xavier_uniform_(self.att_src)\n",
    "        nn.init.xavier_uniform_(self.att_dst)\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_probs):\n",
    "        # x: [N, in_dim]\n",
    "        # edge_probs: [E]\n",
    "        x_proj = self.W(x)                                # [N, H*D]\n",
    "        N = x_proj.size(0)\n",
    "        return self.propagate(edge_index, x=x_proj, edge_attr=edge_probs, size=(N, N))\n",
    "\n",
    "    def message(self, x_j, x_i, edge_attr, index, ptr, size_i):\n",
    "        # x_j, x_i: [E, H*D]  (reshape back)\n",
    "        # edge_attr: [E]\n",
    "        H = self.heads\n",
    "        D = self.out_dim\n",
    "\n",
    "        # reshape node features to [E, H, D]\n",
    "        x_j = x_j.view(-1, H, D)\n",
    "        x_i = x_i.view(-1, H, D)\n",
    "\n",
    "        ep = edge_attr\n",
    "        if ep.dim() == 1:\n",
    "            ep = ep\n",
    "        elif ep.dim() == 2 and ep.shape[1] == 1:\n",
    "            ep = ep.squeeze(-1)\n",
    "        else:\n",
    "            ep = ep.view(-1)\n",
    "\n",
    "        # compute attention per head\n",
    "        alpha = (x_i * self.att_dst + x_j * self.att_src).sum(dim=-1) / math.sqrt(D)\n",
    "        alpha = alpha + torch.log(ep.unsqueeze(-1) + 1e-8)\n",
    "        alpha = F.leaky_relu(alpha, 0.2)\n",
    "        alpha = softmax(alpha, index)\n",
    "        alpha = self.dropout(alpha)\n",
    "\n",
    "        # scale messages: x_j * alpha * edge_prob\n",
    "        return (x_j * alpha.unsqueeze(-1) * ep.unsqueeze(-1).unsqueeze(-1)).view(-1, H * D)\n",
    "\n",
    "# ==========================================================\n",
    "\n",
    "# 2️⃣ Vollständiges Modell\n",
    "\n",
    "# ==========================================================\n",
    "\n",
    "class GraphHGTModel(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=32, out_dim=1, heads=2, layers=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(layers):\n",
    "            self.layers.append(HGTAttentionLayer(in_dim if i==0 else hidden, hidden, heads=heads))\n",
    "        self.node_out = nn.Linear(hidden * heads, out_dim)\n",
    "        \n",
    "    def forward(self, data, edge_logits):\n",
    "\n",
    "        x = data.hot_encoded_nucleotide_sequences          # (N, F)\n",
    "        if not hasattr(data, \"num_nodes\") or data.num_nodes != x.shape[0]:\n",
    "            data.num_nodes = x.shape[0]\n",
    "        \n",
    "        base_edges = data.edge_index.to(x.device)\n",
    "        cand_edges = data.candidate_edges.to(x.device)\n",
    "    \n",
    "        # combine edge sets\n",
    "        if cand_edges.numel() > 0:\n",
    "            edge_index = torch.cat([base_edges, cand_edges], dim=1)\n",
    "            edge_probs = torch.cat([\n",
    "                torch.ones(base_edges.shape[1], device=x.device),\n",
    "                torch.sigmoid(edge_logits)\n",
    "            ])\n",
    "        else:\n",
    "            edge_index = base_edges\n",
    "            edge_probs = torch.ones(base_edges.shape[1], device=x.device)\n",
    "    \n",
    "        h = x\n",
    "        for layer in self.layers:\n",
    "            h = F.relu(layer(h, edge_index, edge_probs))\n",
    "        out_nodes = self.node_out(h).squeeze(-1)\n",
    "        return out_nodes, edge_probs\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "\n",
    "# 3️⃣ Training-Funktion\n",
    "\n",
    "# ==========================================================\n",
    "\n",
    "def train_hgt(model, data_list, device=\"cpu\", epochs=20, lr=2e-3):\n",
    "    model.to(device)\n",
    "    # initialize edge logits\n",
    "    global_edge_logits = {}\n",
    "    for data in data_list:\n",
    "        if data.candidate_edges.numel() > 0:\n",
    "            E = data.candidate_edges.shape[1]\n",
    "            global_edge_logits[data.file] = nn.Parameter(torch.full((E,), -3.0, device=device))\n",
    "        else:\n",
    "            global_edge_logits[data.file] = None\n",
    "    params = list(model.parameters()) + [p for p in global_edge_logits.values() if p is not None]\n",
    "    optim = torch.optim.Adam(params, lr=lr, weight_decay=1e-5)\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for data in data_list:\n",
    "            data = data.to(device)\n",
    "            edge_logits = global_edge_logits[data.file]\n",
    "            y_true = data.y.float().to(device)\n",
    "            out_nodes, edge_probs = model(data, edge_logits)\n",
    "    \n",
    "            # split probabilities\n",
    "            if edge_logits is not None and data.candidate_edges.numel() > 0:\n",
    "                cand_scores = edge_probs[data.edge_index.shape[1]:]\n",
    "                target_scores = y_true[data.candidate_edges[1]]\n",
    "    \n",
    "                loss_node = bce(out_nodes, y_true)\n",
    "                loss_edge = bce(cand_scores, target_scores)\n",
    "                loss_sparsity = 0.01 * cand_scores.sum()\n",
    "                loss = loss_node + 0.5 * loss_edge + loss_sparsity\n",
    "            else:\n",
    "                loss = bce(out_nodes, y_true)\n",
    "    \n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "        print(f\"Epoch {epoch:03d} | Avg Loss: {total_loss/len(data_list):.4f}\")\n",
    "    \n",
    "    # output learned candidate edge probabilities\n",
    "    for data in data_list:\n",
    "        edge_logits = global_edge_logits[data.file]\n",
    "        if edge_logits is not None:\n",
    "            p = torch.sigmoid(edge_logits).detach().cpu().numpy()\n",
    "            print(f\"File {data.file} | Candidate edge probs: {p[:10]}\")\n",
    "\n",
    "# ==========================================================\n",
    "\n",
    "# 4️⃣ Anwendung\n",
    "\n",
    "# ==========================================================\n",
    "\n",
    "folder = \"/mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks\"\n",
    "\n",
    "files = [os.path.join(folder, f) for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "\n",
    "\"\"\"\n",
    "list_of_Data = []\n",
    "for f in files:\n",
    "    try:\n",
    "        d = load_file(f)\n",
    "        list_of_Data.append(d)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden von {f}: {e}\")\n",
    "\n",
    "print(f\"{len(list_of_Data)} Dateien erfolgreich geladen.\")\n",
    "\"\"\"\n",
    "\n",
    "model = GraphHGTModel(in_dim=list_of_Data[0].hot_encoded_nucleotide_sequences.shape[1])\n",
    "\n",
    "train_hgt(model, list_of_Data, device=\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2de79ba-b35c-4462-aac9-1602dc6282f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 8], y=[9], nucleotide_sequences=[5], hot_encoded_nucleotide_sequences=[9, 1500], candidate_edges=[2, 18], file='/mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks/simulation_0.h5', G=DiGraph with 9 nodes and 8 edges, recipient_parent_nodes=[0], gene_absence_presence_matrix=[5], node_times=[9], node_levels=[9])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_Data[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
