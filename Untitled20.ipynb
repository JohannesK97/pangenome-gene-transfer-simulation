{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f3e1dfb9-6d07-46ee-bf56-727ba86c85d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13654/4233334138.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edges = torch.tensor(graph_properties[1], dtype=torch.long)  # [2, num_edges]\n",
      "/tmp/ipykernel_13654/4233334138.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coords = torch.tensor(graph_properties[2].T)             # [2, num_nodes]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import h5py\n",
    "import pickle\n",
    "import torch\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "\n",
    "def one_hot_encode(sequences, gene_present, gene_length, alphabet=['A','C','T','G','-']):\n",
    "    \"\"\"\n",
    "    sequences: List of strings (DNA sequences)\n",
    "    gene_present: np.array(bool) oder Torch Tensor, gleiche L√§nge wie sequences\n",
    "    gene_length: int, fixe L√§nge f√ºr das Hot-Encoding\n",
    "    alphabet: list, Zeichenalphabet\n",
    "    \"\"\"\n",
    "    num_samples = len(sequences)\n",
    "    num_chars = len(alphabet)\n",
    "    char_to_idx = {c:i for i,c in enumerate(alphabet)}\n",
    "    sequences_str = [s.decode('utf-8') for s in sequences]\n",
    "    gene_present = np.array(gene_present, dtype=bool)\n",
    "    \n",
    "    # 1Ô∏è‚É£ Leere Batch-Matrix vorbereiten: (num_samples, gene_length, num_chars)\n",
    "    batch = np.zeros((num_samples, gene_length, num_chars), dtype=np.float32)\n",
    "    \n",
    "    # 2Ô∏è‚É£ Hot-Encode alle Sequenzen\n",
    "    for i, seq in enumerate(sequences_str):\n",
    "        if gene_present[i]:\n",
    "            L = min(len(seq), gene_length)  # abschneiden\n",
    "            for j, c in enumerate(seq[:L]):\n",
    "                if c in char_to_idx:\n",
    "                    batch[i, j, char_to_idx[c]] = 1.0\n",
    "        elif gene_present[i] == 0:\n",
    "            batch[i, :, :] = -1.0\n",
    "            \n",
    "    # 3Ô∏è‚É£ Zuf√§llige, aber konsistente Spaltenpermutation\n",
    "    perm = np.random.permutation(gene_length)\n",
    "    batch = batch[:, perm, :]\n",
    "    \n",
    "    # 4Ô∏è‚É£ Optional: Flatten zu Vektor (num_samples, gene_length*num_chars)\n",
    "    batch_flat = batch.reshape(num_samples, -1)\n",
    "    \n",
    "    return torch.tensor(batch_flat)  # shape: (num_samples, gene_length*num_chars)\n",
    "\n",
    "def load_file(file):\n",
    "\n",
    "    gene_length = 300\n",
    "    #nucleotide_mutation_rate = 0.1\n",
    "    \n",
    "    with h5py.File(file, \"r\") as f:\n",
    "            grp = f[\"results\"]\n",
    "            # Load graph_properties (pickle stored in dataset)\n",
    "            graph_properties = pickle.loads(grp[\"graph_properties\"][()])\n",
    "    \n",
    "            # Unpack graph properties\n",
    "            nodes = torch.tensor(graph_properties[0])                # [num_nodes]\n",
    "            edges = torch.tensor(graph_properties[1], dtype=torch.long)  # [2, num_edges]\n",
    "            coords = torch.tensor(graph_properties[2].T)             # [2, num_nodes]\n",
    "    \n",
    "            # Load datasets instead of attrs\n",
    "            gene_absence_presence_matrix = grp[\"gene_absence_presence_matrix\"][()]\n",
    "            nucleotide_sequences = grp[\"nucleotide_sequences\"][()]\n",
    "            #children_gene_nodes_loss_events = grp[\"children_gene_nodes_loss_events\"][()]\n",
    "        \n",
    "            # Load HGT events (simplified)\n",
    "            hgt_events = {}\n",
    "            hgt_grp_simpl = grp[\"nodes_hgt_events_simplified\"]\n",
    "            for site_id in hgt_grp_simpl.keys():\n",
    "                hgt_events[int(site_id)] = hgt_grp_simpl[site_id][()]\n",
    "\n",
    "            hot_encoded_nucleotide_sequences = one_hot_encode(nucleotide_sequences, gene_absence_presence_matrix, gene_length)\n",
    "\n",
    "            # Fill the remaining nodes with zeros.\n",
    "            pad_rows = len(nodes) - len(nucleotide_sequences)\n",
    "            pad = torch.zeros((pad_rows, hot_encoded_nucleotide_sequences.shape[1]), dtype=hot_encoded_nucleotide_sequences.dtype)\n",
    "            hot_encoded_nucleotide_sequences = torch.cat([hot_encoded_nucleotide_sequences, pad], dim=0)\n",
    "        \n",
    "            G = nx.DiGraph()\n",
    "            \n",
    "            # F√ºge Knoten hinzu (optional mit Koordinaten als Attribut)\n",
    "            for i, node_id in enumerate(nodes.tolist()):\n",
    "                G.add_node(node_id, node_time = coords[:, i].tolist()[5])\n",
    "            \n",
    "            # F√ºge Kanten hinzu\n",
    "            edge_list = edges.tolist()\n",
    "            for src, dst in zip(edge_list[0], edge_list[1]):\n",
    "                G.add_edge(src, dst)\n",
    "            \n",
    "            # Collect all recipient_parent_nodes from all sites\n",
    "            recipient_parent_nodes = set()\n",
    "            for site_id in hgt_grp_simpl.keys():\n",
    "                arr = hgt_grp_simpl[site_id][()]  # load dataset as numpy structured array\n",
    "                recipient_parent_nodes.update(arr[\"recipient_child_node\"].tolist())\n",
    "            \n",
    "            # Build theta_gains: 1 if node is in recipient_parent_nodes, else 0\n",
    "            theta_gains = torch.tensor(\n",
    "                [1 if node in recipient_parent_nodes else 0 for node in range(len(G.nodes))],\n",
    "                dtype=torch.long\n",
    "            )\n",
    "\n",
    "            level = {n: 0 for n in G.nodes}  # Leaves haben Level 0\n",
    "            \n",
    "            # 3. Topologische Sortierung (damit Kinder vor Eltern behandelt werden)\n",
    "            for node in reversed(list(nx.topological_sort(G))):\n",
    "                successors = list(G.successors(node))\n",
    "                if successors:\n",
    "                    level[node] = 1 + max(level[s] for s in successors)\n",
    "            \n",
    "            # 4. Level als Attribut setzen\n",
    "            nx.set_node_attributes(G, level, \"level\")\n",
    "\n",
    "            ### Add candidate egdes, i.e. potential hgt edges:\n",
    "        \n",
    "            # Hole alle Knoten und ihre Zeiten\n",
    "            node_times = {n: G.nodes[n]['node_time'] for n in G.nodes}\n",
    "            sorted_nodes = sorted(node_times.keys(), key=lambda n: node_times[n])\n",
    "\n",
    "            # Sort the level and node_times from 0 to max_node_id and not in G.nodes order:\n",
    "            node_times = torch.tensor([node_times[n] for n in sorted_nodes], dtype=torch.float)\n",
    "            node_levels = torch.tensor([level[n] for n in sorted_nodes], dtype=torch.float)\n",
    "\n",
    "            # F√ºge Kanten hinzu\n",
    "            existing_edges = set(zip(edges[0].tolist(), edges[1].tolist()))\n",
    "            candidate_edges = []\n",
    "            for i, src in enumerate(sorted_nodes):\n",
    "                t_src = node_times[src]\n",
    "                for dst in sorted_nodes[i+1:]:  # nur sp√§tere Knoten\n",
    "                    t_dst = node_times[dst]\n",
    "                    if t_dst > t_src:\n",
    "                        if (dst, src) not in existing_edges:  # vermeidet doppelte\n",
    "                            #G.add_edge(src, dst)\n",
    "                            candidate_edges.append((dst, src))\n",
    "            if candidate_edges:  \n",
    "                candidate_edges = torch.tensor(candidate_edges, dtype=torch.long).t()\n",
    "            else:\n",
    "                candidate_edges = torch.empty((2,0), dtype=torch.long)\n",
    "                    \n",
    "            data = Data(\n",
    "                nucleotide_sequences = nucleotide_sequences,\n",
    "                hot_encoded_nucleotide_sequences = hot_encoded_nucleotide_sequences,       # Node Features [num_nodes, 2]\n",
    "                edge_index = edges[[1, 0], :],        # Edge Index [2, num_edges]\n",
    "                candidate_edges = candidate_edges[[1, 0], :],\n",
    "                y = theta_gains,            # Labels [num_nodes]\n",
    "                file = file,\n",
    "                G = G,\n",
    "                recipient_parent_nodes = recipient_parent_nodes,\n",
    "                gene_absence_presence_matrix = gene_absence_presence_matrix,\n",
    "                node_times = node_times,\n",
    "                node_levels = node_levels\n",
    "            )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "folder = \"/mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks\"\n",
    "\n",
    "files = [os.path.join(folder, f) for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "\n",
    "examples = load_file(random.choice(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90dfd27b-651d-4cb3-a875-0964099596cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5508 Dateien erfolgreich geladen.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [9] at entry 0 and [5] at entry 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 310\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# =============================\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# üîπ Training starten\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# =============================\u001b[39;00m\n\u001b[1;32m    309\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 310\u001b[0m final_val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_with_val\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_of_Data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[59], line 262\u001b[0m, in \u001b[0;36mtrain_with_val\u001b[0;34m(model, dataset, optimizer, device, epochs, val_ratio, batch_size)\u001b[0m\n\u001b[1;32m    259\u001b[0m y \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    261\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 262\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# model.forward nutzt data.G\u001b[39;00m\n\u001b[1;32m    263\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, y_edges)\n\u001b[1;32m    264\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[59], line 139\u001b[0m, in \u001b[0;36mTreeEdgeCorrectionModel.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    137\u001b[0m theta \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(hot\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Node embeddings\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m h0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_levels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgene_present\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (N, D)\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# GNN over existing tree edges\u001b[39;00m\n\u001b[1;32m    141\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgnn(h0, data\u001b[38;5;241m.\u001b[39medge_index\u001b[38;5;241m.\u001b[39mto(hot\u001b[38;5;241m.\u001b[39mdevice))\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[59], line 58\u001b[0m, in \u001b[0;36mNodeFeatureEncoder.forward\u001b[0;34m(self, hot_seq_flat, node_times, node_levels, gene_present)\u001b[0m\n\u001b[1;32m     56\u001b[0m seq_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_encoder(hot_seq_flat)  \u001b[38;5;66;03m# (N, D1)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Stack numeric features (unsqueeze falls n√∂tig)\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m numeric \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_times\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnode_levels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgene_present\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#theta_gain.float()\u001b[39;49;00m\n\u001b[1;32m     63\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([seq_emb, numeric], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [9] at entry 0 and [5] at entry 2"
     ]
    }
   ],
   "source": [
    "# Ben√∂tigte Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, GCNConv, TransformerConv, global_mean_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# --------------------------\n",
    "# 1) Sequence Encoder (1D-CNN; alternative: tiny Transformer)\n",
    "# --------------------------\n",
    "class SeqEncoderCNN(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim=128, kernel_sizes=[3,5,7], dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(in_channels=input_dim, out_channels=emb_dim, kernel_size=k, padding=k//2)\n",
    "            for k in kernel_sizes\n",
    "        ])\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(len(kernel_sizes)*emb_dim, emb_dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x_seq_flat):\n",
    "        # x_seq_flat: [num_nodes, gene_length * alphabet_size]  (wie dein hot_encoded batch_flat)\n",
    "        # Wir m√ºssen es wieder in (B, C, L)\n",
    "        # Du musst gene_length und alphabet_size kennen (hier Annahme: 300, 5). Passen falls n√∂tig.\n",
    "        B = x_seq_flat.size(0)\n",
    "        # Beispielwerte, pass an:\n",
    "        gene_length = 300\n",
    "        alphabet_size = x_seq_flat.size(1) // gene_length\n",
    "        x = x_seq_flat.view(B, gene_length, alphabet_size).permute(0,2,1)  # (B, C, L)\n",
    "        conv_outs = []\n",
    "        for conv in self.convs:\n",
    "            y = F.relu(conv(x))\n",
    "            y = self.pool(y).squeeze(-1)  # (B, out_channels)\n",
    "            conv_outs.append(y)\n",
    "        y = torch.cat(conv_outs, dim=1)\n",
    "        y = self.drop(F.relu(self.fc(y)))\n",
    "        return y  # (B, emb_dim)\n",
    "\n",
    "# --------------------------\n",
    "# 2) Node Feature encoder: kombiniert seq-emb + numeric features\n",
    "# --------------------------\n",
    "class NodeFeatureEncoder(nn.Module):\n",
    "    def __init__(self, node_emb_dim=128):\n",
    "        super().__init__()\n",
    "        self.seq_encoder = SeqEncoderCNN(input_dim=5, emb_dim=node_emb_dim//2)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(node_emb_dim//2 + 3, node_emb_dim),  # 3 numerische features: time, level, gene_present\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(node_emb_dim)\n",
    "        )\n",
    "    def forward(self, hot_seq_flat, node_times, node_levels, gene_present):\n",
    "        seq_emb = self.seq_encoder(hot_seq_flat)  # (N, D1)\n",
    "        # Stack numeric features (unsqueeze falls n√∂tig)\n",
    "        numeric = torch.stack([\n",
    "            node_times.float(),\n",
    "            node_levels.float(),\n",
    "            gene_present.float(),\n",
    "            #theta_gain.float()\n",
    "        ], dim=1)\n",
    "        x = torch.cat([seq_emb, numeric], dim=1)\n",
    "        return self.mlp(x)  # (N, node_emb_dim)\n",
    "\n",
    "# --------------------------\n",
    "# 3) GNN Encoder (operates on existing tree edges)\n",
    "# --------------------------\n",
    "class GNNEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=128, num_layers=3, use_transformer=False):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_c = in_dim if i==0 else hidden_dim\n",
    "            if use_transformer:\n",
    "                conv = TransformerConv(in_c, hidden_dim//1, heads=4, concat=False)\n",
    "            else:\n",
    "                conv = GATConv(in_c, hidden_dim//4, heads=4, concat=True)  # outputs hidden_dim\n",
    "            self.convs.append(conv)\n",
    "            self.bn = nn.LayerNorm(hidden_dim)\n",
    "        self.out_bn = nn.LayerNorm(hidden_dim)\n",
    "    def forward(self, x, edge_index):\n",
    "        h = x\n",
    "        for conv in self.convs:\n",
    "            h = conv(h, edge_index)\n",
    "            h = F.relu(h)\n",
    "            h = self.bn(h)\n",
    "        return self.out_bn(h)\n",
    "\n",
    "# --------------------------\n",
    "# 4) Edge classifier\n",
    "# --------------------------\n",
    "class EdgeClassifier(nn.Module):\n",
    "    def __init__(self, node_emb_dim, hidden=32):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(node_emb_dim*2 + 3, hidden),  # src||dst||3 pair-features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, 1)\n",
    "        )\n",
    "    def forward(self, h_src, h_dst, pair_feats):\n",
    "        # h_src, h_dst: (E, D)\n",
    "        x = torch.cat([h_src, h_dst, pair_feats], dim=1)\n",
    "        return self.mlp(x).squeeze(-1)  # logits (E,)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 5) Full Model wrapper\n",
    "# --------------------------\n",
    "class TreeEdgeCorrectionModel(nn.Module):\n",
    "    def __init__(self, node_emb_dim=32, gnn_layers=3, use_transformer=False):\n",
    "        super().__init__()\n",
    "        self.node_encoder = NodeFeatureEncoder(node_emb_dim=node_emb_dim)\n",
    "        self.gnn = GNNEncoder(node_emb_dim, hidden_dim=node_emb_dim, num_layers=gnn_layers, use_transformer=use_transformer)\n",
    "        self.edge_clf = EdgeClassifier(node_emb_dim, hidden=node_emb_dim)\n",
    "    def forward(self, data):\n",
    "        # Unpack data (assume fields from your load_file)\n",
    "        hot = data.hot_encoded_nucleotide_sequences  # (N, gene_len*alpha)\n",
    "        node_times = data.node_times\n",
    "        node_levels = data.node_levels\n",
    "        # gene presence: from your matrix (first len(nucleotide_sequences) True or False)\n",
    "        # we assume data.gene_absence_presence_matrix is numpy array length = num_leaves (but pad rows were added)\n",
    "        \"\"\"\n",
    "        if isinstance(data.gene_absence_presence_matrix, np.ndarray):\n",
    "            gene_present = torch.tensor(data.gene_absence_presence_matrix, dtype=torch.float, device=hot.device)\n",
    "            # pad to full nodes if necessary:\n",
    "            if gene_present.shape[0] < hot.shape[0]:\n",
    "                pad = torch.zeros(hot.shape[0] - gene_present.shape[0], dtype=torch.float, device=hot.device)\n",
    "                gene_present = torch.cat([gene_present, pad], dim=0)\n",
    "        else:\n",
    "            # fallback zeros\n",
    "            gene_present = torch.zeros(hot.shape[0], device=hot.device)\n",
    "        \"\"\"\n",
    "        gene_present = torch.tensor(data.gene_absence_presence_matrix, dtype=torch.float, device=hot.device)\n",
    "        # theta_gain (you already stored)\n",
    "        theta = data.y.float().to(hot.device)\n",
    "        # Node embeddings\n",
    "        h0 = self.node_encoder(hot, node_times, node_levels, gene_present)  # (N, D)\n",
    "        # GNN over existing tree edges\n",
    "        h = self.gnn(h0, data.edge_index.to(hot.device))\n",
    "        # Prepare candidate edges\n",
    "        cand = data.candidate_edges.to(hot.device)  # shape [2, E_cand]\n",
    "        src_idx = cand[0]\n",
    "        dst_idx = cand[1]\n",
    "        h_src = h[src_idx]\n",
    "        h_dst = h[dst_idx]\n",
    "        # Pair features: time_diff_abs, time_sign, phylo_distance_placeholder\n",
    "        # time difference and sign:\n",
    "        time_src = node_times[src_idx]\n",
    "        time_dst = node_times[dst_idx]\n",
    "        time_diff = (time_dst - time_src).unsqueeze(1)       # (E,1) positive means dst is later (younger)\n",
    "        time_abs = torch.abs(time_diff)\n",
    "        time_sign = (time_diff > 0).float()\n",
    "        # optional phylogenetic distance: here we don't compute real tree distance; put zeros or compute shortest_path in G\n",
    "        # for speed, we use absolute level difference as proxy\n",
    "        level_src = node_levels[src_idx].unsqueeze(1)\n",
    "        level_dst = node_levels[dst_idx].unsqueeze(1)\n",
    "        level_diff = torch.abs(level_dst - level_src)\n",
    "        pair_feats = torch.cat([time_abs, time_sign, level_diff], dim=1)\n",
    "        logits = self.edge_clf(h_src, h_dst, pair_feats)\n",
    "        return logits  # (E_cand,)\n",
    "\n",
    "# --------------------------\n",
    "# 6) Edge labels construction (aus deinen HGT-Ereignissen)\n",
    "# --------------------------\n",
    "def build_edge_labels(data):\n",
    "    # data.candidate_edges: [2, E]\n",
    "    cand = data.candidate_edges.numpy().T.tolist()  # list of (src, dst)\n",
    "    cand_set = set((int(s), int(t)) for s,t in cand)\n",
    "    # Extract true HGT edges from your hgt_grp_simpl (data.recipient_parent_nodes only contains nodes, not full pairs)\n",
    "    # If you have raw hgt event arrays accessible somewhere in Data object, use them; else we try reconstruct entries from file\n",
    "    # Here we assume data has attribute 'G' and also stored raw events maybe in data.recipient_parent_nodes is set of recipients.\n",
    "    # We will create a label per candidate: positive if dst in recipient_parent_nodes (heuristic).\n",
    "    labels = []\n",
    "    recipients = getattr(data, 'recipient_parent_nodes', set())\n",
    "    for s,t in cand:\n",
    "        # Heuristic: if the child (t) is present in recipients set -> positive\n",
    "        if t in recipients:\n",
    "            labels.append(1)\n",
    "        else:\n",
    "            labels.append(0)\n",
    "    return torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "# --------------------------\n",
    "# 8) Beispiel: Initialisierung\n",
    "# --------------------------\n",
    "\"\"\"\n",
    "class TreeEdgeCorrectionModel(nn.Module):\n",
    "    def __init__(self, seq_input_dim, node_emb_dim=128, gnn_layers=3):\n",
    "        super().__init__()\n",
    "        self.input_lin = nn.Linear(seq_input_dim, node_emb_dim)\n",
    "        self.convs = nn.ModuleList([GCNConv(node_emb_dim, node_emb_dim) for _ in range(gnn_layers)])\n",
    "        self.output_lin = nn.Linear(node_emb_dim, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.hot_encoded_nucleotide_sequences, data.edge_index\n",
    "        x = torch.relu(self.input_lin(x))\n",
    "        for conv in self.convs:\n",
    "            x = torch.relu(conv(x, edge_index))\n",
    "        out = self.output_lin(x).squeeze(-1)   # [num_nodes]\n",
    "        return out\n",
    "\n",
    "\"\"\"\n",
    "# =============================\n",
    "# üîπ Train/Val Split & Training\n",
    "# =============================\n",
    "\n",
    "def train_val_split(dataset, val_ratio=0.2, seed=42):\n",
    "    \"\"\"Teilt Datensatz in Training und Validierung.\"\"\"\n",
    "    n_total = len(dataset)\n",
    "    n_val = int(n_total * val_ratio)\n",
    "    n_train = n_total - n_val\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    train_ds, val_ds = random_split(dataset, [n_train, n_val], generator=generator)\n",
    "    return train_ds, val_ds\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, device=\"cpu\"):\n",
    "    \"\"\"Berechnet den Loss auf dem Validierungsdatensatz.\"\"\"\n",
    "    model.eval()\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch)\n",
    "            y = batch.y.float()\n",
    "            min_len = min(out.shape[0], y.shape[0])\n",
    "            loss = criterion(out[:min_len], y[:min_len])\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def train_with_val(model, dataset, optimizer, device=\"cpu\", epochs=20, val_ratio=0.2, batch_size=8):\n",
    "    # ----------------------------\n",
    "    # 1) Train/Val Split\n",
    "    # ----------------------------\n",
    "    \n",
    "    train_ds, val_ds = train_val_split(dataset, val_ratio)\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # ----------------------------\n",
    "    # 2) Training Loop\n",
    "    # ----------------------------\n",
    "    model.to(device)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_list in train_loader:\n",
    "            data = batch_list[0]  # Data-Objekt\n",
    "            hot = data.hot_encoded_nucleotide_sequences.to(device)\n",
    "            edge_index = data.edge_index.to(device)\n",
    "            candidate_edges = data.candidate_edges.to(device)\n",
    "            y_edges = build_edge_labels(data).to(device)\n",
    "            y = data.y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)  # model.forward nutzt data.G\n",
    "            loss = criterion(out, y_edges)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        val_loss = evaluate(model, val_loader, device=device)\n",
    "        print(f\"Epoch {epoch:03d} | Train-Loss: {avg_train_loss:.4f} | Val-Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3) Endg√ºltiger Validierungsfehler\n",
    "    # ----------------------------\n",
    "    final_val_loss = evaluate(model, val_loader, device=device)\n",
    "    print(f\"\\n‚úÖ Endg√ºltiger Validierungs-Loss: {final_val_loss:.4f}\")\n",
    "    return final_val_loss\n",
    "\n",
    "# =============================\n",
    "# üîπ Dateien laden\n",
    "# =============================\n",
    "folder = \"/mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks\"\n",
    "\n",
    "files = [os.path.join(folder, f) for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "\n",
    "\"\"\"\n",
    "list_of_Data = []\n",
    "for f in files:\n",
    "    try:\n",
    "        d = load_file(f)\n",
    "        list_of_Data.append(d)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden von {f}: {e}\")\n",
    "\"\"\"\n",
    "\n",
    "print(f\"{len(list_of_Data)} Dateien erfolgreich geladen.\")\n",
    "\n",
    "# =============================\n",
    "# üîπ Model, Optimizer\n",
    "# =============================\n",
    "gene_length = 300\n",
    "seq_input_dim = gene_length * 5\n",
    "model = TreeEdgeCorrectionModel(node_emb_dim=128, gnn_layers=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# =============================\n",
    "# üîπ Training starten\n",
    "# =============================\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "final_val_loss = train_with_val(model, list_of_Data, optimizer, device=device, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e908310-6b3b-495c-89cf-0a49e317b275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13654/1371238802.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  hot_encoded_nucleotide_sequences = torch.tensor(d.hot_encoded_nucleotide_sequences, dtype=torch.float),\n",
      "/tmp/ipykernel_13654/1371238802.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edge_index = torch.tensor(d.edge_index, dtype=torch.long),\n",
      "/tmp/ipykernel_13654/1371238802.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  candidate_edges = torch.tensor(d.candidate_edges, dtype=torch.long),\n",
      "/tmp/ipykernel_13654/1371238802.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(d.y, dtype=torch.float)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0a41f080-0396-434c-a8a4-ed9fc46dc405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TreeEdgeCorrectionModel(\n",
       "  (input_lin): Linear(in_features=1500, out_features=128, bias=True)\n",
       "  (convs): ModuleList()\n",
       "  (output_lin): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79f6daed-ff30-466e-a7b6-2644cd4282ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.992\n",
      "Precision: 0.986\n",
      "Recall:    0.787\n",
      "F1-Score:  0.875\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "def evaluate_predictions(model, dataloader, device=\"cpu\", threshold=0.5):\n",
    "    model.eval()\n",
    "    y_true_all = []\n",
    "    y_pred_all = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = batch.to(device)\n",
    "            out = model(batch)                 # Logits pro Node\n",
    "            probs = torch.sigmoid(out)         # Wahrscheinlichkeiten [0,1]\n",
    "            preds = (probs > threshold).long() # 1 = vorhergesagt als \"hgt\"\n",
    "            #print(torch.round(probs * 100) / 100)\n",
    "            \n",
    "            # Sicherstellen, dass L√§ngen passen\n",
    "            min_len = min(len(batch.y), len(preds))\n",
    "            y_true_all.extend(batch.y[:min_len].cpu().numpy())\n",
    "            y_pred_all.extend(preds[:min_len].cpu().numpy())\n",
    "    \n",
    "    y_true_all = torch.tensor(y_true_all)\n",
    "    y_pred_all = torch.tensor(y_pred_all)\n",
    "    \n",
    "    acc = accuracy_score(y_true_all, y_pred_all)\n",
    "    prec = precision_score(y_true_all, y_pred_all, zero_division=0)\n",
    "    rec = recall_score(y_true_all, y_pred_all, zero_division=0)\n",
    "    f1 = f1_score(y_true_all, y_pred_all, zero_division=0)\n",
    "    \n",
    "    print(f\"Accuracy:  {acc:.3f}\")\n",
    "    print(f\"Precision: {prec:.3f}\")\n",
    "    print(f\"Recall:    {rec:.3f}\")\n",
    "    print(f\"F1-Score:  {f1:.3f}\")\n",
    "    \n",
    "    return y_true_all, y_pred_all\n",
    "\n",
    "y_true, y_pred = evaluate_predictions(model, dataloader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "028eeedb-8489-42a2-99fb-b64319dc59f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(edge_index=[2, 32], y=[36], nucleotide_sequences=[4], hot_encoded_nucleotide_sequences=[36, 1500], candidate_edges=[8, 18], file=[4], G=[4], recipient_parent_nodes=[4], gene_absence_presence_matrix=[4], batch=[36], ptr=[5])\n",
      "DataBatch(edge_index=[2, 32], y=[36], nucleotide_sequences=[4], hot_encoded_nucleotide_sequences=[36, 1500], candidate_edges=[8, 18], file=[4], G=[4], recipient_parent_nodes=[4], gene_absence_presence_matrix=[4], batch=[36], ptr=[5])\n",
      "DataBatch(edge_index=[2, 32], y=[36], nucleotide_sequences=[4], hot_encoded_nucleotide_sequences=[36, 1500], candidate_edges=[8, 18], file=[4], G=[4], recipient_parent_nodes=[4], gene_absence_presence_matrix=[4], batch=[36], ptr=[5])\n",
      "DataBatch(edge_index=[2, 32], y=[36], nucleotide_sequences=[4], hot_encoded_nucleotide_sequences=[36, 1500], candidate_edges=[8, 18], file=[4], G=[4], recipient_parent_nodes=[4], gene_absence_presence_matrix=[4], batch=[36], ptr=[5])\n",
      "DataBatch(edge_index=[2, 32], y=[36], nucleotide_sequences=[4], hot_encoded_nucleotide_sequences=[36, 1500], candidate_edges=[8, 18], file=[4], G=[4], recipient_parent_nodes=[4], gene_absence_presence_matrix=[4], batch=[36], ptr=[5])\n",
      "DataBatch(edge_index=[2, 32], y=[36], nucleotide_sequences=[4], hot_encoded_nucleotide_sequences=[36, 1500], candidate_edges=[8, 18], file=[4], G=[4], recipient_parent_nodes=[4], gene_absence_presence_matrix=[4], batch=[36], ptr=[5])\n",
      "DataBatch(edge_index=[2, 32], y=[36], nucleotide_sequences=[4], hot_encoded_nucleotide_sequences=[36, 1500], candidate_edges=[8, 18], file=[4], G=[4], recipient_parent_nodes=[4], gene_absence_presence_matrix=[4], batch=[36], ptr=[5])\n",
      "DataBatch(edge_index=[2, 32], y=[36], nucleotide_sequences=[4], hot_encoded_nucleotide_sequences=[36, 1500], candidate_edges=[8, 18], file=[4], G=[4], recipient_parent_nodes=[4], gene_absence_presence_matrix=[4], batch=[36], ptr=[5])\n",
      "DataBatch(edge_index=[2, 32], y=[36], nucleotide_sequences=[4], hot_encoded_nucleotide_sequences=[36, 1500], candidate_edges=[8, 18], file=[4], G=[4], recipient_parent_nodes=[4], gene_absence_presence_matrix=[4], batch=[36], ptr=[5])\n",
      "DataBatch(edge_index=[2, 32], y=[36], nucleotide_sequences=[4], hot_encoded_nucleotide_sequences=[36, 1500], candidate_edges=[8, 18], file=[4], G=[4], recipient_parent_nodes=[4], gene_absence_presence_matrix=[4], batch=[36], ptr=[5])\n",
      "DataBatch(edge_index=[2, 32], y=[36], nucleotide_sequences=[4], hot_encoded_nucleotide_sequences=[36, 1500], candidate_edges=[8, 18], file=[4], G=[4], recipient_parent_nodes=[4], gene_absence_presence_matrix=[4], batch=[36], ptr=[5])\n",
      "DataBatch(edge_index=[2, 32], y=[36], nucleotide_sequences=[4], hot_encoded_nucleotide_sequences=[36, 1500], candidate_edges=[8, 18], file=[4], G=[4], recipient_parent_nodes=[4], gene_absence_presence_matrix=[4], batch=[36], ptr=[5])\n",
      "DataBatch(edge_index=[2, 32], y=[36], nucleotide_sequences=[4], hot_encoded_nucleotide_sequences=[36, 1500], candidate_edges=[8, 18], file=[4], G=[4], recipient_parent_nodes=[4], gene_absence_presence_matrix=[4], batch=[36], ptr=[5])\n",
      "DataBatch(edge_index=[2, 8], y=[9], nucleotide_sequences=[1], hot_encoded_nucleotide_sequences=[9, 1500], candidate_edges=[2, 18], file=[1], G=[1], recipient_parent_nodes=[1], gene_absence_presence_matrix=[1], batch=[9], ptr=[2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kippnich/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch_geometric/data/storage.py:452: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_index', 'y', 'candidate_edges', 'file', 'hot_encoded_nucleotide_sequences', 'nucleotide_sequences', 'recipient_parent_nodes', 'gene_absence_presence_matrix', 'G'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for i in dataloader:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
