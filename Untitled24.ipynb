{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "efb0e7a1-34a4-4bc5-9dd5-e53012d4572f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24794/615299766.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edges = torch.tensor(graph_properties[1], dtype=torch.long)  # [2, num_edges]\n",
      "/tmp/ipykernel_24794/615299766.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coords = torch.tensor(graph_properties[2].T)             # [2, num_nodes]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fehler beim Laden von /mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks/simulation_186274.h5: cannot reshape array of size 0 into shape (0,newaxis)\n",
      "99 Dateien erfolgreich geladen.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import h5py\n",
    "import pickle\n",
    "import torch\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, ReLU, Dropout, BatchNorm1d\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "def one_hot_encode(sequences, gene_present, gene_length, alphabet=['A','C','T','G','-']):\n",
    "    \"\"\"\n",
    "    sequences: List of strings (DNA sequences)\n",
    "    gene_present: np.array(bool) oder Torch Tensor, gleiche Länge wie sequences\n",
    "    gene_length: int, fixe Länge für das Hot-Encoding\n",
    "    alphabet: list, Zeichenalphabet\n",
    "    \"\"\"\n",
    "    num_samples = len(sequences)\n",
    "    num_chars = len(alphabet)\n",
    "    char_to_idx = {c:i for i,c in enumerate(alphabet)}\n",
    "    sequences_str = [s.decode('utf-8') for s in sequences]\n",
    "    gene_present = np.array(gene_present, dtype=bool)\n",
    "    \n",
    "    # 1️⃣ Leere Batch-Matrix vorbereiten: (num_samples, gene_length, num_chars)\n",
    "    batch = np.zeros((num_samples, gene_length, num_chars), dtype=np.float32)\n",
    "    \n",
    "    # 2️⃣ Hot-Encode alle Sequenzen\n",
    "    for i, seq in enumerate(sequences_str):\n",
    "        if gene_present[i]:\n",
    "            L = min(len(seq), gene_length)  # abschneiden\n",
    "            for j, c in enumerate(seq[:L]):\n",
    "                if c in char_to_idx:\n",
    "                    batch[i, j, char_to_idx[c]] = 1.0\n",
    "        elif gene_present[i] == 0:\n",
    "            batch[i, :, :] = -1.0\n",
    "            \n",
    "    # 3️⃣ Zufällige, aber konsistente Spaltenpermutation\n",
    "    perm = np.random.permutation(gene_length)\n",
    "    batch = batch[:, perm, :]\n",
    "    \n",
    "    # 4️⃣ Optional: Flatten zu Vektor (num_samples, gene_length*num_chars)\n",
    "    batch_flat = batch.reshape(num_samples, -1)\n",
    "    \n",
    "    return torch.tensor(batch_flat)  # shape: (num_samples, gene_length*num_chars)\n",
    "\n",
    "def load_file(file):\n",
    "\n",
    "    gene_length = 300\n",
    "    #nucleotide_mutation_rate = 0.1\n",
    "    \n",
    "    with h5py.File(file, \"r\") as f:\n",
    "            grp = f[\"results\"]\n",
    "            # Load graph_properties (pickle stored in dataset)\n",
    "            graph_properties = pickle.loads(grp[\"graph_properties\"][()])\n",
    "    \n",
    "            # Unpack graph properties\n",
    "            nodes = torch.tensor(graph_properties[0])                # [num_nodes]\n",
    "            edges = torch.tensor(graph_properties[1], dtype=torch.long)  # [2, num_edges]\n",
    "            coords = torch.tensor(graph_properties[2].T)             # [2, num_nodes]\n",
    "    \n",
    "            # Load datasets instead of attrs\n",
    "            gene_absence_presence_matrix = grp[\"gene_absence_presence_matrix\"][()]\n",
    "            nucleotide_sequences = grp[\"nucleotide_sequences\"][()]\n",
    "            #children_gene_nodes_loss_events = grp[\"children_gene_nodes_loss_events\"][()]\n",
    "        \n",
    "            # Load HGT events (simplified)\n",
    "            hgt_events = {}\n",
    "            hgt_grp_simpl = grp.get(\"nodes_hgt_events_simplified\", None)\n",
    "            if hgt_grp_simpl is not None:\n",
    "                for site_id in hgt_grp_simpl.keys():\n",
    "                    hgt_events[int(site_id)] = hgt_grp_simpl[site_id][()]\n",
    "            else:\n",
    "                hgt_events = {}\n",
    "\n",
    "            hot_encoded_nucleotide_sequences = one_hot_encode(nucleotide_sequences, gene_absence_presence_matrix, gene_length)\n",
    "\n",
    "            # Fill the remaining nodes with zeros.\n",
    "            pad_rows = len(nodes) - len(nucleotide_sequences)\n",
    "            pad = torch.zeros((pad_rows, hot_encoded_nucleotide_sequences.shape[1]), dtype=hot_encoded_nucleotide_sequences.dtype)\n",
    "            hot_encoded_nucleotide_sequences = torch.cat([hot_encoded_nucleotide_sequences, pad], dim=0)\n",
    "        \n",
    "            G = nx.DiGraph()\n",
    "            \n",
    "            # Füge Knoten hinzu (optional mit Koordinaten als Attribut)\n",
    "            node_id_list = nodes.tolist()\n",
    "            for i, node_id in enumerate(node_id_list):\n",
    "                # coords[:, i] exists; using index 5 as in your code for node_time\n",
    "                G.add_node(node_id, node_time = coords[:, i].tolist()[5])\n",
    "            \n",
    "            # Füge Kanten hinzu\n",
    "            edge_list = edges.tolist()\n",
    "            for src, dst in zip(edge_list[0], edge_list[1]):\n",
    "                G.add_edge(src, dst)\n",
    "            \n",
    "            # Collect all recipient_parent_nodes from all sites (if present)\n",
    "            recipient_parent_nodes = set()\n",
    "            if hgt_grp_simpl is not None:\n",
    "                for site_id in hgt_grp_simpl.keys():\n",
    "                    arr = hgt_grp_simpl[site_id][()]  # load dataset as numpy structured array\n",
    "                    # try robust field name for recipient child\n",
    "                    if 'recipient_child_node' in arr.dtype.names:\n",
    "                        recipient_parent_nodes.update(arr[\"recipient_child_node\"].tolist())\n",
    "                    elif 'recipient_child' in arr.dtype.names:\n",
    "                        recipient_parent_nodes.update(arr[\"recipient_child\"].tolist())\n",
    "\n",
    "            # Build theta_gains: 1 if node is in recipient_parent_nodes, else 0\n",
    "            theta_gains = torch.tensor(\n",
    "                [1 if node in recipient_parent_nodes else 0 for node in range(len(G.nodes))],\n",
    "                dtype=torch.long\n",
    "            )\n",
    "\n",
    "            level = {n: 0 for n in G.nodes}  # Leaves haben Level 0\n",
    "            \n",
    "            # 3. Topologische Sortierung (damit Kinder vor Eltern behandelt werden)\n",
    "            for node in reversed(list(nx.topological_sort(G))):\n",
    "                successors = list(G.successors(node))\n",
    "                if successors:\n",
    "                    level[node] = 1 + max(level[s] for s in successors)\n",
    "            \n",
    "            # 4. Level als Attribut setzen\n",
    "            nx.set_node_attributes(G, level, \"level\")\n",
    "\n",
    "            ### Add candidate egdes, i.e. potential hgt edges:\n",
    "        \n",
    "            # Hole alle Knoten und ihre Zeiten\n",
    "            node_times = {n: G.nodes[n]['node_time'] for n in G.nodes}\n",
    "            sorted_nodes = sorted(node_times.keys(), key=lambda n: node_times[n])\n",
    "\n",
    "            # Sort the level and node_times from 0 to max_node_id and not in G.nodes order:\n",
    "            node_times = torch.tensor([node_times[n] for n in sorted_nodes], dtype=torch.float)\n",
    "            node_levels = torch.tensor([level[n] for n in sorted_nodes], dtype=torch.float)\n",
    "\n",
    "            # Füge Kanten hinzu\n",
    "            existing_edges = set(zip(edges[0].tolist(), edges[1].tolist()))\n",
    "            candidate_edges = []\n",
    "            for i, src in enumerate(sorted_nodes):\n",
    "                t_src = node_times[src]\n",
    "                for dst in sorted_nodes[i+1:]:  # nur spätere Knoten\n",
    "                    t_dst = node_times[dst]\n",
    "                    if t_dst > t_src:\n",
    "                        if (dst, src) not in existing_edges:  # vermeidet doppelte\n",
    "                            #G.add_edge(src, dst)\n",
    "                            candidate_edges.append((dst, src))\n",
    "            if candidate_edges:  \n",
    "                candidate_edges = torch.tensor(candidate_edges, dtype=torch.long).t()\n",
    "            else:\n",
    "                candidate_edges = torch.empty((2,0), dtype=torch.long)\n",
    "                    \n",
    "            data = Data(\n",
    "                #nucleotide_sequences = nucleotide_sequences,\n",
    "                hot_encoded_nucleotide_sequences = hot_encoded_nucleotide_sequences,       # Node Features [num_nodes, 2]\n",
    "                edge_index = edges[[1, 0], :],        # Edge Index [2, num_edges]\n",
    "                candidate_edges = candidate_edges[[1, 0], :],\n",
    "                y = theta_gains,            # Labels [num_nodes]\n",
    "                file = file,\n",
    "                G = G,\n",
    "                #recipient_parent_nodes = recipient_parent_nodes,\n",
    "                gene_absence_presence_matrix = gene_absence_presence_matrix,\n",
    "                node_times = node_times,\n",
    "                node_levels = node_levels,\n",
    "                hgt_events = hgt_events,\n",
    "            )\n",
    "\n",
    "            # -----------------------------\n",
    "            # Compute label maps directly here (no external call)\n",
    "            # -----------------------------\n",
    "            recip_label_map = {}\n",
    "            donor_map = {}\n",
    "            # initialize maps for internal nodes with exactly 2 children\n",
    "            for n in G.nodes:\n",
    "                children = list(G.successors(n))\n",
    "                if len(children) == 2:\n",
    "                    recip_label_map[n] = (0, None)\n",
    "                    donor_map[n] = None\n",
    "\n",
    "            # parse hgt_events and fill maps\n",
    "            if hgt_events:\n",
    "                for site_id, arr in hgt_events.items():\n",
    "                    # arr is expected to be a numpy structured array\n",
    "                    try:\n",
    "                        ln = len(arr)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    for i in range(ln):\n",
    "                        # robust field extraction\n",
    "                        rec_parent = None\n",
    "                        rec_child = None\n",
    "                        donor_parent = None\n",
    "                        if 'recipient_parent_node' in arr.dtype.names:\n",
    "                            rec_parent = int(arr[i]['recipient_parent_node'])\n",
    "                        elif 'recipient_parent' in arr.dtype.names:\n",
    "                            rec_parent = int(arr[i]['recipient_parent'])\n",
    "                        if 'recipient_child_node' in arr.dtype.names:\n",
    "                            rec_child = int(arr[i]['recipient_child_node'])\n",
    "                        elif 'recipient_child' in arr.dtype.names:\n",
    "                            rec_child = int(arr[i]['recipient_child'])\n",
    "                        if 'donor_parent_node' in arr.dtype.names:\n",
    "                            donor_parent = int(arr[i]['donor_parent_node'])\n",
    "                        elif 'donor_parent' in arr.dtype.names:\n",
    "                            donor_parent = int(arr[i]['donor_parent'])\n",
    "\n",
    "                        if rec_parent is None:\n",
    "                            continue\n",
    "                        if rec_parent in recip_label_map:\n",
    "                            recip_label_map[rec_parent] = (1, rec_child)\n",
    "                            donor_map[rec_parent] = donor_parent\n",
    "\n",
    "            # attach precomputed label maps to data\n",
    "            data.recip_label_map = recip_label_map\n",
    "            data.donor_map = donor_map\n",
    "\n",
    "    return data\n",
    "\n",
    "folder = \"/mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks\"\n",
    "\n",
    "files = [os.path.join(folder, f) for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "\n",
    "if len(files) > 100:\n",
    "    files = random.sample(files, 100)\n",
    "\n",
    "list_of_Data = []\n",
    "for f in files:\n",
    "    try:\n",
    "        d = load_file(f)\n",
    "        list_of_Data.append(d)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden von {f}: {e}\")\n",
    "\n",
    "# example = load_file(random.choice(files))\n",
    "\n",
    "print(f\"{len(list_of_Data)} Dateien erfolgreich geladen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1d246db-249e-4e02-82dd-b414c0bae3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 90 graphs | Validation on 9 graphs | pos_weight=21.500\n",
      "Epoch 1/4 | time 0.2s\n",
      " TRAIN loss 1.2130 (rec 1.2130, don 0.0000) Prec/Rec/F1 0.077/0.062/0.069 | avg_expected_donor_dist 0.000\n",
      " VAL   loss 1.8585 (rec 1.8585, don 0.0000) Prec/Rec/F1 0.000/0.000/0.000 | avg_expected_donor_dist 0.000\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "[(5, 3), (5, 4), (8, 0), (8, 7), (7, 1), (7, 6), (6, 2), (6, 5)]\n",
      "{0: array([(7, 1, 1, 7, 6)],\n",
      "      dtype=[('recipient_parent_node', '<i4'), ('recipient_child_node', '<i4'), ('leaf', '<i4'), ('donor_parent_node', '<i4'), ('donor_child_node', '<i4')])}\n",
      "Predicted recipient: 7 1\n",
      "[(5, 3), (5, 4), (8, 0), (8, 7), (7, 1), (7, 6), (6, 2), (6, 5)]\n",
      "{0: array([(7, 1, 1, 7, 6)],\n",
      "      dtype=[('recipient_parent_node', '<i4'), ('recipient_child_node', '<i4'), ('leaf', '<i4'), ('donor_parent_node', '<i4'), ('donor_child_node', '<i4')])}\n",
      "[(5, 3), (5, 4), (8, 0), (8, 7), (7, 6), (7, 9), (6, 2), (6, 5), (9, 1)]\n",
      "[(8, 7), (8, 6), (7, 2), (7, 5), (6, 0), (6, 1), (5, 3), (5, 4)]\n",
      "{0: array([(5, 3, 3, 7, 5), (8, 1, 1, 7, 5)],\n",
      "      dtype=[('recipient_parent_node', '<i4'), ('recipient_child_node', '<i4'), ('leaf', '<i4'), ('donor_parent_node', '<i4'), ('donor_child_node', '<i4')])}\n",
      "[(8, 7), (8, 6), (7, 9), (7, 4), (6, 0), (6, 1), (9, 2), (9, 3)]\n",
      "[(5, 1), (5, 2), (8, 0), (8, 7), (7, 5), (7, 6), (6, 4), (6, 3)]\n",
      "{0: array([(6, 3, 3, 8, 7)],\n",
      "      dtype=[('recipient_parent_node', '<i4'), ('recipient_child_node', '<i4'), ('leaf', '<i4'), ('donor_parent_node', '<i4'), ('donor_child_node', '<i4')])}\n",
      "[(5, 1), (5, 2), (8, 7), (8, 9), (7, 5), (7, 4), (9, 0), (9, 3)]\n",
      "[(8, 0), (8, 7), (7, 1), (7, 6), (6, 2), (6, 5), (5, 3), (5, 4)]\n",
      "{0: array([(5, 4, 4, 8, 7), (6, 5, 3, 8, 7)],\n",
      "      dtype=[('recipient_parent_node', '<i4'), ('recipient_child_node', '<i4'), ('leaf', '<i4'), ('donor_parent_node', '<i4'), ('donor_child_node', '<i4')])}\n",
      "[(8, 0), (8, 9), (7, 1), (7, 6), (6, 2), (6, 3), (9, 7), (9, 4)]\n",
      "[(5, 3), (5, 4), (8, 0), (8, 7), (7, 1), (7, 6), (6, 5), (6, 2)]\n",
      "{0: array([(6, 2, 2, 8, 7), (7, 1, 1, 8, 7)],\n",
      "      dtype=[('recipient_parent_node', '<i4'), ('recipient_child_node', '<i4'), ('leaf', '<i4'), ('donor_parent_node', '<i4'), ('donor_child_node', '<i4')])}\n",
      "[(5, 3), (5, 4), (8, 0), (8, 9), (7, 1), (7, 5), (9, 7), (9, 2)]\n",
      "Epoch 2/4 | time 0.2s\n",
      " TRAIN loss 0.6620 (rec 0.6619, don 0.0015) Prec/Rec/F1 0.333/0.938/0.492 | avg_expected_donor_dist 0.417\n",
      " VAL   loss 2.1131 (rec 2.1131, don 0.0000) Prec/Rec/F1 0.000/0.000/0.000 | avg_expected_donor_dist 0.000\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "[(8, 7), (8, 6), (7, 3), (7, 4), (6, 0), (6, 5), (5, 1), (5, 2)]\n",
      "{0: array([(7, 4, 4, 8, 7)],\n",
      "      dtype=[('recipient_parent_node', '<i4'), ('recipient_child_node', '<i4'), ('leaf', '<i4'), ('donor_parent_node', '<i4'), ('donor_child_node', '<i4')])}\n",
      "[(8, 6), (8, 9), (6, 0), (6, 5), (5, 1), (5, 2), (9, 4), (9, 3)]\n",
      "[(8, 7), (8, 6), (7, 3), (7, 4), (6, 0), (6, 5), (5, 2), (5, 1)]\n",
      "{0: array([(5, 1, 1, 8, 6)],\n",
      "      dtype=[('recipient_parent_node', '<i4'), ('recipient_child_node', '<i4'), ('leaf', '<i4'), ('donor_parent_node', '<i4'), ('donor_child_node', '<i4')])}\n",
      "[(8, 6), (8, 9), (7, 3), (7, 4), (6, 0), (6, 2), (9, 7), (9, 1)]\n",
      "[(5, 1), (5, 2), (8, 0), (8, 7), (7, 5), (7, 6), (6, 3), (6, 4)]\n",
      "{0: array([(7, 6, 4, 8, 7), (7, 5, 2, 8, 7), (8, 0, 0, 7, 6)],\n",
      "      dtype=[('recipient_parent_node', '<i4'), ('recipient_child_node', '<i4'), ('leaf', '<i4'), ('donor_parent_node', '<i4'), ('donor_child_node', '<i4')])}\n",
      "Predicted recipient: 8 0\n",
      "[(5, 1), (5, 2), (8, 0), (8, 7), (7, 5), (7, 6), (6, 3), (6, 4)]\n",
      "{0: array([(7, 6, 4, 8, 7), (7, 5, 2, 8, 7), (8, 0, 0, 7, 6)],\n",
      "      dtype=[('recipient_parent_node', '<i4'), ('recipient_child_node', '<i4'), ('leaf', '<i4'), ('donor_parent_node', '<i4'), ('donor_child_node', '<i4')])}\n",
      "[(5, 1), (5, 2), (8, 7), (7, 6), (7, 9), (6, 3), (6, 4), (9, 5), (9, 0)]\n",
      "Epoch 3/4 | time 0.2s\n",
      " TRAIN loss 0.3210 (rec 0.3210, don 0.0000) Prec/Rec/F1 0.480/1.000/0.649 | avg_expected_donor_dist 0.000\n",
      " VAL   loss 4.2286 (rec 4.2286, don 0.0000) Prec/Rec/F1 0.000/0.000/0.000 | avg_expected_donor_dist 0.000\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "[(8, 7), (8, 6), (7, 3), (7, 4), (6, 0), (6, 5), (5, 1), (5, 2)]\n",
      "{0: array([(7, 4, 4, 8, 7)],\n",
      "      dtype=[('recipient_parent_node', '<i4'), ('recipient_child_node', '<i4'), ('leaf', '<i4'), ('donor_parent_node', '<i4'), ('donor_child_node', '<i4')])}\n",
      "[(8, 6), (8, 9), (6, 0), (6, 5), (5, 1), (5, 2), (9, 4), (9, 3)]\n",
      "[(8, 0), (8, 7), (7, 6), (7, 5), (6, 3), (6, 4), (5, 2), (5, 1)]\n",
      "{0: array([(6, 4, 4, 7, 5)],\n",
      "      dtype=[('recipient_parent_node', '<i4'), ('recipient_child_node', '<i4'), ('leaf', '<i4'), ('donor_parent_node', '<i4'), ('donor_child_node', '<i4')])}\n",
      "[(8, 0), (8, 7), (7, 9), (7, 3), (5, 2), (5, 1), (9, 5), (9, 4)]\n",
      "Epoch 4/4 | time 0.2s\n",
      " TRAIN loss 0.2391 (rec 0.2391, don 0.0000) Prec/Rec/F1 0.429/0.900/0.581 | avg_expected_donor_dist 0.000\n",
      " VAL   loss 3.5218 (rec 3.5218, don 0.0000) Prec/Rec/F1 0.000/0.000/0.000 | avg_expected_donor_dist 0.000\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import networkx as nx\n",
    "import copy\n",
    "\n",
    "# -------------------------\n",
    "# Model components\n",
    "# -------------------------\n",
    "\n",
    "def make_mlp(input_dim, hidden_dims=[256,128], output_dim=None, dropout=0.1):\n",
    "    \"\"\"\n",
    "    Build an MLP that uses LayerNorm instead of BatchNorm.\n",
    "    LayerNorm is stable for batch size 1.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    dims = [input_dim] + hidden_dims\n",
    "    for i in range(len(hidden_dims)):\n",
    "        layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "        # use LayerNorm over feature dimension\n",
    "        layers.append(nn.LayerNorm(dims[i+1]))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "    if output_dim is not None:\n",
    "        layers.append(nn.Linear(dims[-1], output_dim))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class RecipientFinder(nn.Module):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - hgt_logit: raw logit (no sigmoid) shape [B]\n",
    "      - which_logits: raw logits for left/right shape [B,2]\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_vec_dim, aux_dim=3, hidden=[512,256], dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.seq_vec_dim = seq_vec_dim\n",
    "        self.aux_dim = aux_dim\n",
    "        mlp_input = seq_vec_dim * 4 + aux_dim\n",
    "        self.mlp = make_mlp(mlp_input, hidden_dims=hidden, output_dim=None, dropout=dropout)\n",
    "        self.hgt_head = nn.Linear(hidden[-1], 1)    # logit\n",
    "        self.which_head = nn.Linear(hidden[-1], 2)  # logits\n",
    "\n",
    "    def forward(self, left_vec, right_vec, aux_feats):\n",
    "        # left_vec/right_vec: [B, D]\n",
    "        absdiff = torch.abs(left_vec - right_vec)\n",
    "        prod = left_vec * right_vec\n",
    "        x = torch.cat([left_vec, right_vec, absdiff, prod, aux_feats], dim=1)\n",
    "        h = self.mlp(x)\n",
    "        hgt_logit = self.hgt_head(h).squeeze(-1)         # [B]\n",
    "        which_logits = self.which_head(h)                # [B,2]\n",
    "        return hgt_logit, which_logits\n",
    "\n",
    "\n",
    "class DonorFinder(nn.Module):\n",
    "    \"\"\"\n",
    "    Scores candidate donors relative to a recipient vector.\n",
    "    Returns raw scores (logits) of shape [M]\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_vec_dim, aux_dim=3, hidden=[512,256], dropout=0.1):\n",
    "        super().__init__()\n",
    "        mlp_input = seq_vec_dim * 4 + aux_dim\n",
    "        self.score_mlp = make_mlp(mlp_input, hidden_dims=hidden, output_dim=1, dropout=dropout)\n",
    "\n",
    "    def forward(self, recipient_vec, donor_vecs, aux_feats):\n",
    "        # recipient_vec: [D] or [1,D]\n",
    "        if recipient_vec.dim() == 1:\n",
    "            r = recipient_vec.unsqueeze(0).expand(donor_vecs.shape[0], -1)\n",
    "        else:\n",
    "            r = recipient_vec.expand(donor_vecs.shape[0], -1)\n",
    "        absdiff = torch.abs(r - donor_vecs)\n",
    "        prod = r * donor_vecs\n",
    "        x = torch.cat([r, donor_vecs, absdiff, prod, aux_feats], dim=1)\n",
    "        scores = self.score_mlp(x).squeeze(-1)  # [M]\n",
    "        return scores\n",
    "\n",
    "\n",
    "class HGTDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Orchestrator that contains RecipientFinder and DonorFinder and provides utility compute_aggregates.\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_vec_dim, aux_recipient_dim=3, aux_donor_dim=3,\n",
    "                 rec_hidden=[16], donor_hidden=[16], hgt_threshold=0.5, topk_donors=1):\n",
    "        super().__init__()\n",
    "        self.recipient_finder = RecipientFinder(seq_vec_dim, aux_dim=aux_recipient_dim, hidden=rec_hidden)\n",
    "        self.donor_finder = DonorFinder(seq_vec_dim, aux_dim=aux_donor_dim, hidden=donor_hidden)\n",
    "        self.hgt_threshold = hgt_threshold\n",
    "        self.topk_donors = topk_donors\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_aggregates(G: nx.DiGraph, node_seq_matrix: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Compute aggregates such that agg[node_id] is valid for arbitrary integer node ids\n",
    "        (i.e., agg's first dimension has size max_node_id+1). Aggregation is elementwise MAX\n",
    "        over children (not sum), as requested.\n",
    "\n",
    "        Returns:\n",
    "            agg: Tensor [max_node_id+1, D] where rows for node ids not present are zeros.\n",
    "        \"\"\"\n",
    "        # node ids may not be contiguous or start at 0. We'll allocate by max node id.\n",
    "        nodes = list(G.nodes)\n",
    "        if len(nodes) == 0:\n",
    "            return node_seq_matrix.clone()\n",
    "\n",
    "        max_node_id = int(max(nodes))\n",
    "        D = node_seq_matrix.shape[1]\n",
    "        device = node_seq_matrix.device\n",
    "        dtype = node_seq_matrix.dtype\n",
    "\n",
    "        # allocate agg with rows 0..max_node_id\n",
    "        agg = torch.zeros((max_node_id + 1, D), dtype=dtype, device=device)\n",
    "\n",
    "        # copy available rows from node_seq_matrix into agg by node id when possible.\n",
    "        n_rows = node_seq_matrix.shape[0]\n",
    "        for n in nodes:\n",
    "            if isinstance(n, int) and n < n_rows:\n",
    "                agg[n] = node_seq_matrix[n].to(device)\n",
    "            else:\n",
    "                agg[n] = agg[n]  # noop\n",
    "\n",
    "        # bottom-up aggregation: children before parent\n",
    "        topo = list(nx.topological_sort(G))\n",
    "        for node in reversed(topo):\n",
    "            children = list(G.successors(node))\n",
    "            if len(children) == 0:\n",
    "                continue\n",
    "            child_vecs = []\n",
    "            for c in children:\n",
    "                if not isinstance(c, int) or c > max_node_id:\n",
    "                    continue\n",
    "                child_vecs.append(agg[c].unsqueeze(0))\n",
    "            if len(child_vecs) == 0:\n",
    "                continue\n",
    "            child_stack = torch.cat(child_vecs, dim=0)  # [C, D]\n",
    "            agg_val, _ = torch.max(child_stack, dim=0)\n",
    "            agg[node] = agg_val\n",
    "\n",
    "        return agg\n",
    "\n",
    "# -------------------------\n",
    "# Utilities for training\n",
    "# -------------------------\n",
    "def tree_distance(G: nx.DiGraph, node_a, node_b, max_penalty=None):\n",
    "    \"\"\"\n",
    "    Shortest path length in undirected tree, fallback to max_penalty or len(G.nodes)\n",
    "    \"\"\"\n",
    "    und = G.to_undirected()\n",
    "    try:\n",
    "        return nx.shortest_path_length(und, source=node_a, target=node_b)\n",
    "    except Exception:\n",
    "        return max_penalty if max_penalty is not None else len(G.nodes)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Training loop (fully adjusted)\n",
    "# -------------------------\n",
    "def train_hgt_detector(model: HGTDetector, list_of_Data, epochs=10, val_frac=0.1, lr=1e-3,\n",
    "                       lambda_donor=0.1, hgt_threshold=0.5, max_candidates=300,\n",
    "                       device=None, clip_grad=5.0):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "    N = len(list_of_Data)\n",
    "    idxs = list(range(N))\n",
    "    random.shuffle(idxs)\n",
    "    n_val = max(1, int(val_frac * N))\n",
    "    val_idx = set(idxs[:n_val])\n",
    "    train_idx = idxs[n_val:]\n",
    "\n",
    "    # Estimate pos_weight for BCEWithLogitsLoss on training set (clamped)\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    for i in train_idx:\n",
    "        d = list_of_Data[i]\n",
    "        recip_map = d.recip_label_map\n",
    "        _ = d.donor_map\n",
    "        for p, (lbl, _) in recip_map.items():\n",
    "            if lbl == 1:\n",
    "                pos_count += 1\n",
    "            else:\n",
    "                neg_count += 1\n",
    "    pos_count = max(pos_count, 1)\n",
    "    neg_count = max(neg_count, 1)\n",
    "    ratio = neg_count / pos_count\n",
    "    pos_weight_val = max(1.0, min(ratio, 50.0))  # clamp between 1 and 50\n",
    "    pos_weight = torch.tensor([pos_weight_val], device=device)\n",
    "    bce_loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    print(f\"Training on {len(train_idx)} graphs | Validation on {len(val_idx)} graphs | pos_weight={pos_weight_val:.3f}\")\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_rec_loss = 0.0\n",
    "        train_don_loss = 0.0\n",
    "        train_samples = 0\n",
    "\n",
    "        # metrics\n",
    "        tp = 0; fp = 0; fn = 0\n",
    "        donor_expected_distance_accum = 0.0\n",
    "        donor_expected_count = 0\n",
    "\n",
    "        random.shuffle(train_idx)\n",
    "        for idx in train_idx:\n",
    "            data = list_of_Data[idx]\n",
    "            # move seqs to device once\n",
    "            data.hot_encoded_nucleotide_sequences = data.hot_encoded_nucleotide_sequences.to(device)\n",
    "\n",
    "            recip_map = data.recip_label_map\n",
    "            donor_map = data.donor_map\n",
    "            parents = [n for n in data.G.nodes if len(list(data.G.successors(n))) == 2]\n",
    "            if len(parents) == 0:\n",
    "                continue\n",
    "\n",
    "            # compute aggregates\n",
    "            agg = model.compute_aggregates(data.G, data.hot_encoded_nucleotide_sequences)  # agg shape [max_id+1, D]\n",
    "\n",
    "            # collect batch lists\n",
    "            L_list = []\n",
    "            R_list = []\n",
    "            aux_list = []\n",
    "            true_label_list = []\n",
    "            true_rec_child_list = []\n",
    "            true_donor_parent_list = []\n",
    "            parent_nodes_list = []\n",
    "\n",
    "            for parent in parents:\n",
    "                children = list(data.G.successors(parent))\n",
    "                left, right = children[0], children[1]\n",
    "                # left/right exist as node ids; agg is indexed by node id\n",
    "                L_list.append(agg[left].unsqueeze(0))\n",
    "                R_list.append(agg[right].unsqueeze(0))\n",
    "                # aux: node_time, level, gene_frac\n",
    "                node_time = float(data.G.nodes[parent].get('node_time', 0.0))\n",
    "                node_level = float(data.G.nodes[parent].get('level', 0.0))\n",
    "                # avoid divide by zero\n",
    "                parent_sum = agg[parent].sum().clamp(min=1.0)\n",
    "                gene_frac = ((agg[left].sum() + agg[right].sum()) / parent_sum).unsqueeze(0).unsqueeze(1)\n",
    "                aux = torch.tensor([[node_time, node_level, float(gene_frac.item())]], device=device, dtype=torch.float32)\n",
    "                aux_list.append(aux)\n",
    "                lbl, true_rec = recip_map.get(parent, (0, None))\n",
    "                true_label_list.append(lbl)\n",
    "                true_rec_child_list.append(true_rec)\n",
    "                true_donor_parent_list.append(donor_map.get(parent, None))\n",
    "                parent_nodes_list.append(parent)\n",
    "\n",
    "            if len(L_list) == 0:\n",
    "                continue\n",
    "\n",
    "            left_vecs = torch.cat(L_list, dim=0)      # [P, D]\n",
    "            right_vecs = torch.cat(R_list, dim=0)     # [P, D]\n",
    "            aux_feats = torch.cat(aux_list, dim=0)    # [P, 3]\n",
    "            true_labels = torch.tensor(true_label_list, dtype=torch.float32, device=device)  # [P]\n",
    "\n",
    "            # Recipient forward\n",
    "            hgt_logits, which_logits = model.recipient_finder(left_vecs, right_vecs, aux_feats)\n",
    "            rec_loss = bce_loss_fn(hgt_logits, true_labels)\n",
    "\n",
    "            # compute predicted probabilities & which\n",
    "            probs = torch.sigmoid(hgt_logits).detach().cpu().numpy()\n",
    "            which_pred = torch.argmax(which_logits.detach().cpu(), dim=1).tolist()\n",
    "\n",
    "            # Donor loss (differentiable expected distance) aggregated for events\n",
    "            donor_loss_total = torch.tensor(0.0, device=device)\n",
    "            donor_events = 0\n",
    "            # For each parent with true_label==1, check whether recipient predicted correctly\n",
    "            for i_parent, parent in enumerate(parent_nodes_list):\n",
    "                lbl = int(true_labels[i_parent].item())\n",
    "                if lbl != 1:\n",
    "                    continue\n",
    "                true_rec_child = true_rec_child_list[i_parent]\n",
    "                true_donor_parent = true_donor_parent_list[i_parent]\n",
    "                # predicted recipient child:\n",
    "                pred_rec_child = list(data.G.successors(parent))[which_pred[i_parent]]\n",
    "                pred_prob = float(probs[i_parent])\n",
    "                # condition: only add donor loss if recipient correctly identified\n",
    "                if (pred_prob >= hgt_threshold) and (true_rec_child is not None) and (pred_rec_child == true_rec_child):\n",
    "                    # build candidate list: nodes with node_time < recipient_time\n",
    "                    rec_node = pred_rec_child\n",
    "                    rec_time = float(data.G.nodes[rec_node].get('node_time', 0.0))\n",
    "                    candidates = [n for n in data.G.nodes if float(data.G.nodes[n].get('node_time',0.0)) < rec_time and n != rec_node]\n",
    "                    # ensure true donor parent is among candidates - if not, append it (keeps learning signal)\n",
    "                    if true_donor_parent is not None and true_donor_parent not in candidates:\n",
    "                        candidates.append(true_donor_parent)\n",
    "                    if len(candidates) == 0:\n",
    "                        # no candidates -> penalize by max distance (normalized)\n",
    "                        max_pen = len(data.G.nodes)\n",
    "                        donor_loss_total = donor_loss_total + (torch.tensor(float(max_pen), device=device) / float(max_pen))\n",
    "                        donor_events += 1\n",
    "                        continue\n",
    "\n",
    "                    # sample candidates if too many\n",
    "                    if len(candidates) > max_candidates:\n",
    "                        random.shuffle(candidates)\n",
    "                        candidates = candidates[:max_candidates]\n",
    "                    cand_idx_tensor = torch.tensor(candidates, dtype=torch.long, device=device)\n",
    "\n",
    "                    donor_vecs = agg[cand_idx_tensor]  # [M, D]\n",
    "                    donor_times = torch.tensor([float(data.G.nodes[n].get('node_time',0.0)) for n in candidates], device=device)\n",
    "                    donor_levels = torch.tensor([float(data.G.nodes[n].get('level',0.0)) for n in candidates], device=device)\n",
    "                    time_diff = (rec_time - donor_times).unsqueeze(1)              # [M,1]\n",
    "                    level_diff = (data.G.nodes[rec_node].get('level',0.0) - donor_levels).unsqueeze(1)\n",
    "                    donor_frac = (donor_vecs.sum(dim=1).unsqueeze(1) / (agg.sum(dim=1).clamp(min=1.0)[cand_idx_tensor].unsqueeze(1))).clamp(0.0,1.0)\n",
    "                    donor_aux = torch.cat([time_diff, level_diff, donor_frac], dim=1)\n",
    "\n",
    "                    # get raw scores (no softmax yet)\n",
    "                    scores = model.donor_finder(agg[rec_node].to(device), donor_vecs, donor_aux)  # [M]\n",
    "\n",
    "                    # compute distances vector to true donor\n",
    "                    und = data.G.to_undirected()\n",
    "                    max_pen = len(data.G.nodes)\n",
    "                    dists = []\n",
    "                    for cand in candidates:\n",
    "                        if true_donor_parent is None:\n",
    "                            # if no true donor known, give zero distance (no loss)\n",
    "                            dists.append(0.0)\n",
    "                        else:\n",
    "                            try:\n",
    "                                dd = nx.shortest_path_length(und, source=cand, target=true_donor_parent)\n",
    "                                dists.append(float(dd))\n",
    "                            except Exception:\n",
    "                                dists.append(float(max_pen))\n",
    "                    dists = torch.tensor(dists, dtype=torch.float32, device=device)  # [M]\n",
    "\n",
    "                    # Softmax probabilities over scores (temperature can be used)\n",
    "                    probs_soft = torch.softmax(scores, dim=0)  # [M]\n",
    "                    # Expected distance (differentiable)\n",
    "                    expected_dist = torch.dot(probs_soft, dists)  # scalar\n",
    "                    # Normalize by max_pen to keep scale ~[0,1]\n",
    "                    expected_dist = expected_dist / float(max_pen)\n",
    "\n",
    "                    donor_loss_total = donor_loss_total + expected_dist\n",
    "                    donor_events += 1\n",
    "\n",
    "                    # accumulate for reporting expected donor distance (as float)\n",
    "                    donor_expected_distance_accum += float((expected_dist * float(max_pen)).item())\n",
    "                    donor_expected_count += 1\n",
    "\n",
    "                    # -----------------------\n",
    "                    # HERE: perform the structure correction exactly when:\n",
    "                    # recipient is predicted correctly (pred_rec_child == true_rec_child) AND\n",
    "                    # donor parent predicted equals true donor parent (donor selected by something),\n",
    "                    # We check whether the highest-scoring candidate equals true_donor_parent.\n",
    "                    # -----------------------\n",
    "                    _, max_idx = torch.max(scores, dim=0)\n",
    "                    pred_donor_parent_candidate = candidates[int(max_idx.item())] if len(candidates) > 0 else None\n",
    "\n",
    "                    # Only perform *safe* structural edit when predicted donor parent matches truth.\n",
    "                    if (pred_donor_parent_candidate == true_donor_parent) and (pred_donor_parent_candidate is not None):\n",
    "                        # do a safe structural edit on a COPY of data.G (work on copy to be safe)\n",
    "                        G_mod = copy.deepcopy(data.G)\n",
    "                        node_seq_matrix_current = data.hot_encoded_nucleotide_sequences.clone().to(device)\n",
    "                        current_max_node_id = int(max(list(G_mod.nodes))) if len(G_mod.nodes) > 0 else -1\n",
    "                        new_node = current_max_node_id + 1\n",
    "\n",
    "                        # extend node_seq_matrix_current by zeros if needed\n",
    "                        rows_needed = new_node + 1 - node_seq_matrix_current.shape[0]\n",
    "                        if rows_needed > 0:\n",
    "                            pad = torch.zeros((rows_needed, node_seq_matrix_current.shape[1]), dtype=node_seq_matrix_current.dtype, device=node_seq_matrix_current.device)\n",
    "                            node_seq_matrix_current = torch.cat([node_seq_matrix_current, pad], dim=0)\n",
    "\n",
    "                        # compute rec_child_time and donor_parent_time ON THE COPY\n",
    "                        rec_child_time = float(G_mod.nodes[pred_rec_child].get('node_time', 0.0))\n",
    "                        donor_parent_time = float(G_mod.nodes[pred_donor_parent_candidate].get('node_time', 0.0))\n",
    "\n",
    "                        # Use the predicted donor parent candidate variable consistently here:\n",
    "                        donor_children = list(G_mod.successors(pred_donor_parent_candidate))\n",
    "                        if len(donor_children) == 0:\n",
    "                            # cannot split; skip structural edit\n",
    "                            pass\n",
    "                        else:\n",
    "                            # pick donor_child heuristically: closest subtree-sum to rec subtree-sum\n",
    "                            try:\n",
    "                                agg_mod = model.compute_aggregates(G_mod, node_seq_matrix_current)\n",
    "                            except Exception:\n",
    "                                agg_mod = agg  # fallback to previously computed agg if compute fails\n",
    "\n",
    "                            rec_sum = float(agg_mod[pred_rec_child].sum().item()) if pred_rec_child < agg_mod.shape[0] else 0.0\n",
    "                            cand_sums = []\n",
    "                            for c in donor_children:\n",
    "                                if c < agg_mod.shape[0]:\n",
    "                                    cand_sums.append((c, float(agg_mod[c].sum().item())))\n",
    "                                else:\n",
    "                                    cand_sums.append((c, 0.0))\n",
    "                            if len(cand_sums) == 0:\n",
    "                                donor_child = donor_children[0]\n",
    "                            else:\n",
    "                                donor_child = min(cand_sums, key=lambda x: abs(x[1] - rec_sum))[0]\n",
    "\n",
    "                            donor_child_time = float(G_mod.nodes[donor_child].get('node_time', 0.0))\n",
    "\n",
    "                            t1 = rec_child_time + donor_parent_time\n",
    "                            t2 = donor_child_time + donor_parent_time\n",
    "                            new_time = 0.5 * max(t1, t2)\n",
    "\n",
    "                            # add new node and attributes to G_mod\n",
    "                            G_mod.add_node(new_node)\n",
    "                            G_mod.nodes[new_node]['node_time'] = new_time\n",
    "                            G_mod.nodes[new_node]['level'] = (G_mod.nodes[pred_rec_child].get('level', 0.0) + G_mod.nodes[pred_donor_parent_candidate].get('level', 0.0))/2.0\n",
    "\n",
    "                            # Rewire recipient edge: remove (parent -> pred_rec_child), insert (parent -> new_node) and (new_node -> pred_rec_child)\n",
    "\n",
    "                            try:\n",
    "                                G_mod.remove_edge(pred_donor_parent_candidate, donor_child)\n",
    "                                G_mod.add_edge(pred_donor_parent_candidate, new_node)\n",
    "                                G_mod.add_edge(new_node, donor_child)\n",
    "                                \n",
    "                                G_mod.remove_edge(parent, pred_rec_child)\n",
    "                                G_mod.add_edge(new_node, pred_rec_child)\n",
    "    \n",
    "                                parent_of_recipient_parental_node = list(G_mod.predecessors(parent))[0]\n",
    "                                other_child_of_recipient_parental_node = list(G_mod.successors(parent))[0]\n",
    "    \n",
    "                                G_mod.remove_edge(parent_of_recipient_parental_node, parent)\n",
    "                                G_mod.remove_edge(parent, other_child_of_recipient_parental_node)\n",
    "                                G_mod.add_edge(parent_of_recipient_parental_node, other_child_of_recipient_parental_node)\n",
    "                            except:\n",
    "                                print(data.G.edges)\n",
    "                                print(data.hgt_events)\n",
    "                                print(\"Predicted recipient:\", parent, pred_rec_child)\n",
    "\n",
    "                            #G_mod.remove_edge(pred_donor_parent_candidate, donor_child)\n",
    "                            #G_mod.add_edge(pred_donor_parent_candidate, new_node)\n",
    "                            \n",
    "\n",
    "                            #G_mod.remove_edge(parent_of_recipient_parental_node, other_child_of_parent)\n",
    "                            #G_mod.add_edge(parent_of_recipient_parental_node, other_child_of_parent)\n",
    "                            \n",
    "\n",
    "                            # Split donor edge: remove (pred_donor_parent_candidate -> donor_child) and add (pred_donor_parent_candidate -> new_node) and (new_node -> donor_child)\n",
    "                            #if G_mod.has_edge(pred_donor_parent_candidate, donor_child):\n",
    "                            #G_mod.add_edge(new_node, donor_child)\n",
    "\n",
    "                            print(data.G.edges)\n",
    "                            print(data.hgt_events)\n",
    "                            print(G_mod.edges)\n",
    "\n",
    "                            # Safety check: avoid creating cycles\n",
    "                            if nx.has_path(G_mod, donor_child, parent):\n",
    "                                # rollback: remove new node and its edges\n",
    "                                if new_node in G_mod.nodes:\n",
    "                                    G_mod.remove_node(new_node)\n",
    "                                # don't change original data.G\n",
    "                            else:\n",
    "                                if nx.is_directed_acyclic_graph(G_mod):\n",
    "                                    # Accept modification: update data.G so subsequent parents see change\n",
    "                                    data.G = G_mod\n",
    "                                    # Recompute agg so next parents in loop see updated aggregates\n",
    "                                    try:\n",
    "                                        agg = model.compute_aggregates(data.G, node_seq_matrix_current)\n",
    "                                    except Exception:\n",
    "                                        pass\n",
    "                                else:\n",
    "                                    # rollback due to cycle\n",
    "                                    if new_node in G_mod.nodes:\n",
    "                                        G_mod.remove_node(new_node)\n",
    "                                    # leave original graph alone\n",
    "\n",
    "            # end for each parent donor loss / possible structure edits\n",
    "\n",
    "            # Normalize donor loss by donor_events (if >0)\n",
    "            if donor_events > 0:\n",
    "                donor_loss = donor_loss_total / donor_events\n",
    "            else:\n",
    "                donor_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "            loss = rec_loss + lambda_donor * donor_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), clip_grad)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_rec_loss += rec_loss.item()\n",
    "            train_don_loss += float(donor_loss.item())\n",
    "            train_samples += 1\n",
    "\n",
    "            # update metrics for recipient detection\n",
    "            for i_parent, parent in enumerate(parent_nodes_list):\n",
    "                true_lbl = int(true_labels[i_parent].item())\n",
    "                pred_prob = float(probs[i_parent])\n",
    "                pred_class = 1 if pred_prob >= hgt_threshold else 0\n",
    "                if pred_class == 1 and true_lbl == 1:\n",
    "                    tp += 1\n",
    "                if pred_class == 1 and true_lbl == 0:\n",
    "                    fp += 1\n",
    "                if pred_class == 0 and true_lbl == 1:\n",
    "                    fn += 1\n",
    "\n",
    "        # End epoch training stats\n",
    "        avg_train_loss = train_loss / max(1, train_samples)\n",
    "        avg_train_rec = train_rec_loss / max(1, train_samples)\n",
    "        avg_train_don = train_don_loss / max(1, train_samples)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        avg_expected_donor_distance = (donor_expected_distance_accum / donor_expected_count) if donor_expected_count > 0 else 0.0\n",
    "\n",
    "        # Validation pass (keeps previous validation logic; structure edits are not applied during validation)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            val_rec_loss = 0.0\n",
    "            val_don_loss = 0.0\n",
    "            val_samples = 0\n",
    "            v_tp = v_fp = v_fn = 0\n",
    "            v_donor_expected_dist_acc = 0.0\n",
    "            v_donor_expected_cnt = 0\n",
    "\n",
    "            for idx in val_idx:\n",
    "                data = list_of_Data[idx]\n",
    "                data.hot_encoded_nucleotide_sequences = data.hot_encoded_nucleotide_sequences.to(device)\n",
    "                recip_map = data.recip_label_map\n",
    "                donor_map = data.donor_map\n",
    "                parents = [n for n in data.G.nodes if len(list(data.G.successors(n))) == 2]\n",
    "                if len(parents) == 0:\n",
    "                    continue\n",
    "                agg = model.compute_aggregates(data.G, data.hot_encoded_nucleotide_sequences)\n",
    "                # collect lists\n",
    "                L_list, R_list, aux_list = [], [], []\n",
    "                true_labels = []\n",
    "                true_rec_child_list = []\n",
    "                true_donor_parent_list = []\n",
    "                parent_nodes_list = []\n",
    "                for parent in parents:\n",
    "                    left, right = list(data.G.successors(parent))[0], list(data.G.successors(parent))[1]\n",
    "                    L_list.append(agg[left].unsqueeze(0))\n",
    "                    R_list.append(agg[right].unsqueeze(0))\n",
    "                    node_time = float(data.G.nodes[parent].get('node_time', 0.0))\n",
    "                    node_level = float(data.G.nodes[parent].get('level', 0.0))\n",
    "                    parent_sum = agg[parent].sum().clamp(min=1.0)\n",
    "                    gene_frac = ((agg[left].sum() + agg[right].sum()) / parent_sum).unsqueeze(0).unsqueeze(1)\n",
    "                    aux = torch.tensor([[node_time, node_level, float(gene_frac.item())]], device=device, dtype=torch.float32)\n",
    "                    aux_list.append(aux)\n",
    "                    lbl, true_rec = recip_map.get(parent, (0, None))\n",
    "                    true_labels.append(lbl)\n",
    "                    true_rec_child_list.append(true_rec)\n",
    "                    true_donor_parent_list.append(donor_map.get(parent, None))\n",
    "                    parent_nodes_list.append(parent)\n",
    "                if len(L_list) == 0:\n",
    "                    continue\n",
    "                left_vecs = torch.cat(L_list, dim=0)\n",
    "                right_vecs = torch.cat(R_list, dim=0)\n",
    "                aux_feats = torch.cat(aux_list, dim=0)\n",
    "                true_labels = torch.tensor(true_labels, dtype=torch.float32, device=device)\n",
    "\n",
    "                hgt_logits, which_logits = model.recipient_finder(left_vecs, right_vecs, aux_feats)\n",
    "                rec_loss = bce_loss_fn(hgt_logits, true_labels)\n",
    "\n",
    "                # donor loss computation: same expected-distance approach but without gradients\n",
    "                probs = torch.sigmoid(hgt_logits).cpu().numpy()\n",
    "                which_pred = torch.argmax(which_logits, dim=1).tolist()\n",
    "\n",
    "                donor_loss_total = 0.0\n",
    "                donor_events = 0\n",
    "                for i_parent, parent in enumerate(parent_nodes_list):\n",
    "                    lbl = int(true_labels[i_parent].item())\n",
    "                    if lbl != 1:\n",
    "                        continue\n",
    "                    true_rec_child = true_rec_child_list[i_parent]\n",
    "                    true_donor_parent = true_donor_parent_list[i_parent]\n",
    "                    pred_rec_child = list(data.G.successors(parent))[which_pred[i_parent]]\n",
    "                    pred_prob = float(probs[i_parent])\n",
    "                    if (pred_prob >= hgt_threshold) and (true_rec_child is not None) and (pred_rec_child == true_rec_child):\n",
    "                        rec_node = pred_rec_child\n",
    "                        rec_time = float(data.G.nodes[rec_node].get('node_time', 0.0))\n",
    "                        candidates = [n for n in data.G.nodes if float(data.G.nodes[n].get('node_time',0.0)) < rec_time and n != rec_node]\n",
    "                        if true_donor_parent is not None and true_donor_parent not in candidates:\n",
    "                            candidates.append(true_donor_parent)\n",
    "                        if len(candidates) == 0:\n",
    "                            donor_loss_total += 1.0\n",
    "                            donor_events += 1\n",
    "                            continue\n",
    "                        if len(candidates) > max_candidates:\n",
    "                            random.shuffle(candidates)\n",
    "                            candidates = candidates[:max_candidates]\n",
    "                        cand_idx_tensor = torch.tensor(candidates, dtype=torch.long, device=device)\n",
    "                        donor_vecs = agg[cand_idx_tensor]\n",
    "                        donor_times = torch.tensor([float(data.G.nodes[n].get('node_time',0.0)) for n in candidates], device=device)\n",
    "                        donor_levels = torch.tensor([float(data.G.nodes[n].get('level',0.0)) for n in candidates], device=device)\n",
    "                        time_diff = (rec_time - donor_times).unsqueeze(1)\n",
    "                        level_diff = (data.G.nodes[rec_node].get('level',0.0) - donor_levels).unsqueeze(1)\n",
    "                        donor_frac = (donor_vecs.sum(dim=1).unsqueeze(1) / (agg.sum(dim=1).clamp(min=1.0)[cand_idx_tensor].unsqueeze(1))).clamp(0.0,1.0)\n",
    "                        donor_aux = torch.cat([time_diff, level_diff, donor_frac], dim=1)\n",
    "                        scores = model.donor_finder(agg[rec_node].to(device), donor_vecs, donor_aux)\n",
    "                        # distances\n",
    "                        und = data.G.to_undirected()\n",
    "                        max_pen = len(data.G.nodes)\n",
    "                        dists = []\n",
    "                        for cand in candidates:\n",
    "                            if true_donor_parent is None:\n",
    "                                dists.append(0.0)\n",
    "                            else:\n",
    "                                try:\n",
    "                                    dd = nx.shortest_path_length(und, source=cand, target=true_donor_parent)\n",
    "                                    dists.append(float(dd))\n",
    "                                except Exception:\n",
    "                                    dists.append(float(max_pen))\n",
    "                        dists = torch.tensor(dists, dtype=torch.float32, device=device)\n",
    "                        probs_soft = torch.softmax(scores, dim=0)\n",
    "                        expected_dist = float(torch.dot(probs_soft, dists).item()) / float(max_pen)\n",
    "                        donor_loss_total += expected_dist\n",
    "                        donor_events += 1\n",
    "                        v_donor_expected_dist_acc += (expected_dist * float(max_pen))\n",
    "                        v_donor_expected_cnt += 1\n",
    "\n",
    "                donor_loss = (donor_loss_total / donor_events) if donor_events > 0 else 0.0\n",
    "                loss = rec_loss.item() + lambda_donor * donor_loss\n",
    "\n",
    "                val_loss += loss\n",
    "                val_rec_loss += rec_loss.item()\n",
    "                val_don_loss += donor_loss\n",
    "                val_samples += 1\n",
    "\n",
    "                # update val metrics for recipient detection\n",
    "                for i_parent, parent in enumerate(parent_nodes_list):\n",
    "                    true_lbl = int(true_labels[i_parent].item())\n",
    "                    pred_prob = float(probs[i_parent])\n",
    "                    pred_class = 1 if pred_prob >= hgt_threshold else 0\n",
    "                    if pred_class == 1 and true_lbl == 1:\n",
    "                        v_tp += 1\n",
    "                    if pred_class == 1 and true_lbl == 0:\n",
    "                        v_fp += 1\n",
    "                    if pred_class == 0 and true_lbl == 1:\n",
    "                        v_fn += 1\n",
    "\n",
    "            avg_val_loss = val_loss / max(1, val_samples)\n",
    "            avg_val_rec = val_rec_loss / max(1, val_samples)\n",
    "            avg_val_don = val_don_loss / max(1, val_samples)\n",
    "            v_precision = v_tp / (v_tp + v_fp) if (v_tp + v_fp) > 0 else 0.0\n",
    "            v_recall = v_tp / (v_tp + v_fn) if (v_tp + v_fn) > 0 else 0.0\n",
    "            v_f1 = 2 * v_precision * v_recall / (v_precision + v_recall) if (v_precision + v_recall) > 0 else 0.0\n",
    "            avg_v_donor_expected = (v_donor_expected_dist_acc / v_donor_expected_cnt) if v_donor_expected_cnt > 0 else 0.0\n",
    "\n",
    "        # scheduler step (use validation loss)\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        t1 = time.time()\n",
    "        print(f\"Epoch {epoch}/{epochs} | time {t1-t0:.1f}s\")\n",
    "        print(f\" TRAIN loss {avg_train_loss:.4f} (rec {avg_train_rec:.4f}, don {avg_train_don:.4f}) \"\n",
    "              f\"Prec/Rec/F1 {precision:.3f}/{recall:.3f}/{f1:.3f} | avg_expected_donor_dist {avg_expected_donor_distance:.3f}\")\n",
    "        print(f\" VAL   loss {avg_val_loss:.4f} (rec {avg_val_rec:.4f}, don {avg_val_don:.4f}) \"\n",
    "              f\"Prec/Rec/F1 {v_precision:.3f}/{v_recall:.3f}/{v_f1:.3f} | avg_expected_donor_dist {avg_v_donor_expected:.3f}\")\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Example usage snippet (unchanged seeds + instantiation)\n",
    "# -------------------------\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# choose a sample data to get seq_vec_dim\n",
    "sample_data = random.choice(list_of_Data)\n",
    "seq_vec_dim = sample_data.hot_encoded_nucleotide_sequences.shape[1]\n",
    "model = HGTDetector(seq_vec_dim=seq_vec_dim, hgt_threshold=0.5, topk_donors=1)\n",
    "\n",
    "trained_model = train_hgt_detector(model, list_of_Data,\n",
    "                                  epochs=4,\n",
    "                                  val_frac=0.1,\n",
    "                                  lr=1e-3,\n",
    "                                  lambda_donor=0.1,\n",
    "                                  hgt_threshold=0.5,\n",
    "                                  max_candidates=300,\n",
    "                                  device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                                  clip_grad=5.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5abc0e0-f81e-48eb-814e-26f5aeb6bb7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
