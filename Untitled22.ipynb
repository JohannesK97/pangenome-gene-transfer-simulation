{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61cba947-8905-4343-9a10-23d5985483e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17864/3194679396.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edges = torch.tensor(graph_properties[1], dtype=torch.long)  # [2, num_edges]\n",
      "/tmp/ipykernel_17864/3194679396.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coords = torch.tensor(graph_properties[2].T)             # [2, num_nodes]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fehler beim Laden von /mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks/simulation_1277.h5: cannot reshape array of size 0 into shape (0,newaxis)\n",
      "Fehler beim Laden von /mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks/simulation_4830.h5: cannot reshape array of size 0 into shape (0,newaxis)\n",
      "Fehler beim Laden von /mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks/simulation_6090.h5: cannot reshape array of size 0 into shape (0,newaxis)\n",
      "Fehler beim Laden von /mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks/simulation_6496.h5: cannot reshape array of size 0 into shape (0,newaxis)\n",
      "Fehler beim Laden von /mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks/simulation_6986.h5: cannot reshape array of size 0 into shape (0,newaxis)\n",
      "Fehler beim Laden von /mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks/simulation_7401.h5: cannot reshape array of size 0 into shape (0,newaxis)\n",
      "Fehler beim Laden von /mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks/simulation_8148.h5: cannot reshape array of size 0 into shape (0,newaxis)\n",
      "Fehler beim Laden von /mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks/simulation_8325.h5: cannot reshape array of size 0 into shape (0,newaxis)\n",
      "Fehler beim Laden von /mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks/simulation_8699.h5: cannot reshape array of size 0 into shape (0,newaxis)\n",
      "Fehler beim Laden von /mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks/simulation_9604.h5: cannot reshape array of size 0 into shape (0,newaxis)\n",
      "5508 Dateien erfolgreich geladen.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import h5py\n",
    "import pickle\n",
    "import torch\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, ReLU, Dropout, BatchNorm1d\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "def one_hot_encode(sequences, gene_present, gene_length, alphabet=['A','C','T','G','-']):\n",
    "    \"\"\"\n",
    "    sequences: List of strings (DNA sequences)\n",
    "    gene_present: np.array(bool) oder Torch Tensor, gleiche Länge wie sequences\n",
    "    gene_length: int, fixe Länge für das Hot-Encoding\n",
    "    alphabet: list, Zeichenalphabet\n",
    "    \"\"\"\n",
    "    num_samples = len(sequences)\n",
    "    num_chars = len(alphabet)\n",
    "    char_to_idx = {c:i for i,c in enumerate(alphabet)}\n",
    "    sequences_str = [s.decode('utf-8') for s in sequences]\n",
    "    gene_present = np.array(gene_present, dtype=bool)\n",
    "    \n",
    "    # 1️⃣ Leere Batch-Matrix vorbereiten: (num_samples, gene_length, num_chars)\n",
    "    batch = np.zeros((num_samples, gene_length, num_chars), dtype=np.float32)\n",
    "    \n",
    "    # 2️⃣ Hot-Encode alle Sequenzen\n",
    "    for i, seq in enumerate(sequences_str):\n",
    "        if gene_present[i]:\n",
    "            L = min(len(seq), gene_length)  # abschneiden\n",
    "            for j, c in enumerate(seq[:L]):\n",
    "                if c in char_to_idx:\n",
    "                    batch[i, j, char_to_idx[c]] = 1.0\n",
    "        elif gene_present[i] == 0:\n",
    "            batch[i, :, :] = -1.0\n",
    "            \n",
    "    # 3️⃣ Zufällige, aber konsistente Spaltenpermutation\n",
    "    perm = np.random.permutation(gene_length)\n",
    "    batch = batch[:, perm, :]\n",
    "    \n",
    "    # 4️⃣ Optional: Flatten zu Vektor (num_samples, gene_length*num_chars)\n",
    "    batch_flat = batch.reshape(num_samples, -1)\n",
    "    \n",
    "    return torch.tensor(batch_flat)  # shape: (num_samples, gene_length*num_chars)\n",
    "\n",
    "def load_file(file):\n",
    "\n",
    "    gene_length = 300\n",
    "    #nucleotide_mutation_rate = 0.1\n",
    "    \n",
    "    with h5py.File(file, \"r\") as f:\n",
    "            grp = f[\"results\"]\n",
    "            # Load graph_properties (pickle stored in dataset)\n",
    "            graph_properties = pickle.loads(grp[\"graph_properties\"][()])\n",
    "    \n",
    "            # Unpack graph properties\n",
    "            nodes = torch.tensor(graph_properties[0])                # [num_nodes]\n",
    "            edges = torch.tensor(graph_properties[1], dtype=torch.long)  # [2, num_edges]\n",
    "            coords = torch.tensor(graph_properties[2].T)             # [2, num_nodes]\n",
    "    \n",
    "            # Load datasets instead of attrs\n",
    "            gene_absence_presence_matrix = grp[\"gene_absence_presence_matrix\"][()]\n",
    "            nucleotide_sequences = grp[\"nucleotide_sequences\"][()]\n",
    "            #children_gene_nodes_loss_events = grp[\"children_gene_nodes_loss_events\"][()]\n",
    "        \n",
    "            # Load HGT events (simplified)\n",
    "            hgt_events = {}\n",
    "            hgt_grp_simpl = grp[\"nodes_hgt_events_simplified\"]\n",
    "            for site_id in hgt_grp_simpl.keys():\n",
    "                hgt_events[int(site_id)] = hgt_grp_simpl[site_id][()]\n",
    "\n",
    "            hot_encoded_nucleotide_sequences = one_hot_encode(nucleotide_sequences, gene_absence_presence_matrix, gene_length)\n",
    "\n",
    "            # Fill the remaining nodes with zeros.\n",
    "            pad_rows = len(nodes) - len(nucleotide_sequences)\n",
    "            pad = torch.zeros((pad_rows, hot_encoded_nucleotide_sequences.shape[1]), dtype=hot_encoded_nucleotide_sequences.dtype)\n",
    "            hot_encoded_nucleotide_sequences = torch.cat([hot_encoded_nucleotide_sequences, pad], dim=0)\n",
    "        \n",
    "            G = nx.DiGraph()\n",
    "            \n",
    "            # Füge Knoten hinzu (optional mit Koordinaten als Attribut)\n",
    "            for i, node_id in enumerate(nodes.tolist()):\n",
    "                G.add_node(node_id, node_time = coords[:, i].tolist()[5])\n",
    "            \n",
    "            # Füge Kanten hinzu\n",
    "            edge_list = edges.tolist()\n",
    "            for src, dst in zip(edge_list[0], edge_list[1]):\n",
    "                G.add_edge(src, dst)\n",
    "            \n",
    "            # Collect all recipient_parent_nodes from all sites\n",
    "            recipient_parent_nodes = set()\n",
    "            for site_id in hgt_grp_simpl.keys():\n",
    "                arr = hgt_grp_simpl[site_id][()]  # load dataset as numpy structured array\n",
    "                recipient_parent_nodes.update(arr[\"recipient_child_node\"].tolist())\n",
    "            \n",
    "            # Build theta_gains: 1 if node is in recipient_parent_nodes, else 0\n",
    "            theta_gains = torch.tensor(\n",
    "                [1 if node in recipient_parent_nodes else 0 for node in range(len(G.nodes))],\n",
    "                dtype=torch.long\n",
    "            )\n",
    "\n",
    "            level = {n: 0 for n in G.nodes}  # Leaves haben Level 0\n",
    "            \n",
    "            # 3. Topologische Sortierung (damit Kinder vor Eltern behandelt werden)\n",
    "            for node in reversed(list(nx.topological_sort(G))):\n",
    "                successors = list(G.successors(node))\n",
    "                if successors:\n",
    "                    level[node] = 1 + max(level[s] for s in successors)\n",
    "            \n",
    "            # 4. Level als Attribut setzen\n",
    "            nx.set_node_attributes(G, level, \"level\")\n",
    "\n",
    "            ### Add candidate egdes, i.e. potential hgt edges:\n",
    "        \n",
    "            # Hole alle Knoten und ihre Zeiten\n",
    "            node_times = {n: G.nodes[n]['node_time'] for n in G.nodes}\n",
    "            sorted_nodes = sorted(node_times.keys(), key=lambda n: node_times[n])\n",
    "\n",
    "            # Sort the level and node_times from 0 to max_node_id and not in G.nodes order:\n",
    "            node_times = torch.tensor([node_times[n] for n in sorted_nodes], dtype=torch.float)\n",
    "            node_levels = torch.tensor([level[n] for n in sorted_nodes], dtype=torch.float)\n",
    "\n",
    "            # Füge Kanten hinzu\n",
    "            existing_edges = set(zip(edges[0].tolist(), edges[1].tolist()))\n",
    "            candidate_edges = []\n",
    "            for i, src in enumerate(sorted_nodes):\n",
    "                t_src = node_times[src]\n",
    "                for dst in sorted_nodes[i+1:]:  # nur spätere Knoten\n",
    "                    t_dst = node_times[dst]\n",
    "                    if t_dst > t_src:\n",
    "                        if (dst, src) not in existing_edges:  # vermeidet doppelte\n",
    "                            #G.add_edge(src, dst)\n",
    "                            candidate_edges.append((dst, src))\n",
    "            if candidate_edges:  \n",
    "                candidate_edges = torch.tensor(candidate_edges, dtype=torch.long).t()\n",
    "            else:\n",
    "                candidate_edges = torch.empty((2,0), dtype=torch.long)\n",
    "                    \n",
    "            data = Data(\n",
    "                #nucleotide_sequences = nucleotide_sequences,\n",
    "                hot_encoded_nucleotide_sequences = hot_encoded_nucleotide_sequences,       # Node Features [num_nodes, 2]\n",
    "                edge_index = edges[[1, 0], :],        # Edge Index [2, num_edges]\n",
    "                candidate_edges = candidate_edges[[1, 0], :],\n",
    "                y = theta_gains,            # Labels [num_nodes]\n",
    "                file = file,\n",
    "                G = G,\n",
    "                #recipient_parent_nodes = recipient_parent_nodes,\n",
    "                gene_absence_presence_matrix = gene_absence_presence_matrix,\n",
    "                node_times = node_times,\n",
    "                node_levels = node_levels,\n",
    "                hgt_events = hgt_events,\n",
    "            )\n",
    "\n",
    "    return data\n",
    "\n",
    "folder = \"/mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks\"\n",
    "\n",
    "files = [os.path.join(folder, f) for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "\n",
    "\"\"\"\n",
    "list_of_Data = []\n",
    "for f in files:\n",
    "    try:\n",
    "        d = load_file(f)\n",
    "        list_of_Data.append(d)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden von {f}: {e}\")\n",
    "\"\"\"\n",
    "print(f\"{len(list_of_Data)} Dateien erfolgreich geladen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e947a975-ec0e-477c-b752-59da59d6a261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 4958 graphs | Validation on 550 graphs | pos_weight=13.257\n",
      "Epoch 1/10 | time 179.5s\n",
      " TRAIN loss 1.1910 (rec 1.1908, don 0.0012) Prec/Rec/F1 0.264/0.538/0.354 | avg_expected_donor_dist 0.159\n",
      " VAL   loss 0.7946 (rec 0.7945, don 0.0015) Prec/Rec/F1 0.237/0.863/0.372 | avg_expected_donor_dist 0.118\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 2/10 | time 91.2s\n",
      " TRAIN loss 1.1273 (rec 1.1271, don 0.0013) Prec/Rec/F1 0.355/0.591/0.443 | avg_expected_donor_dist 0.153\n",
      " VAL   loss 1.1095 (rec 1.1094, don 0.0017) Prec/Rec/F1 0.310/0.581/0.404 | avg_expected_donor_dist 0.169\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 3/10 | time 141.3s\n",
      " TRAIN loss 1.0575 (rec 1.0574, don 0.0012) Prec/Rec/F1 0.470/0.599/0.526 | avg_expected_donor_dist 0.150\n",
      " VAL   loss 1.7704 (rec 1.7703, don 0.0012) Prec/Rec/F1 0.390/0.356/0.373 | avg_expected_donor_dist 0.228\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 4/10 | time 121.6s\n",
      " TRAIN loss 1.0319 (rec 1.0318, don 0.0011) Prec/Rec/F1 0.592/0.613/0.602 | avg_expected_donor_dist 0.126\n",
      " VAL   loss 2.1517 (rec 2.1517, don 0.0003) Prec/Rec/F1 0.363/0.306/0.332 | avg_expected_donor_dist 0.041\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 5/10 | time 110.0s\n",
      " TRAIN loss 0.9531 (rec 0.9530, don 0.0010) Prec/Rec/F1 0.687/0.662/0.674 | avg_expected_donor_dist 0.105\n",
      " VAL   loss 1.9464 (rec 1.9464, don 0.0001) Prec/Rec/F1 0.376/0.388/0.382 | avg_expected_donor_dist 0.011\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 6/10 | time 106.1s\n",
      " TRAIN loss 0.9146 (rec 0.9145, don 0.0006) Prec/Rec/F1 0.766/0.694/0.728 | avg_expected_donor_dist 0.062\n",
      " VAL   loss 2.4347 (rec 2.4347, don 0.0000) Prec/Rec/F1 0.325/0.338/0.331 | avg_expected_donor_dist 0.002\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 7/10 | time 152.4s\n",
      " TRAIN loss 0.8968 (rec 0.8968, don 0.0006) Prec/Rec/F1 0.782/0.711/0.745 | avg_expected_donor_dist 0.064\n",
      " VAL   loss 2.5620 (rec 2.5620, don 0.0000) Prec/Rec/F1 0.425/0.319/0.364 | avg_expected_donor_dist 0.006\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 8/10 | time 169.4s\n",
      " TRAIN loss 0.8201 (rec 0.8200, don 0.0006) Prec/Rec/F1 0.850/0.743/0.793 | avg_expected_donor_dist 0.058\n",
      " VAL   loss 2.6179 (rec 2.6178, don 0.0001) Prec/Rec/F1 0.394/0.312/0.348 | avg_expected_donor_dist 0.010\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 9/10 | time 104.0s\n",
      " TRAIN loss 0.7935 (rec 0.7934, don 0.0004) Prec/Rec/F1 0.868/0.759/0.810 | avg_expected_donor_dist 0.036\n",
      " VAL   loss 3.0926 (rec 3.0925, don 0.0001) Prec/Rec/F1 0.388/0.250/0.304 | avg_expected_donor_dist 0.024\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 10/10 | time 155.0s\n",
      " TRAIN loss 0.7652 (rec 0.7652, don 0.0006) Prec/Rec/F1 0.891/0.769/0.826 | avg_expected_donor_dist 0.049\n",
      " VAL   loss 3.2293 (rec 3.2293, don 0.0002) Prec/Rec/F1 0.372/0.263/0.308 | avg_expected_donor_dist 0.027\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import networkx as nx\n",
    "\n",
    "# -------------------------\n",
    "# Model components\n",
    "# -------------------------\n",
    "\n",
    "def make_mlp(input_dim, hidden_dims=[256,128], output_dim=None, dropout=0.1):\n",
    "    \"\"\"\n",
    "    Build an MLP that uses LayerNorm instead of BatchNorm.\n",
    "    LayerNorm is stable for batch size 1.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    dims = [input_dim] + hidden_dims\n",
    "    for i in range(len(hidden_dims)):\n",
    "        layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "        # use LayerNorm over feature dimension\n",
    "        layers.append(nn.LayerNorm(dims[i+1]))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "    if output_dim is not None:\n",
    "        layers.append(nn.Linear(dims[-1], output_dim))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class RecipientFinder(nn.Module):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - hgt_logit: raw logit (no sigmoid) shape [B]\n",
    "      - which_logits: raw logits for left/right shape [B,2]\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_vec_dim, aux_dim=3, hidden=[512,256], dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.seq_vec_dim = seq_vec_dim\n",
    "        self.aux_dim = aux_dim\n",
    "        mlp_input = seq_vec_dim * 4 + aux_dim\n",
    "        self.mlp = make_mlp(mlp_input, hidden_dims=hidden, output_dim=None, dropout=dropout)\n",
    "        self.hgt_head = nn.Linear(hidden[-1], 1)    # logit\n",
    "        self.which_head = nn.Linear(hidden[-1], 2)  # logits\n",
    "\n",
    "    def forward(self, left_vec, right_vec, aux_feats):\n",
    "        # left_vec/right_vec: [B, D]\n",
    "        absdiff = torch.abs(left_vec - right_vec)\n",
    "        prod = left_vec * right_vec\n",
    "        x = torch.cat([left_vec, right_vec, absdiff, prod, aux_feats], dim=1)\n",
    "        h = self.mlp(x)\n",
    "        hgt_logit = self.hgt_head(h).squeeze(-1)         # [B]\n",
    "        which_logits = self.which_head(h)                # [B,2]\n",
    "        return hgt_logit, which_logits\n",
    "\n",
    "class DonorFinder(nn.Module):\n",
    "    \"\"\"\n",
    "    Scores candidate donors relative to a recipient vector.\n",
    "    Returns raw scores (logits) of shape [M]\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_vec_dim, aux_dim=3, hidden=[512,256], dropout=0.1):\n",
    "        super().__init__()\n",
    "        mlp_input = seq_vec_dim * 4 + aux_dim\n",
    "        self.score_mlp = make_mlp(mlp_input, hidden_dims=hidden, output_dim=1, dropout=dropout)\n",
    "\n",
    "    def forward(self, recipient_vec, donor_vecs, aux_feats):\n",
    "        # recipient_vec: [D] or [1,D]\n",
    "        if recipient_vec.dim() == 1:\n",
    "            r = recipient_vec.unsqueeze(0).expand(donor_vecs.shape[0], -1)\n",
    "        else:\n",
    "            r = recipient_vec.expand(donor_vecs.shape[0], -1)\n",
    "        absdiff = torch.abs(r - donor_vecs)\n",
    "        prod = r * donor_vecs\n",
    "        x = torch.cat([r, donor_vecs, absdiff, prod, aux_feats], dim=1)\n",
    "        scores = self.score_mlp(x).squeeze(-1)  # [M]\n",
    "        return scores\n",
    "\n",
    "class HGTDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Orchestrator that contains RecipientFinder and DonorFinder and provides utility compute_aggregates.\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_vec_dim, aux_recipient_dim=3, aux_donor_dim=3,\n",
    "                 rec_hidden=[32,16], donor_hidden=[32,16], hgt_threshold=0.5, topk_donors=1):\n",
    "        super().__init__()\n",
    "        self.recipient_finder = RecipientFinder(seq_vec_dim, aux_dim=aux_recipient_dim, hidden=rec_hidden)\n",
    "        self.donor_finder = DonorFinder(seq_vec_dim, aux_dim=aux_donor_dim, hidden=donor_hidden)\n",
    "        self.hgt_threshold = hgt_threshold\n",
    "        self.topk_donors = topk_donors\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_aggregates(G: nx.DiGraph, node_seq_matrix: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Sum one-hot vectors of leaves under each node.\n",
    "        Assumes node_seq_matrix rows correspond to node indices aligned with node ids in G (0..N-1),\n",
    "        otherwise adapt mapping externally.\n",
    "        \"\"\"\n",
    "        nodes = list(G.nodes)\n",
    "        N = len(nodes)\n",
    "        # Build mapping node_id -> index if node ids are not 0..N-1\n",
    "        if nodes == list(range(N)):\n",
    "            node_to_idx = {n: n for n in nodes}\n",
    "        else:\n",
    "            node_to_idx = {n: i for i, n in enumerate(nodes)}\n",
    "        children = {n: list(G.successors(n)) for n in nodes}\n",
    "\n",
    "        # We'll create agg in the same indexing as node_to_idx\n",
    "        D = node_seq_matrix.shape[1]\n",
    "        agg = torch.zeros((N, D), dtype=node_seq_matrix.dtype, device=node_seq_matrix.device)\n",
    "\n",
    "        # If node_seq_matrix ordering matches node_to_idx, copy directly\n",
    "        # Attempt to copy: if node_seq_matrix has N rows we copy by index order 0..N-1\n",
    "        if node_seq_matrix.shape[0] == N:\n",
    "            try:\n",
    "                agg = node_seq_matrix.clone()\n",
    "            except Exception:\n",
    "                agg = node_seq_matrix.clone().to(node_seq_matrix.device)\n",
    "        else:\n",
    "            # fallback: place available rows in order of nodes if possible (rare)\n",
    "            raise RuntimeError(\"node_seq_matrix rows != number of nodes; please supply node-ordered matrix.\")\n",
    "\n",
    "        # children processed before parent: do topological sort\n",
    "        topo = list(nx.topological_sort(G))\n",
    "        for node in reversed(topo):\n",
    "            i = node_to_idx[node]\n",
    "            for c in children[node]:\n",
    "                j = node_to_idx[c]\n",
    "                agg[i] = agg[i] + agg[j]\n",
    "        return agg\n",
    "\n",
    "# -------------------------\n",
    "# Utilities for training\n",
    "# -------------------------\n",
    "def tree_distance_in_edges(G: nx.DiGraph, node_a, node_b, max_penalty=None):\n",
    "    \"\"\"\n",
    "    Shortest path length in undirected tree, fallback to max_penalty or len(G.nodes)\n",
    "    \"\"\"\n",
    "    und = G.to_undirected()\n",
    "    try:\n",
    "        return nx.shortest_path_length(und, source=node_a, target=node_b)\n",
    "    except Exception:\n",
    "        return max_penalty if max_penalty is not None else len(G.nodes)\n",
    "\n",
    "def build_labels_from_data(data):\n",
    "    \"\"\"\n",
    "    Build maps:\n",
    "      - recip_label_map[parent_node] = (0/1, true_recipient_child_node or None)\n",
    "      - donor_map[parent_node] = true_donor_parent_node or None\n",
    "    Uses data.hgt_events which contains {0: structured_array([...])}\n",
    "    \"\"\"\n",
    "    recip_label_map = {}\n",
    "    donor_map = {}\n",
    "    G = data.G\n",
    "    for n in G.nodes:\n",
    "        children = list(G.successors(n))\n",
    "        if len(children) == 2:\n",
    "            recip_label_map[n] = (0, None)\n",
    "            donor_map[n] = None\n",
    "\n",
    "    # parse events dict\n",
    "    hgt_events = getattr(data, \"hgt_events\", None)\n",
    "    if not hgt_events:\n",
    "        return recip_label_map, donor_map\n",
    "\n",
    "    for site_id, arr in hgt_events.items():\n",
    "        # arr could be structured numpy array with fields:\n",
    "        # ('recipient_parent_node', 'recipient_child_node', 'leaf', 'donor_parent_node', 'donor_child_node')\n",
    "        # Might be shape (k,) or (k,).\n",
    "        try:\n",
    "            ln = len(arr)\n",
    "        except Exception:\n",
    "            continue\n",
    "        for i in range(ln):\n",
    "            rec_parent = int(arr[i]['recipient_parent_node']) if 'recipient_parent_node' in arr.dtype.names else None\n",
    "            rec_child = int(arr[i]['recipient_child_node']) if 'recipient_child_node' in arr.dtype.names else None\n",
    "            donor_parent = int(arr[i]['donor_parent_node']) if 'donor_parent_node' in arr.dtype.names else None\n",
    "\n",
    "            if rec_parent is None:\n",
    "                continue\n",
    "            if rec_parent in recip_label_map:\n",
    "                recip_label_map[rec_parent] = (1, rec_child)\n",
    "                donor_map[rec_parent] = donor_parent\n",
    "    return recip_label_map, donor_map\n",
    "\n",
    "# -------------------------\n",
    "# Training loop (fully adjusted)\n",
    "# -------------------------\n",
    "def train_hgt_detector(model: HGTDetector, list_of_Data, epochs=10, val_frac=0.1, lr=1e-3,\n",
    "                       lambda_donor=0.1, hgt_threshold=0.5, max_candidates=300,\n",
    "                       device=None, clip_grad=5.0):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "    N = len(list_of_Data)\n",
    "    idxs = list(range(N))\n",
    "    random.shuffle(idxs)\n",
    "    n_val = max(1, int(val_frac * N))\n",
    "    val_idx = set(idxs[:n_val])\n",
    "    train_idx = idxs[n_val:]\n",
    "\n",
    "    # Estimate pos_weight for BCEWithLogitsLoss on training set (clamped)\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    for i in train_idx:\n",
    "        d = list_of_Data[i]\n",
    "        recip_map, _ = build_labels_from_data(d)\n",
    "        for p, (lbl, _) in recip_map.items():\n",
    "            if lbl == 1:\n",
    "                pos_count += 1\n",
    "            else:\n",
    "                neg_count += 1\n",
    "    pos_count = max(pos_count, 1)\n",
    "    neg_count = max(neg_count, 1)\n",
    "    ratio = neg_count / pos_count\n",
    "    pos_weight_val = max(1.0, min(ratio, 50.0))  # clamp between 1 and 50\n",
    "    pos_weight = torch.tensor([pos_weight_val], device=device)\n",
    "    bce_loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    print(f\"Training on {len(train_idx)} graphs | Validation on {len(val_idx)} graphs | pos_weight={pos_weight_val:.3f}\")\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_rec_loss = 0.0\n",
    "        train_don_loss = 0.0\n",
    "        train_samples = 0\n",
    "\n",
    "        # metrics\n",
    "        tp = 0; fp = 0; fn = 0\n",
    "        donor_expected_distance_accum = 0.0\n",
    "        donor_expected_count = 0\n",
    "\n",
    "        random.shuffle(train_idx)\n",
    "        for idx in train_idx:\n",
    "            data = list_of_Data[idx]\n",
    "            # move seqs to device once\n",
    "            data.hot_encoded_nucleotide_sequences = data.hot_encoded_nucleotide_sequences.to(device)\n",
    "\n",
    "            recip_map, donor_map = build_labels_from_data(data)\n",
    "            parents = [n for n in data.G.nodes if len(list(data.G.successors(n))) == 2]\n",
    "            if len(parents) == 0:\n",
    "                continue\n",
    "\n",
    "            # compute aggregates\n",
    "            agg = model.compute_aggregates(data.G, data.hot_encoded_nucleotide_sequences)  # [N,D]\n",
    "\n",
    "            # collect batch lists\n",
    "            L_list = []\n",
    "            R_list = []\n",
    "            aux_list = []\n",
    "            true_label_list = []\n",
    "            true_rec_child_list = []\n",
    "            true_donor_parent_list = []\n",
    "            parent_nodes_list = []\n",
    "\n",
    "            for parent in parents:\n",
    "                children = list(data.G.successors(parent))\n",
    "                left, right = children[0], children[1]\n",
    "                L_list.append(agg[left].unsqueeze(0))\n",
    "                R_list.append(agg[right].unsqueeze(0))\n",
    "                # aux: node_time, level, gene_frac\n",
    "                node_time = float(data.G.nodes[parent].get('node_time', 0.0))\n",
    "                node_level = float(data.G.nodes[parent].get('level', 0.0))\n",
    "                # avoid divide by zero\n",
    "                parent_sum = agg[parent].sum().clamp(min=1.0)\n",
    "                gene_frac = ((agg[left].sum() + agg[right].sum()) / parent_sum).unsqueeze(0).unsqueeze(1)\n",
    "                aux = torch.tensor([[node_time, node_level, float(gene_frac.item())]], device=device, dtype=torch.float32)\n",
    "                aux_list.append(aux)\n",
    "                lbl, true_rec = recip_map.get(parent, (0, None))\n",
    "                true_label_list.append(lbl)\n",
    "                true_rec_child_list.append(true_rec)\n",
    "                true_donor_parent_list.append(donor_map.get(parent, None))\n",
    "                parent_nodes_list.append(parent)\n",
    "\n",
    "            left_vecs = torch.cat(L_list, dim=0)      # [P, D]\n",
    "            right_vecs = torch.cat(R_list, dim=0)     # [P, D]\n",
    "            aux_feats = torch.cat(aux_list, dim=0)    # [P, 3]\n",
    "            true_labels = torch.tensor(true_label_list, dtype=torch.float32, device=device)  # [P]\n",
    "\n",
    "            # Recipient forward\n",
    "            hgt_logits, which_logits = model.recipient_finder(left_vecs, right_vecs, aux_feats)\n",
    "            rec_loss = bce_loss_fn(hgt_logits, true_labels)\n",
    "\n",
    "            # compute predicted probabilities & which\n",
    "            probs = torch.sigmoid(hgt_logits).detach().cpu().numpy()\n",
    "            which_pred = torch.argmax(which_logits.detach().cpu(), dim=1).tolist()\n",
    "\n",
    "            # Donor loss (differentiable expected distance) aggregated for events\n",
    "            donor_loss_total = torch.tensor(0.0, device=device)\n",
    "            donor_events = 0\n",
    "            # For each parent with true_label==1, check whether recipient predicted correctly\n",
    "            for i_parent, parent in enumerate(parent_nodes_list):\n",
    "                lbl = int(true_labels[i_parent].item())\n",
    "                if lbl != 1:\n",
    "                    continue\n",
    "                true_rec_child = true_rec_child_list[i_parent]\n",
    "                true_donor_parent = true_donor_parent_list[i_parent]\n",
    "                # predicted recipient child:\n",
    "                pred_rec_child = list(data.G.successors(parent))[which_pred[i_parent]]\n",
    "                pred_prob = float(probs[i_parent])\n",
    "                # condition: only add donor loss if recipient correctly identified\n",
    "                if (pred_prob >= hgt_threshold) and (true_rec_child is not None) and (pred_rec_child == true_rec_child):\n",
    "                    # build candidate list: nodes with node_time < recipient_time\n",
    "                    rec_node = pred_rec_child\n",
    "                    rec_time = float(data.G.nodes[rec_node].get('node_time', 0.0))\n",
    "                    candidates = [n for n in data.G.nodes if float(data.G.nodes[n].get('node_time',0.0)) < rec_time and n != rec_node]\n",
    "                    # ensure true donor parent is among candidates - if not, append it (keeps learning signal)\n",
    "                    if true_donor_parent is not None and true_donor_parent not in candidates:\n",
    "                        candidates.append(true_donor_parent)\n",
    "                    if len(candidates) == 0:\n",
    "                        # no candidates -> penalize by max distance (normalized)\n",
    "                        max_pen = len(data.G.nodes)\n",
    "                        donor_loss_total = donor_loss_total + (torch.tensor(float(max_pen), device=device) / float(max_pen))\n",
    "                        donor_events += 1\n",
    "                        continue\n",
    "\n",
    "                    # sample candidates if too many\n",
    "                    if len(candidates) > max_candidates:\n",
    "                        random.shuffle(candidates)\n",
    "                        candidates = candidates[:max_candidates]\n",
    "                    cand_idx_tensor = torch.tensor(candidates, dtype=torch.long, device=device)\n",
    "\n",
    "                    donor_vecs = agg[cand_idx_tensor]  # [M, D]\n",
    "                    donor_times = torch.tensor([float(data.G.nodes[n].get('node_time',0.0)) for n in candidates], device=device)\n",
    "                    donor_levels = torch.tensor([float(data.G.nodes[n].get('level',0.0)) for n in candidates], device=device)\n",
    "                    time_diff = (rec_time - donor_times).unsqueeze(1)              # [M,1]\n",
    "                    level_diff = (data.G.nodes[rec_node].get('level',0.0) - donor_levels).unsqueeze(1)\n",
    "                    donor_frac = (donor_vecs.sum(dim=1).unsqueeze(1) / (agg.sum(dim=1).clamp(min=1.0)[cand_idx_tensor].unsqueeze(1))).clamp(0.0,1.0)\n",
    "                    donor_aux = torch.cat([time_diff, level_diff, donor_frac], dim=1)\n",
    "\n",
    "                    # get raw scores (no softmax yet)\n",
    "                    scores = model.donor_finder(agg[rec_node].to(device), donor_vecs, donor_aux)  # [M]\n",
    "\n",
    "                    # compute distances vector to true donor\n",
    "                    # ensure we have a numeric dist for each candidate\n",
    "                    und = data.G.to_undirected()\n",
    "                    max_pen = len(data.G.nodes)\n",
    "                    dists = []\n",
    "                    for cand in candidates:\n",
    "                        if true_donor_parent is None:\n",
    "                            # if no true donor known, give zero distance (no loss)\n",
    "                            dists.append(0.0)\n",
    "                        else:\n",
    "                            try:\n",
    "                                dd = nx.shortest_path_length(und, source=cand, target=true_donor_parent)\n",
    "                                dists.append(float(dd))\n",
    "                            except Exception:\n",
    "                                dists.append(float(max_pen))\n",
    "                    dists = torch.tensor(dists, dtype=torch.float32, device=device)  # [M]\n",
    "\n",
    "                    # Softmax probabilities over scores (temperature can be used)\n",
    "                    probs_soft = torch.softmax(scores, dim=0)  # [M]\n",
    "                    # Expected distance (differentiable)\n",
    "                    expected_dist = torch.dot(probs_soft, dists)  # scalar\n",
    "                    # Normalize by max_pen to keep scale ~[0,1]\n",
    "                    expected_dist = expected_dist / float(max_pen)\n",
    "\n",
    "                    donor_loss_total = donor_loss_total + expected_dist\n",
    "                    donor_events += 1\n",
    "\n",
    "                    # accumulate for reporting expected donor distance (as float)\n",
    "                    donor_expected_distance_accum += float((expected_dist * float(max_pen)).item())\n",
    "                    donor_expected_count += 1\n",
    "\n",
    "            # Normalize donor loss by donor_events (if >0)\n",
    "            if donor_events > 0:\n",
    "                donor_loss = donor_loss_total / donor_events\n",
    "            else:\n",
    "                donor_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "            loss = rec_loss + lambda_donor * donor_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), clip_grad)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_rec_loss += rec_loss.item()\n",
    "            train_don_loss += float(donor_loss.item())\n",
    "            train_samples += 1\n",
    "\n",
    "            # update metrics for recipient detection\n",
    "            for i_parent, parent in enumerate(parent_nodes_list):\n",
    "                true_lbl = int(true_labels[i_parent].item())\n",
    "                pred_prob = float(probs[i_parent])\n",
    "                pred_class = 1 if pred_prob >= hgt_threshold else 0\n",
    "                if pred_class == 1 and true_lbl == 1:\n",
    "                    tp += 1\n",
    "                if pred_class == 1 and true_lbl == 0:\n",
    "                    fp += 1\n",
    "                if pred_class == 0 and true_lbl == 1:\n",
    "                    fn += 1\n",
    "\n",
    "        # End epoch training stats\n",
    "        avg_train_loss = train_loss / max(1, train_samples)\n",
    "        avg_train_rec = train_rec_loss / max(1, train_samples)\n",
    "        avg_train_don = train_don_loss / max(1, train_samples)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        avg_expected_donor_distance = (donor_expected_distance_accum / donor_expected_count) if donor_expected_count > 0 else 0.0\n",
    "\n",
    "        # Validation pass\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            val_rec_loss = 0.0\n",
    "            val_don_loss = 0.0\n",
    "            val_samples = 0\n",
    "            v_tp = v_fp = v_fn = 0\n",
    "            v_donor_expected_dist_acc = 0.0\n",
    "            v_donor_expected_cnt = 0\n",
    "\n",
    "            for idx in val_idx:\n",
    "                data = list_of_Data[idx]\n",
    "                data.hot_encoded_nucleotide_sequences = data.hot_encoded_nucleotide_sequences.to(device)\n",
    "                recip_map, donor_map = build_labels_from_data(data)\n",
    "                parents = [n for n in data.G.nodes if len(list(data.G.successors(n))) == 2]\n",
    "                if len(parents) == 0:\n",
    "                    continue\n",
    "                agg = model.compute_aggregates(data.G, data.hot_encoded_nucleotide_sequences)\n",
    "                # collect lists\n",
    "                L_list, R_list, aux_list = [], [], []\n",
    "                true_labels = []\n",
    "                true_rec_child_list = []\n",
    "                true_donor_parent_list = []\n",
    "                parent_nodes_list = []\n",
    "                for parent in parents:\n",
    "                    left, right = list(data.G.successors(parent))[0], list(data.G.successors(parent))[1]\n",
    "                    L_list.append(agg[left].unsqueeze(0))\n",
    "                    R_list.append(agg[right].unsqueeze(0))\n",
    "                    node_time = float(data.G.nodes[parent].get('node_time', 0.0))\n",
    "                    node_level = float(data.G.nodes[parent].get('level', 0.0))\n",
    "                    parent_sum = agg[parent].sum().clamp(min=1.0)\n",
    "                    gene_frac = ((agg[left].sum() + agg[right].sum()) / parent_sum).unsqueeze(0).unsqueeze(1)\n",
    "                    aux = torch.tensor([[node_time, node_level, float(gene_frac.item())]], device=device, dtype=torch.float32)\n",
    "                    aux_list.append(aux)\n",
    "                    lbl, true_rec = recip_map.get(parent, (0, None))\n",
    "                    true_labels.append(lbl)\n",
    "                    true_rec_child_list.append(true_rec)\n",
    "                    true_donor_parent_list.append(donor_map.get(parent, None))\n",
    "                    parent_nodes_list.append(parent)\n",
    "                left_vecs = torch.cat(L_list, dim=0)\n",
    "                right_vecs = torch.cat(R_list, dim=0)\n",
    "                aux_feats = torch.cat(aux_list, dim=0)\n",
    "                true_labels = torch.tensor(true_labels, dtype=torch.float32, device=device)\n",
    "\n",
    "                hgt_logits, which_logits = model.recipient_finder(left_vecs, right_vecs, aux_feats)\n",
    "                rec_loss = bce_loss_fn(hgt_logits, true_labels)\n",
    "\n",
    "                # donor loss computation: same expected-distance approach but without gradients\n",
    "                probs = torch.sigmoid(hgt_logits).cpu().numpy()\n",
    "                which_pred = torch.argmax(which_logits, dim=1).tolist()\n",
    "\n",
    "                donor_loss_total = 0.0\n",
    "                donor_events = 0\n",
    "                for i_parent, parent in enumerate(parent_nodes_list):\n",
    "                    lbl = int(true_labels[i_parent].item())\n",
    "                    if lbl != 1:\n",
    "                        continue\n",
    "                    true_rec_child = true_rec_child_list[i_parent]\n",
    "                    true_donor_parent = true_donor_parent_list[i_parent]\n",
    "                    pred_rec_child = list(data.G.successors(parent))[which_pred[i_parent]]\n",
    "                    pred_prob = float(probs[i_parent])\n",
    "                    if (pred_prob >= hgt_threshold) and (true_rec_child is not None) and (pred_rec_child == true_rec_child):\n",
    "                        rec_node = pred_rec_child\n",
    "                        rec_time = float(data.G.nodes[rec_node].get('node_time', 0.0))\n",
    "                        candidates = [n for n in data.G.nodes if float(data.G.nodes[n].get('node_time',0.0)) < rec_time and n != rec_node]\n",
    "                        if true_donor_parent is not None and true_donor_parent not in candidates:\n",
    "                            candidates.append(true_donor_parent)\n",
    "                        if len(candidates) == 0:\n",
    "                            donor_loss_total += 1.0\n",
    "                            donor_events += 1\n",
    "                            continue\n",
    "                        if len(candidates) > max_candidates:\n",
    "                            random.shuffle(candidates)\n",
    "                            candidates = candidates[:max_candidates]\n",
    "                        cand_idx_tensor = torch.tensor(candidates, dtype=torch.long, device=device)\n",
    "                        donor_vecs = agg[cand_idx_tensor]\n",
    "                        donor_times = torch.tensor([float(data.G.nodes[n].get('node_time',0.0)) for n in candidates], device=device)\n",
    "                        donor_levels = torch.tensor([float(data.G.nodes[n].get('level',0.0)) for n in candidates], device=device)\n",
    "                        time_diff = (rec_time - donor_times).unsqueeze(1)\n",
    "                        level_diff = (data.G.nodes[rec_node].get('level',0.0) - donor_levels).unsqueeze(1)\n",
    "                        donor_frac = (donor_vecs.sum(dim=1).unsqueeze(1) / (agg.sum(dim=1).clamp(min=1.0)[cand_idx_tensor].unsqueeze(1))).clamp(0.0,1.0)\n",
    "                        donor_aux = torch.cat([time_diff, level_diff, donor_frac], dim=1)\n",
    "                        scores = model.donor_finder(agg[rec_node].to(device), donor_vecs, donor_aux)\n",
    "                        # distances\n",
    "                        und = data.G.to_undirected()\n",
    "                        max_pen = len(data.G.nodes)\n",
    "                        dists = []\n",
    "                        for cand in candidates:\n",
    "                            if true_donor_parent is None:\n",
    "                                dists.append(0.0)\n",
    "                            else:\n",
    "                                try:\n",
    "                                    dd = nx.shortest_path_length(und, source=cand, target=true_donor_parent)\n",
    "                                    dists.append(float(dd))\n",
    "                                except Exception:\n",
    "                                    dists.append(float(max_pen))\n",
    "                        dists = torch.tensor(dists, dtype=torch.float32, device=device)\n",
    "                        probs_soft = torch.softmax(scores, dim=0)\n",
    "                        expected_dist = float(torch.dot(probs_soft, dists).item()) / float(max_pen)\n",
    "                        donor_loss_total += expected_dist\n",
    "                        donor_events += 1\n",
    "                        v_donor_expected_dist_acc += (expected_dist * float(max_pen))\n",
    "                        v_donor_expected_cnt += 1\n",
    "\n",
    "                donor_loss = (donor_loss_total / donor_events) if donor_events > 0 else 0.0\n",
    "                loss = rec_loss.item() + lambda_donor * donor_loss\n",
    "\n",
    "                val_loss += loss\n",
    "                val_rec_loss += rec_loss.item()\n",
    "                val_don_loss += donor_loss\n",
    "                val_samples += 1\n",
    "\n",
    "                # update val metrics for recipient detection\n",
    "                for i_parent, parent in enumerate(parent_nodes_list):\n",
    "                    true_lbl = int(true_labels[i_parent].item())\n",
    "                    pred_prob = float(probs[i_parent])\n",
    "                    pred_class = 1 if pred_prob >= hgt_threshold else 0\n",
    "                    if pred_class == 1 and true_lbl == 1:\n",
    "                        v_tp += 1\n",
    "                    if pred_class == 1 and true_lbl == 0:\n",
    "                        v_fp += 1\n",
    "                    if pred_class == 0 and true_lbl == 1:\n",
    "                        v_fn += 1\n",
    "\n",
    "            avg_val_loss = val_loss / max(1, val_samples)\n",
    "            avg_val_rec = val_rec_loss / max(1, val_samples)\n",
    "            avg_val_don = val_don_loss / max(1, val_samples)\n",
    "            v_precision = v_tp / (v_tp + v_fp) if (v_tp + v_fp) > 0 else 0.0\n",
    "            v_recall = v_tp / (v_tp + v_fn) if (v_tp + v_fn) > 0 else 0.0\n",
    "            v_f1 = 2 * v_precision * v_recall / (v_precision + v_recall) if (v_precision + v_recall) > 0 else 0.0\n",
    "            avg_v_donor_expected = (v_donor_expected_dist_acc / v_donor_expected_cnt) if v_donor_expected_cnt > 0 else 0.0\n",
    "\n",
    "        # scheduler step (use validation loss)\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        t1 = time.time()\n",
    "        print(f\"Epoch {epoch}/{epochs} | time {t1-t0:.1f}s\")\n",
    "        print(f\" TRAIN loss {avg_train_loss:.4f} (rec {avg_train_rec:.4f}, don {avg_train_don:.4f}) \"\n",
    "              f\"Prec/Rec/F1 {precision:.3f}/{recall:.3f}/{f1:.3f} | avg_expected_donor_dist {avg_expected_donor_distance:.3f}\")\n",
    "        print(f\" VAL   loss {avg_val_loss:.4f} (rec {avg_val_rec:.4f}, don {avg_val_don:.4f}) \"\n",
    "              f\"Prec/Rec/F1 {v_precision:.3f}/{v_recall:.3f}/{v_f1:.3f} | avg_expected_donor_dist {avg_v_donor_expected:.3f}\")\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# choose a sample data to get seq_vec_dim\n",
    "sample_data = random.choice(list_of_Data)\n",
    "seq_vec_dim = sample_data.hot_encoded_nucleotide_sequences.shape[1]\n",
    "model = HGTDetector(seq_vec_dim=seq_vec_dim, hgt_threshold=0.5, topk_donors=1)\n",
    "\n",
    "trained_model = train_hgt_detector(model, list_of_Data,\n",
    "                                  epochs=10,\n",
    "                                  val_frac=0.1,\n",
    "                                  lr=1e-3,\n",
    "                                  lambda_donor=0.1,\n",
    "                                  hgt_threshold=0.5,\n",
    "                                  max_candidates=300,\n",
    "                                  device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                                  clip_grad=5.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fcb62a6-cee0-4878-848e-0cd85ad1f2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(\n",
      "  edge_index=[2, 8],\n",
      "  y=[9],\n",
      "  hot_encoded_nucleotide_sequences=[9, 1500],\n",
      "  candidate_edges=[2, 18],\n",
      "  file='/mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks/simulation_6198.h5',\n",
      "  G=DiGraph with 9 nodes and 8 edges,\n",
      "  gene_absence_presence_matrix=[5],\n",
      "  node_times=[9],\n",
      "  node_levels=[9],\n",
      "  hgt_events={}\n",
      ")\n",
      "{}\n",
      "[3, 4, 6, 2, 1, 0, 7, 5, 8]\n",
      "[(5, 0), (5, 1), (8, 5), (8, 7), (7, 2), (7, 6), (6, 4), (6, 3)]\n"
     ]
    }
   ],
   "source": [
    "data = random.choice(list_of_Data)\n",
    "\n",
    "print(data)\n",
    "print(data.hgt_events)\n",
    "build_labels_from_data(data)\n",
    "\n",
    "print(list(nx.topological_sort(data.G))[::-1])\n",
    "print(data.G.edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43a8c867-8e05-4d9b-9f50-c78c513b836c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ausgewählte Datei: /mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks/simulation_4333.h5\n",
      "Anzahl der Knoten: 9\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Module [HGTDetector] is missing the required \"forward\" function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m model\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# kein Training\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 21\u001b[0m     events \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 4️⃣ Ergebnisse anschauen\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnzahl vorhergesagter HGT-Ereignisse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(events)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch/nn/modules/module.py:399\u001b[0m, in \u001b[0;36m_forward_unimplemented\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_unimplemented\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Define the computation performed at every call.\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m    Should be overridden by all subclasses.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m        registered hooks while the latter silently ignores them.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] is missing the required \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m function\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    401\u001b[0m     )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Module [HGTDetector] is missing the required \"forward\" function"
     ]
    }
   ],
   "source": [
    "import torch, random\n",
    "\n",
    "# 1️⃣ Einen zufälligen Baum aus deiner Liste wählen\n",
    "data = random.choice(list_of_Data)\n",
    "\n",
    "print(f\"Ausgewählte Datei: {data.file}\")\n",
    "print(f\"Anzahl der Knoten: {data.hot_encoded_nucleotide_sequences.shape[0]}\")\n",
    "\n",
    "# 2️⃣ Modell initialisieren\n",
    "seq_vec_dim = data.hot_encoded_nucleotide_sequences.shape[1]\n",
    "\n",
    "model = HGTDetector(seq_vec_dim=seq_vec_dim)\n",
    "\n",
    "# Wenn du eine GPU hast:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 3️⃣ Modell auf den zufälligen Baum anwenden\n",
    "model.eval()  # kein Training\n",
    "with torch.no_grad():\n",
    "    events = model(data)\n",
    "\n",
    "# 4️⃣ Ergebnisse anschauen\n",
    "print(f\"\\nAnzahl vorhergesagter HGT-Ereignisse: {len(events)}\")\n",
    "for e in events[:5]:  # nur die ersten 5 ausgeben\n",
    "    print(\"───────────────────────────────\")\n",
    "    print(f\"Parent node: {e['parent_node']}\")\n",
    "    print(f\"Recipient child: {e['recipient_child']}\")\n",
    "    print(f\"Prob(HGT): {e['prob_hgt']:.3f}\")\n",
    "    if e.get(\"donor_candidates\"):\n",
    "        best = e[\"donor_candidates\"][0]\n",
    "        print(f\"  → Top donor: {best['donor_node']}  (score: {best['score']:.3f})\")\n",
    "    else:\n",
    "        print(\"  → Kein Donor gefunden.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d40825-9fb5-4ad3-a137-6504443e2fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
