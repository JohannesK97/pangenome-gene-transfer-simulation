{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06d82946-bd9d-4f34-bfb0-c50b36e61fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/uhewm/OneDrive/PhD/Project No.2/pangenome-gene-transfer-simulation/ImportData.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edges = torch.tensor(graph_properties[1], dtype=torch.long)  # [2, num_edges]\n",
      "/mnt/c/Users/uhewm/OneDrive/PhD/Project No.2/pangenome-gene-transfer-simulation/ImportData.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coords = torch.tensor(graph_properties[2].T)           # [2, num_nodes]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m data \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m random\u001b[38;5;241m.\u001b[39msample(all_files, \u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#for file in all_files:\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     single_data \u001b[38;5;241m=\u001b[39m \u001b[43mImportData\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     data\u001b[38;5;241m.\u001b[39mappend(single_data)\n\u001b[1;32m     36\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(data)\n",
      "File \u001b[0;32m/mnt/c/Users/uhewm/OneDrive/PhD/Project No.2/pangenome-gene-transfer-simulation/ImportData.py:56\u001b[0m, in \u001b[0;36mload_file\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m children:\n\u001b[1;32m     54\u001b[0m     allele_sum \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(G\u001b[38;5;241m.\u001b[39mnodes[child][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallele_distance\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m children)\n\u001b[0;32m---> 56\u001b[0m     child1, child2 \u001b[38;5;241m=\u001b[39m children\n\u001b[1;32m     57\u001b[0m     n \u001b[38;5;241m=\u001b[39m snp_percentage_of_gen_to_absolute_number(G\u001b[38;5;241m.\u001b[39mnodes[child1][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue_allele_distance\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m gene_length, gene_length) \n\u001b[1;32m     58\u001b[0m     m \u001b[38;5;241m=\u001b[39m snp_percentage_of_gen_to_absolute_number(G\u001b[38;5;241m.\u001b[39mnodes[child2][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue_allele_distance\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m gene_length, gene_length)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, DirGNNConv, GraphConv, GATConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "import random, h5py, pickle, glob, os\n",
    "import networkx as nx\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import ImportData\n",
    "import numpy as np\n",
    "\n",
    "import importlib\n",
    "importlib.reload(ImportData)\n",
    "\n",
    "### WINDOWS:\n",
    "#output_dir = r\"C:\\Users\\uhewm\\Desktop\\ProjectHGT\\simulation_chunks(4)\"\n",
    "output_dir = r\"C:\\Users\\uhewm\\Desktop\\ProjectHGT\\simulation_chunks\"\n",
    "all_files = sorted(glob.glob(os.path.join(output_dir, \"*.h5\")))\n",
    "\n",
    "### LINUX:\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "output_dir = Path(\"/mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks\")\n",
    "all_files = sorted(glob.glob(str(output_dir / \"*.h5\")))\n",
    "\n",
    "data = []\n",
    "\n",
    "for file in random.sample(all_files, 100):\n",
    "#for file in all_files:\n",
    "    single_data = ImportData.load_file(file)\n",
    "    data.append(single_data)\n",
    "\n",
    "random.shuffle(data)\n",
    "split_idx = int(0.8 * len(data))\n",
    "train_data = data[:split_idx]\n",
    "test_data = data[split_idx:]\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16)\n",
    "evaluate_loader = DataLoader(data, batch_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b3e3e3e-a23c-4a32-a2ae-e716e60ba1fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18432/650970100.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos_weight = torch.tensor((ratio**0.3), dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos Weight: 4.97\n",
      "Epoch 01 | Loss: 0.1435 | Acc: 0.995 | Prec: 0.475 | Rec: 0.813 | F1: 0.599\n",
      "Epoch 02 | Loss: 0.0330 | Acc: 0.997 | Prec: 0.601 | Rec: 0.782 | F1: 0.680\n",
      "Epoch 03 | Loss: 0.0250 | Acc: 0.997 | Prec: 0.622 | Rec: 0.778 | F1: 0.692\n",
      "Epoch 04 | Loss: 0.0211 | Acc: 0.997 | Prec: 0.668 | Rec: 0.756 | F1: 0.709\n",
      "Epoch 05 | Loss: 0.0184 | Acc: 0.997 | Prec: 0.677 | Rec: 0.767 | F1: 0.720\n",
      "Epoch 06 | Loss: 0.0173 | Acc: 0.997 | Prec: 0.704 | Rec: 0.789 | F1: 0.744\n",
      "Epoch 07 | Loss: 0.0166 | Acc: 0.997 | Prec: 0.608 | Rec: 0.847 | F1: 0.708\n",
      "Epoch 08 | Loss: 0.0160 | Acc: 0.997 | Prec: 0.590 | Rec: 0.859 | F1: 0.700\n",
      "Epoch 09 | Loss: 0.0152 | Acc: 0.997 | Prec: 0.652 | Rec: 0.836 | F1: 0.733\n",
      "Epoch 10 | Loss: 0.0151 | Acc: 0.997 | Prec: 0.679 | Rec: 0.817 | F1: 0.742\n",
      "Epoch 11 | Loss: 0.0150 | Acc: 0.998 | Prec: 0.777 | Rec: 0.755 | F1: 0.766\n",
      "Epoch 12 | Loss: 0.0147 | Acc: 0.997 | Prec: 0.624 | Rec: 0.861 | F1: 0.723\n",
      "Epoch 13 | Loss: 0.0145 | Acc: 0.998 | Prec: 0.738 | Rec: 0.788 | F1: 0.762\n",
      "Epoch 14 | Loss: 0.0146 | Acc: 0.997 | Prec: 0.682 | Rec: 0.830 | F1: 0.749\n",
      "Epoch 15 | Loss: 0.0146 | Acc: 0.997 | Prec: 0.642 | Rec: 0.842 | F1: 0.729\n",
      "Epoch 16 | Loss: 0.0142 | Acc: 0.998 | Prec: 0.700 | Rec: 0.816 | F1: 0.754\n",
      "Epoch 17 | Loss: 0.0142 | Acc: 0.998 | Prec: 0.709 | Rec: 0.821 | F1: 0.761\n",
      "Epoch 18 | Loss: 0.0140 | Acc: 0.997 | Prec: 0.646 | Rec: 0.845 | F1: 0.732\n",
      "Epoch 19 | Loss: 0.0139 | Acc: 0.998 | Prec: 0.730 | Rec: 0.784 | F1: 0.756\n",
      "Epoch 20 | Loss: 0.0140 | Acc: 0.997 | Prec: 0.662 | Rec: 0.836 | F1: 0.739\n",
      "Epoch 21 | Loss: 0.0140 | Acc: 0.997 | Prec: 0.641 | Rec: 0.862 | F1: 0.735\n",
      "Epoch 22 | Loss: 0.0138 | Acc: 0.997 | Prec: 0.586 | Rec: 0.864 | F1: 0.699\n",
      "Epoch 23 | Loss: 0.0137 | Acc: 0.997 | Prec: 0.662 | Rec: 0.840 | F1: 0.740\n",
      "Epoch 24 | Loss: 0.0138 | Acc: 0.997 | Prec: 0.642 | Rec: 0.839 | F1: 0.728\n",
      "Epoch 25 | Loss: 0.0136 | Acc: 0.997 | Prec: 0.641 | Rec: 0.853 | F1: 0.732\n",
      "Epoch 26 | Loss: 0.0135 | Acc: 0.998 | Prec: 0.696 | Rec: 0.825 | F1: 0.755\n",
      "Epoch 27 | Loss: 0.0137 | Acc: 0.997 | Prec: 0.692 | Rec: 0.823 | F1: 0.751\n",
      "Epoch 28 | Loss: 0.0135 | Acc: 0.997 | Prec: 0.608 | Rec: 0.884 | F1: 0.720\n",
      "Epoch 29 | Loss: 0.0136 | Acc: 0.997 | Prec: 0.641 | Rec: 0.858 | F1: 0.733\n",
      "Epoch 30 | Loss: 0.0134 | Acc: 0.997 | Prec: 0.658 | Rec: 0.828 | F1: 0.733\n",
      "Epoch 31 | Loss: 0.0134 | Acc: 0.997 | Prec: 0.637 | Rec: 0.854 | F1: 0.729\n",
      "Epoch 32 | Loss: 0.0134 | Acc: 0.997 | Prec: 0.637 | Rec: 0.853 | F1: 0.730\n",
      "Epoch 33 | Loss: 0.0133 | Acc: 0.997 | Prec: 0.605 | Rec: 0.873 | F1: 0.715\n",
      "Epoch 34 | Loss: 0.0133 | Acc: 0.997 | Prec: 0.651 | Rec: 0.855 | F1: 0.739\n",
      "Epoch 35 | Loss: 0.0135 | Acc: 0.997 | Prec: 0.644 | Rec: 0.863 | F1: 0.738\n",
      "Epoch 36 | Loss: 0.0133 | Acc: 0.997 | Prec: 0.626 | Rec: 0.867 | F1: 0.727\n",
      "Epoch 37 | Loss: 0.0132 | Acc: 0.997 | Prec: 0.664 | Rec: 0.843 | F1: 0.743\n",
      "Epoch 38 | Loss: 0.0133 | Acc: 0.997 | Prec: 0.660 | Rec: 0.853 | F1: 0.744\n",
      "Epoch 39 | Loss: 0.0133 | Acc: 0.997 | Prec: 0.636 | Rec: 0.853 | F1: 0.728\n",
      "Epoch 40 | Loss: 0.0131 | Acc: 0.997 | Prec: 0.662 | Rec: 0.834 | F1: 0.738\n",
      "Epoch 41 | Loss: 0.0131 | Acc: 0.997 | Prec: 0.648 | Rec: 0.843 | F1: 0.733\n",
      "Epoch 42 | Loss: 0.0130 | Acc: 0.997 | Prec: 0.614 | Rec: 0.872 | F1: 0.720\n",
      "Epoch 43 | Loss: 0.0131 | Acc: 0.997 | Prec: 0.639 | Rec: 0.868 | F1: 0.736\n",
      "Epoch 44 | Loss: 0.0130 | Acc: 0.997 | Prec: 0.662 | Rec: 0.851 | F1: 0.745\n",
      "Epoch 45 | Loss: 0.0132 | Acc: 0.997 | Prec: 0.661 | Rec: 0.842 | F1: 0.741\n",
      "Epoch 46 | Loss: 0.0131 | Acc: 0.997 | Prec: 0.657 | Rec: 0.856 | F1: 0.743\n",
      "Epoch 47 | Loss: 0.0130 | Acc: 0.997 | Prec: 0.590 | Rec: 0.887 | F1: 0.709\n",
      "Epoch 48 | Loss: 0.0130 | Acc: 0.997 | Prec: 0.657 | Rec: 0.846 | F1: 0.740\n",
      "Epoch 49 | Loss: 0.0131 | Acc: 0.997 | Prec: 0.642 | Rec: 0.854 | F1: 0.733\n",
      "Epoch 50 | Loss: 0.0131 | Acc: 0.997 | Prec: 0.653 | Rec: 0.847 | F1: 0.737\n",
      "\n",
      "Bester Threshold: 0.680 mit F1-Score: 0.792\n",
      "Evaluation mit bestem Threshold:\n",
      "Acc: 0.998 | Prec: 0.829 | Rec: 0.727 | F1: 0.775\n"
     ]
    }
   ],
   "source": [
    "#### FUNKTIONIERT!\n",
    "\n",
    "class GCNClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = DirGNNConv(GCNConv(in_channels, hidden_channels), alpha = 0)\n",
    "        self.conv2 = DirGNNConv(GCNConv(hidden_channels, hidden_channels), alpha = 0)\n",
    "        self.lin = nn.Linear(hidden_channels, 1)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        #edge_index =  torch.cat([edge_index, edge_index.flip(0)], dim=1)\n",
    "        #edge_index =  edge_index.flip(0)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin(x)\n",
    "        return x.view(-1)\n",
    "\n",
    "class NodeMLPClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_channels, hidden_channels)\n",
    "        self.fc2 = nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.fc3 = nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.fc4 = nn.Linear(hidden_channels, hidden_channels)\n",
    "        self.out = nn.Linear(hidden_channels, 1)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        x: Tensor mit Shape [num_nodes, in_channels]\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        #x = F.relu(self.fc3(x))\n",
    "        #x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        #x = F.relu(self.fc4(x))\n",
    "        x = self.out(x)\n",
    "        return x.view(-1)\n",
    "\n",
    "# === 3. Modell, Optimizer, Loss ===\n",
    "model = NodeMLPClassifier(in_channels=11, hidden_channels=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Klassengewichte berechnen (gegen Ungleichgewicht)\n",
    "all_labels = torch.cat([g.y for g in train_data])\n",
    "ratio = (len(all_labels) - all_labels.sum()) / all_labels.sum()\n",
    "pos_weight = torch.tensor((ratio**0.3), dtype=torch.float)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "print(f\"Pos Weight: {pos_weight.item():.2f}\")\n",
    "\n",
    "# === 4. Training & Evaluation ===\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = criterion(out, batch.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader, threshold = 0.5):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_nodes = 0\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "\n",
    "    for batch in loader:\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        preds = torch.sigmoid(out) > threshold\n",
    "        total_correct += (preds == batch.y.bool()).sum().item()\n",
    "        total_nodes += batch.y.size(0)\n",
    "\n",
    "        # Metriken für Klasse 1\n",
    "        tp += ((preds == 1) & (batch.y == 1)).sum().item()\n",
    "        fp += ((preds == 1) & (batch.y == 0)).sum().item()\n",
    "        fn += ((preds == 0) & (batch.y == 1)).sum().item()\n",
    "\n",
    "    acc = total_correct / total_nodes\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "@torch.no_grad()\n",
    "def find_best_threshold(loader, thresholds=np.linspace(0, 1, 101)):\n",
    "    model.eval()\n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    # Alle Outputs und Labels sammeln, damit man nicht für jeden Threshold neu durch die Daten geht\n",
    "    all_outs = []\n",
    "    all_labels = []\n",
    "    for batch in loader:\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        all_outs.append(torch.sigmoid(out))\n",
    "        all_labels.append(batch.y)\n",
    "    all_outs = torch.cat(all_outs)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        preds = all_outs > threshold\n",
    "        tp = ((preds == 1) & (all_labels == 1)).sum().item()\n",
    "        fp = ((preds == 1) & (all_labels == 0)).sum().item()\n",
    "        fn = ((preds == 0) & (all_labels == 1)).sum().item()\n",
    "\n",
    "        precision = tp / (tp + fp + 1e-8)\n",
    "        recall = tp / (tp + fn + 1e-8)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "\n",
    "# === 5. Training starten ===\n",
    "for epoch in range(1, 51):\n",
    "    loss = train()\n",
    "    acc, prec, rec, f1 = evaluate(test_loader)\n",
    "    print(f\"Epoch {epoch:02d} | Loss: {loss:.4f} | Acc: {acc:.3f} | Prec: {prec:.3f} | Rec: {rec:.3f} | F1: {f1:.3f}\")\n",
    "\n",
    "# Nach Training besten Threshold bestimmen\n",
    "best_threshold, best_f1 = find_best_threshold(evaluate_loader)\n",
    "print(f\"\\nBester Threshold: {best_threshold:.3f} mit F1-Score: {best_f1:.3f}\")\n",
    "\n",
    "# Evaluation mit bestem Threshold\n",
    "acc, prec, rec, f1 = evaluate(test_loader, threshold=best_threshold)\n",
    "print(f\"Evaluation mit bestem Threshold:\")\n",
    "print(f\"Acc: {acc:.3f} | Prec: {prec:.3f} | Rec: {rec:.3f} | F1: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddf427af-1732-4437-825a-da089421feb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeMLPClassifier(\n",
       "  (fc1): Linear(in_features=11, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc4): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (out): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Basisverzeichnis\n",
    "project_dir = Path(\"/mnt/c/Users/uhewm/Desktop/ProjectHGT\")\n",
    "\n",
    "# Neuer Unterordner für Modelle\n",
    "model_dir = project_dir / \"trained_models\"\n",
    "model_dir.mkdir(exist_ok=True)   # legt den Ordner an, falls er noch nicht existiert\n",
    "\n",
    "# Speicherpfad für das Modell\n",
    "model_path = model_dir / \"node_mlp_classifier_full.pt\"\n",
    "\n",
    "# Speichern\n",
    "torch.save(model, model_path)\n",
    "\n",
    "# Laden\n",
    "#model = torch.load(model_path)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "4758b855-265b-4b64-801f-f275f7528c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos Weight: 3.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4297/3150998850.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pos_weight = torch.tensor((ratio**0.3), dtype=torch.float)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3184x13 and 9x32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[299], line 153\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;66;03m# === 5. Training starten ===\u001b[39;00m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m51\u001b[39m):\n\u001b[0;32m--> 153\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m     acc, prec, rec, f1 \u001b[38;5;241m=\u001b[39m evaluate(test_loader, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Prec: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprec\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Rec: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrec\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[299], line 61\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     60\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 61\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(out, batch\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     63\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[299], line 19\u001b[0m, in \u001b[0;36mGCNClassifier.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, edge_index):\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m#edge_index = edge_index[[1, 0], :]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m#edge_index, _ = add_self_loops(edge_index)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch_geometric/nn/conv/dir_gnn_conv.py:63\u001b[0m, in \u001b[0;36mDirGNNConv.forward\u001b[0;34m(self, x, edge_index)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, edge_index: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m     x_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     x_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_out(x, edge_index\u001b[38;5;241m.\u001b[39mflip([\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     66\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m x_out \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha) \u001b[38;5;241m*\u001b[39m x_in\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch_geometric/nn/conv/gcn_conv.py:260\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m             edge_index \u001b[38;5;241m=\u001b[39m cache\n\u001b[0;32m--> 260\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[1;32m    263\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, edge_weight\u001b[38;5;241m=\u001b[39medge_weight)\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/torch_geometric/nn/dense/linear.py:147\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3184x13 and 9x32)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from torch_geometric.utils import add_self_loops\n",
    "\n",
    "# === 2. Modell definieren ===\n",
    "class GCNClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.conv1 = DirGNNConv(GCNConv(in_channels, hidden_channels), alpha = 0)\n",
    "        self.conv2 = DirGNNConv(GCNConv(hidden_channels, hidden_channels), alpha = 0)\n",
    "        self.conv3 = DirGNNConv(GCNConv(hidden_channels, hidden_channels), alpha = 0)\n",
    "        self.conv4 = DirGNNConv(GCNConv(hidden_channels, hidden_channels), alpha = 0)\n",
    "        self.conv5 = DirGNNConv(GCNConv(hidden_channels, hidden_channels), alpha = 0)\n",
    "        self.lin = nn.Linear(hidden_channels, 1)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        #edge_index = edge_index[[1, 0], :]\n",
    "        #edge_index, _ = add_self_loops(edge_index)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "\n",
    "        x = self.conv5(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.lin(x)\n",
    "        return x.view(-1)\n",
    "\n",
    "\n",
    "# === 3. Modell, Optimizer, Loss ===\n",
    "model = GCNClassifier(in_channels=9, hidden_channels=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Klassengewichte berechnen (gegen Ungleichgewicht)\n",
    "all_labels = torch.cat([g.y for g in train_data])\n",
    "ratio = (len(all_labels) - all_labels.sum()) / all_labels.sum()\n",
    "pos_weight = torch.tensor((ratio**0.3), dtype=torch.float)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "print(f\"Pos Weight: {pos_weight.item():.2f}\")\n",
    "\n",
    "# === 4. Training & Evaluation ===\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = criterion(out, batch.y.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_nodes = 0\n",
    "    tp, fp, fn = 0, 0, 0\n",
    "\n",
    "    for batch in loader:\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        preds = torch.sigmoid(out) > threshold\n",
    "        total_correct += (preds == batch.y.bool()).sum().item()\n",
    "        total_nodes += batch.y.size(0)\n",
    "\n",
    "        tp += ((preds == 1) & (batch.y == 1)).sum().item()\n",
    "        fp += ((preds == 1) & (batch.y == 0)).sum().item()\n",
    "        fn += ((preds == 0) & (batch.y == 1)).sum().item()\n",
    "\n",
    "    acc = total_correct / total_nodes\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "    return acc, precision, recall, f1\n",
    "\n",
    "@torch.no_grad()\n",
    "def find_best_threshold(loader, thresholds=np.linspace(0, 1, 101)):\n",
    "    model.eval()\n",
    "    best_threshold = 0.5\n",
    "    best_f1 = 0.0\n",
    "\n",
    "    # Alle Outputs und Labels sammeln, damit man nicht für jeden Threshold neu durch die Daten geht\n",
    "    all_outs = []\n",
    "    all_labels = []\n",
    "    for batch in loader:\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        all_outs.append(torch.sigmoid(out))\n",
    "        all_labels.append(batch.y)\n",
    "    all_outs = torch.cat(all_outs)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        preds = all_outs > threshold\n",
    "        tp = ((preds == 1) & (all_labels == 1)).sum().item()\n",
    "        fp = ((preds == 1) & (all_labels == 0)).sum().item()\n",
    "        fn = ((preds == 0) & (all_labels == 1)).sum().item()\n",
    "\n",
    "        precision = tp / (tp + fp + 1e-8)\n",
    "        recall = tp / (tp + fn + 1e-8)\n",
    "        f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "\n",
    "    return best_threshold, best_f1\n",
    "\n",
    "@torch.no_grad()\n",
    "def show_some_predictions(loader, n_samples=3, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in loader:\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        probs = torch.sigmoid(out)\n",
    "        preds = (probs > threshold).long()\n",
    "        all_probs.append(probs)\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(batch.y)\n",
    "\n",
    "    all_probs = torch.cat(all_probs)\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "\n",
    "    total_samples = len(all_labels)\n",
    "    indices = random.sample(range(total_samples), k=min(n_samples, total_samples))\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        print(f\"Sample {i + 1}:\")\n",
    "        print(f\"  True label:      {all_labels[idx].item()}\")\n",
    "        print(f\"  Predicted prob:  {all_probs[idx].item():.4f}\")\n",
    "        print(f\"  Predicted label: {all_preds[idx].item()}\")\n",
    "\n",
    "# === 5. Training starten ===\n",
    "for epoch in range(1, 51):\n",
    "    loss = train()\n",
    "    acc, prec, rec, f1 = evaluate(test_loader, threshold=0.5)\n",
    "    print(f\"Epoch {epoch:02d} | Loss: {loss:.4f} | Acc: {acc:.3f} | Prec: {prec:.3f} | Rec: {rec:.3f} | F1: {f1:.3f}\")\n",
    "\n",
    "# Nach Training besten Threshold bestimmen\n",
    "best_threshold, best_f1 = find_best_threshold(test_loader)\n",
    "print(f\"\\nBester Threshold: {best_threshold:.3f} mit F1-Score: {best_f1:.3f}\")\n",
    "\n",
    "# Evaluation mit bestem Threshold\n",
    "acc, prec, rec, f1 = evaluate(test_loader, threshold=best_threshold)\n",
    "print(f\"Evaluation mit bestem Threshold:\")\n",
    "print(f\"Acc: {acc:.3f} | Prec: {prec:.3f} | Rec: {rec:.3f} | F1: {f1:.3f}\")\n",
    "\n",
    "print(\"\\nEin paar Beispielvorhersagen auf Trainingsdaten mit bestem Threshold:\")\n",
    "show_some_predictions(train_loader, n_samples=3, threshold=best_threshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c5554bc2-ba45-409b-8bd2-a7e0d483165d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ein paar Beispielvorhersagen auf Trainingsdaten mit bestem Threshold:\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEin paar Beispielvorhersagen auf Trainingsdaten mit bestem Threshold:\")\n",
    "#show_some_predictions(train_loader, n_samples=25, threshold=best_threshold)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "64655a3b-d5c1-44d6-8a1a-4522da9f9f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0, 13: 0, 14: 0, 15: 0, 16: 0, 17: 0, 18: 0, 19: 0, 20: 0, 21: 0, 22: 0, 23: 0, 24: 0, 25: 0, 26: 0, 27: 0, 28: 0, 29: 0, 30: 0, 31: 0, 32: 0, 33: 0, 34: 0, 35: 0, 36: 0, 37: 0, 38: 0, 39: 0, 40: 0, 41: 0, 42: 0, 43: 0, 44: 0, 45: 0, 46: 0, 47: 0, 48: 0, 49: 0, 50: 0, 51: 0, 52: 0, 53: 0, 54: 0, 55: 0, 56: 0, 57: 0, 58: 0, 59: 0, 60: 0, 61: 0, 62: 0, 63: 0, 64: 0, 65: 0, 66: 0, 67: 0, 68: 0, 69: 0, 70: 0, 71: 0, 72: 0, 73: 0, 74: 0, 75: 0, 76: 0, 77: 0, 78: 0, 79: 0, 80: 0, 81: 0, 82: 0, 83: 0, 84: 0, 85: 0, 86: 0, 87: 0, 88: 0, 89: 0, 90: 0, 91: 0, 92: 0, 93: 0, 94: 0, 95: 0, 96: 0, 97: 0, 98: 0, 99: 0, 100: 1, 101: 1, 102: 1, 103: 2, 104: 1, 105: 1, 106: 1, 107: 1, 108: 1, 109: 1, 110: 3, 111: 1, 112: 1, 113: 1, 114: 1, 115: 1, 116: 2, 117: 1, 118: 1, 119: 1, 120: 2, 121: 1, 122: 2, 123: 3, 124: 2, 125: 2, 126: 2, 127: 2, 128: 3, 129: 1, 130: 1, 131: 1, 134: 1, 133: 1, 132: 1, 137: 2, 136: 2, 135: 1, 140: 1, 139: 1, 138: 1, 143: 2, 142: 1, 141: 1, 149: 4, 148: 3, 147: 3, 146: 3, 145: 2, 144: 2, 155: 1, 154: 3, 153: 2, 152: 3, 151: 1, 150: 1, 164: 4, 163: 5, 162: 2, 161: 4, 160: 2, 159: 2, 158: 5, 157: 4, 156: 1, 176: 2, 175: 6, 174: 7, 173: 1, 172: 6, 171: 4, 170: 2, 169: 5, 168: 2, 167: 1, 166: 3, 165: 5, 198: 11, 197: 10, 196: 9, 195: 8, 194: 7, 193: 7, 192: 6, 191: 6, 190: 10, 189: 3, 188: 5, 187: 8, 186: 3, 185: 9, 184: 4, 183: 1, 182: 2, 181: 4, 180: 5, 179: 8, 178: 7, 177: 4}\n",
      "Predicted Nodes:  [160, 166, 173, 184, 186, 195, 198]\n",
      "True Nodes:  [160, 162, 166, 173, 184, 186, 187, 191, 194, 195, 197]\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "importlib.reload(ImportData)\n",
    "\n",
    "for file in random.sample(all_files, 1):\n",
    "    single_data = ImportData.load_file(file)\n",
    "    \n",
    "# === 1. Modellvorhersagen berechnen ===\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(single_data.x, single_data.edge_index)  # Shape: [num_nodes]\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()  # Werte zwischen 0 und 1\n",
    "\n",
    "# Map von Node-ID zu Wahrscheinlichkeit\n",
    "pred_probs = {i: p > best_threshold for i, p in enumerate(probs)}\n",
    "pred_nodes = sorted([i for i, flag in pred_probs.items() if flag])\n",
    "\n",
    "node_to_id = {node: i for i, node in enumerate(single_data.H.nodes)}\n",
    "print(\"Predicted Nodes: \", sorted([node_to_id[i] for i, flag in pred_probs.items() if flag]))\n",
    "print(\"True Nodes: \", sorted(set(single_data.parental_nodes_hgt_events_corrected)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04488c51-7bb7-4c04-b722-d31584fe58d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/uhewm/OneDrive/PhD/Project No.2/pangenome-gene-transfer-simulation/ImportData.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edges = torch.tensor(graph_properties[1], dtype=torch.long)  # [2, num_edges]\n",
      "/mnt/c/Users/uhewm/OneDrive/PhD/Project No.2/pangenome-gene-transfer-simulation/ImportData.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coords = torch.tensor(graph_properties[2].T)           # [2, num_nodes]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Nodes:  [170, 180, 188, 197]\n",
      "True Nodes:  [170, 180, 188, 191]\n",
      "/mnt/c/Users/uhewm/OneDrive/PhD/Project No.2/pangenome/graph.html\n",
      "\u001b[?1l\u001b>4;1H\u001b[2J\u001b[?47l\u001b84l\u001b[?1h\u001b=\u001b[m\u001b[m\u001b[37m\u001b[40m\u001b[1;1H                                                                                \u001b[2;1H                                                                                \u001b[3;1H                                                                                \u001b[4;1H                                                                                \u001b[5;1H                                                                                \u001b[6;1H                                                                                \u001b[7;1H                                                                                \u001b[8;1H                                                                                \u001b[9;1H                                                                                \u001b[10;1H                                                                                \u001b[11;1H                                                                                \u001b[12;1H                                                                                \u001b[13;1H                                                                                \u001b[14;1H                                                                                \u001b[15;1H                                                                                \u001b[16;1H                                                                                \u001b[17;1H                                                                                \u001b[18;1H                                                                                \u001b[19;1H                                                                                \u001b[20;1H                                                                                \u001b[21;1H                                                                                \u001b[22;1H                                                                                \u001b[23;1H                                                                                \u001b[24;1H                                                                              \u001b[4h\u001b[37m\u001b[40m \u001b[4l\u001b[H\u001b[m\u001b[m\u001b[37m\u001b[40m\u001b[m\u001b[m\u001b[21B\u001b[33m\u001b[44m\u001b[1mGetting file://localhost/mnt/c/Users/uhewm/OneDrive/PhD/Project%20No.2/pangenom \u001b[22;80H\u001b[m\u001b[m"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['cmd.exe', '/C', 'start', 'chrome', 'C:\\\\Users\\\\uhewm\\\\OneDrive\\\\PhD\\\\Project No.2\\\\pangenome\\\\graph.html'], returncode=0)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from pyvis.network import Network\n",
    "import subprocess\n",
    "\n",
    "importlib.reload(ImportData)\n",
    "\n",
    "for file in random.sample(all_files, 1):\n",
    "    single_data = ImportData.load_file(file)\n",
    "    \n",
    "# === 1. Modellvorhersagen berechnen ===\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(single_data.x, single_data.edge_index)  # Shape: [num_nodes]\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()  # Werte zwischen 0 und 1\n",
    "\n",
    "# Map von Node-ID zu Wahrscheinlichkeit\n",
    "pred_probs = {i: p > best_threshold for i, p in enumerate(probs)}\n",
    "pred_nodes = sorted([i for i, flag in pred_probs.items() if flag])\n",
    "\n",
    "node_to_id = {node: i for i, node in enumerate(single_data.H.nodes)}\n",
    "print(\"Predicted Nodes: \", sorted([node_to_id[i] for i, flag in pred_probs.items() if flag]))\n",
    "print(\"True Nodes: \", sorted(set(single_data.parental_nodes_hgt_events_corrected)))\n",
    "\n",
    "pred_probs = {node_to_id[i]: p for i, p in enumerate(probs)}\n",
    "\n",
    "\n",
    "# --- x/y Koordinaten für Blätter und innere Knoten berechnen ---\n",
    "x_spacing = 100\n",
    "y_spacing = 100\n",
    "\n",
    "node_x = {}\n",
    "node_y = {}\n",
    "\n",
    "# Maximaler Level aus single_data.H\n",
    "max_level = max(single_data.H.nodes[n].get(\"level\", 0) for n in single_data.H.nodes)\n",
    "\n",
    "# Hilfsfunktion: finde alle Blätter unterhalb eines Knotens\n",
    "def get_descendant_leaves(G, node):\n",
    "    \"\"\"Alle Blätter, die von `node` erreichbar sind (rekursiv).\"\"\"\n",
    "    stack = list(G.predecessors(node))\n",
    "    reachable_leaves = []\n",
    "    while stack:\n",
    "        temp_node = stack.pop()\n",
    "        children = list(G.predecessors(temp_node))\n",
    "        if len(children) > 0:\n",
    "            stack.extend(children)\n",
    "        else:\n",
    "            reachable_leaves.append(temp_node)\n",
    "    return reachable_leaves\n",
    "\n",
    "# === Blätter (Level 0) oben ===\n",
    "leaves = [n for n in single_data.H.nodes if single_data.H.nodes[n].get(\"level\", 0) == 0]\n",
    "for i, node in enumerate(sorted(leaves)):  \n",
    "    node_x[node] = i * x_spacing\n",
    "    node_y[node] = (max_level - 0) * y_spacing  # Blätter oben\n",
    "\n",
    "# === Innere Knoten: levelweise platzieren ===\n",
    "levels_in_graph = sorted(set(nx.get_node_attributes(single_data.H, \"level\").values()))\n",
    "for level in levels_in_graph[1:]:  # 0 schon behandelt\n",
    "    nodes_in_level = [n for n in single_data.H.nodes if single_data.H.nodes[n].get(\"level\", 0) == level]\n",
    "    for node in nodes_in_level:\n",
    "        # Finde alle Blätter unterhalb\n",
    "        reachable_leaves = get_descendant_leaves(single_data.H, node)\n",
    "        if reachable_leaves:\n",
    "            leaf_x = [node_x[l] for l in reachable_leaves if l in node_x]\n",
    "            node_x[node] = np.mean(leaf_x)\n",
    "        else:\n",
    "            node_x[node] = 0\n",
    "        node_y[node] = (max_level - level) * y_spacing\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# === Blätter (Level 0) oben ===\n",
    "leaves = [n for n in single_data.H.nodes if single_data.H.nodes[n].get(\"level\", 0) == 0]\n",
    "for i, node in enumerate(sorted(leaves)):  \n",
    "    node_x[node] = i * x_spacing\n",
    "    node_y[node] = (max_level - 0) * y_spacing  # Blätter oben\n",
    "\n",
    "# === Innere Knoten: levelweise platzieren ===\n",
    "levels_in_graph = sorted(set(nx.get_node_attributes(single_data.H, \"level\").values()))\n",
    "for level in levels_in_graph[1:]:  # 0 schon behandelt\n",
    "    nodes_in_level = [n for n in single_data.H.nodes if single_data.H.nodes[n].get(\"level\", 0) == level]\n",
    "    for node in nodes_in_level:\n",
    "        children = list(single_data.H.predecessors(node))\n",
    "        if children:\n",
    "            child_x = [node_x[c] for c in children if c in node_x]\n",
    "            if child_x:  # falls Kinder schon positioniert\n",
    "                node_x[node] = np.mean(child_x)\n",
    "            else:\n",
    "                node_x[node] = 0\n",
    "        else:\n",
    "            node_x[node] = 0\n",
    "        node_y[node] = (max_level - level) * y_spacing\n",
    "\"\"\"\n",
    "        \n",
    "# === Netzwerk initialisieren (Hierarchical Layout deaktiviert!) ===\n",
    "net = Network(height=\"900px\", width=\"100%\", directed=True)\n",
    "\n",
    "net.set_options(\"\"\"\n",
    "{\n",
    "  \"nodes\": {\n",
    "    \"shape\": \"dot\",\n",
    "    \"size\": 12,\n",
    "    \"font\": { \"size\": 30 }\n",
    "  },\n",
    "  \"edges\": {\n",
    "    \"arrows\": {\n",
    "      \"to\": { \"enabled\": true, \"scaleFactor\": 0.5 }\n",
    "    }\n",
    "  },\n",
    "  \"physics\": {\n",
    "    \"enabled\": false\n",
    "  }\n",
    "}\n",
    "\"\"\")\n",
    "# === Knoten hinzufügen mit festen x/y ===\n",
    "for node in single_data.H.nodes():\n",
    "    core = single_data.H.nodes[node].get('core_distance', 0) #\n",
    "    allele = single_data.H.nodes[node].get('allele_distance', 0)# * 10000\n",
    "    allele_distance_convolution = single_data.H.nodes[node].get('allele_distance_convolution', 0)\n",
    "    core_distance_convolution = single_data.H.nodes[node].get('core_distance_convolution', 0)\n",
    "    leaf_count = single_data.H.nodes[node].get(\"leaf_count\", 0)\n",
    "    leaf_count_presence_matters = single_data.H.nodes[node].get(\"leaf_count_presence_matters\", 0)\n",
    "    node_count = single_data.H.nodes[node].get(\"node_count\", 0)\n",
    "    node_count_presence_matters = single_data.H.nodes[node].get(\"node_count_presence_matters\", 0)\n",
    "    allele_distance_only_new = single_data.H.nodes[node].get('allele_distance_only_new', 0)\n",
    "    allele_distances_both_children_polymorph = single_data.H.nodes[node].get('allele_distances_both_children_polymorph', 0)\n",
    "    true_allele_distance = single_data.H.nodes[node].get('true_allele_distance', 0)\n",
    "    true_allele_convolution = single_data.H.nodes[node].get('true_allele_convolution', 0)\n",
    "    node_time = single_data.H.nodes[node].get('node_time', 0)\n",
    "    pred = pred_probs[node]\n",
    "    #title = f\"Core: {core:.2f}, Allele: {allele:.2f}, leaf_count: {leaf_count}, leaf_count_presence_matters: {leaf_count_presence_matters}, node_count: {node_count}, node_count_presence_matters: {node_count_presence_matters}, HGT_prob: {pred:.2f}\"\n",
    "    #label = f\"{node}\\n({core:.2f}, {allele:.2f}, {leaf_count}, {leaf_count_presence_matters}, {node_count}, {node_count_presence_matters}, {pred:.2f})\"\n",
    "    title = f\"Core: {core:.2f}, Allele: {allele:.2f}, Core_convolution: {core_distance_convolution:.2f}, Allele_only_new: {allele_distance_only_new:.2f}, Both_polymorph: {allele_distances_both_children_polymorph:.2f}, True_allele_distance: {true_allele_distance:.2f}, True_allele_convolution: {true_allele_convolution:.2f}, Allele_convolution: {allele_distance_convolution:.2f}, Time: {node_time:.2f}, Pred: {pred:.2f}\"\n",
    "    label = f\"{node}\\n({core:.2f}, {allele:.2f}, {core_distance_convolution:.2f}, {allele_distance_only_new:.2f}, {allele_distances_both_children_polymorph:.2f}, {true_allele_distance:.2f}, {true_allele_convolution:.2f}, {allele_distance_convolution:.2f}, {node_time:.2f}, {pred:.2f})\"\n",
    "\n",
    "    # Farbe\n",
    "    if node in single_data.parental_nodes_hgt_events_corrected and pred > best_threshold:\n",
    "        color = \"green\"\n",
    "    elif node not in single_data.parental_nodes_hgt_events_corrected and pred > best_threshold:\n",
    "        color = \"violet\"\n",
    "    elif node in single_data.parental_nodes_hgt_events_corrected and pred <= best_threshold:\n",
    "        color = \"red\"\n",
    "    elif node < 100 and single_data.gene_absence_presence_matrix[node] == 1:\n",
    "        color = \"orange\"\n",
    "    elif node < 100 and single_data.gene_absence_presence_matrix[node] == 0:\n",
    "        color = \"black\"\n",
    "    elif node not in single_data.parental_nodes_hgt_events_corrected and pred <= best_threshold:\n",
    "        color = \"lightblue\"\n",
    "\n",
    "    net.add_node(node, label=label, title=title, color=color,\n",
    "                 x=node_x[node], y=node_y[node])\n",
    "\n",
    "# === Kanten hinzufügen ===\n",
    "for u, v in single_data.H.edges():\n",
    "    net.add_edge(u, v)\n",
    "\n",
    "# === HTML-Datei speichern und direkt in Chrome öffnen ===\n",
    "html_file = Path(\"/mnt/c/Users/uhewm/OneDrive/PhD/Project No.2/pangenome/graph.html\")\n",
    "html_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "net.show(str(html_file), notebook=False)\n",
    "\n",
    "# WSL-Pfad in Windows-Pfad umwandeln\n",
    "win_path = subprocess.run([\"wslpath\", \"-w\", str(html_file)], capture_output=True, text=True).stdout.strip()\n",
    "\n",
    "# Direkt in Chrome öffnen\n",
    "subprocess.run([\"cmd.exe\", \"/C\", \"start\", \"chrome\", win_path])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "ff645753-56b0-4b00-beb1-ecf5830d864c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 [1, 0]\n"
     ]
    }
   ],
   "source": [
    "def get_descendant_leaves(G, node):\n",
    "    \"\"\"Alle Blätter, die von `node` erreichbar sind (rekursiv).\"\"\"\n",
    "    stack = list(G.predecessors(node))\n",
    "    reachable_leaves = []\n",
    "    while stack:\n",
    "        temp_node = stack.pop()\n",
    "        children = list(G.predecessors(temp_node))\n",
    "        if len(children) > 0:\n",
    "            stack.extend(children)\n",
    "        else:\n",
    "            reachable_leaves.append(temp_node)\n",
    "    return reachable_leaves\n",
    "\n",
    "node = 117\n",
    "reachable_leaves = get_descendant_leaves(single_data.H, node)\n",
    "print(node, reachable_leaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "3ac3f9b6-1c37-4691-b109-9b6064bb256d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7366938591003418"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with h5py.File(file, \"r\") as f:\n",
    "    # in jeder Datei steckt eine Gruppe namens 'results'\n",
    "    grp = f[\"results\"]\n",
    "    \n",
    "    hgt_rate = grp.attrs[\"hgt_rate\"]\n",
    "    rho = grp.attrs[\"rho\"]\n",
    "    fitch_score = grp.attrs[\"fitch_score\"]\n",
    "    gene_number_hgt_events_passed = grp.attrs[\"gene_number_hgt_events_passed\"]\n",
    "\n",
    "    gene_absence_presence_matrix = grp.attrs[\"gene_absence_presence_matrix\"] \n",
    "    gene_number_hgt_events_passed = grp.attrs[\"gene_number_hgt_events_passed\"]\n",
    "    gene_number_loss_events = grp.attrs[\"gene_number_loss_events\"]\n",
    "    parental_nodes_hgt_events_corrected = grp.attrs[\"parental_nodes_hgt_events_corrected\"]\n",
    "    children_gene_nodes_loss_events = grp.attrs[\"children_gene_nodes_loss_events\"]\n",
    "\n",
    "rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ca0d4413-cc11-433a-b9ec-22f3ec504b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[129, 130, 138, 155, 173, 176, 181, 186, 192]\n",
      "[129, 130, 138, 155, 170, 171, 172, 173, 174, 175, 176, 177, 178, 181, 186, 192]\n"
     ]
    }
   ],
   "source": [
    "test = random.choice(test_data)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(test.x, test.edge_index)  # Shape: [num_nodes]\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()  # Werte zwischen 0 und 1\n",
    "\n",
    "# Map von Node-ID zu Wahrscheinlichkeit\n",
    "pred_probs = {i: p > best_threshold for i, p in enumerate(probs)}\n",
    "pred_nodes = sorted([i for i, flag in pred_probs.items() if flag])\n",
    "\n",
    "# 2) Alle Werte != 0 im Tensor\n",
    "true_nodes = (test.y * torch.arange(199, dtype=test.y.dtype)).nonzero(as_tuple=True)[0].tolist()\n",
    "\n",
    "print(true_nodes)\n",
    "print(pred_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "id": "9548a283-4791-4158-be16-221348b78414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  1.71  20.    20.     1.71  20.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  2.35  20.    20.     2.35  20.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [156.86 390.    60.    78.64  60.     0.     1.  ]\n",
      " [ 41.18 140.     0.     0.     0.     0.     0.  ]\n",
      " [ 41.18 140.     0.     0.     0.     0.     0.  ]\n",
      " [ 41.18 140.   110.    14.38 110.     1.     1.  ]\n",
      " [ 26.79  30.     0.     8.22   0.     1.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [ 18.57  30.    30.    18.57  30.     1.     1.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [ 37.04 190.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [ 37.04 190.    20.     7.24  20.     0.     0.  ]\n",
      " [ 29.8  170.    10.     4.13  10.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [ 11.78  70.    70.    11.78  70.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [ 13.89  90.    20.     4.63  20.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  6.92  50.    30.     5.21  30.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]\n",
      " [  0.     0.     0.     0.     0.     0.     0.  ]]\n"
     ]
    }
   ],
   "source": [
    "num_nodes = test.x.shape[0]\n",
    "\n",
    "# Spalte für true_nodes: 1, wenn der Knoten in true_nodes ist, sonst 0\n",
    "true_nodes_col = torch.zeros(num_nodes, dtype=test.x.dtype)\n",
    "true_nodes_col[true_nodes] = 1\n",
    "true_nodes_col = true_nodes_col.unsqueeze(1)  # [num_nodes, 1]\n",
    "\n",
    "# Spalte für pred_nodes: 1, wenn der Knoten in pred_nodes ist, sonst 0\n",
    "pred_nodes_col = torch.zeros(num_nodes, dtype=test.x.dtype)\n",
    "pred_nodes_col[pred_nodes] = 1\n",
    "pred_nodes_col = pred_nodes_col.unsqueeze(1)  # [num_nodes, 1]\n",
    "\n",
    "# Spaltenweise zusammenfügen\n",
    "matrix = torch.cat([test.x[:, 0:5], true_nodes_col, pred_nodes_col], dim=1)\n",
    "matrix = matrix[99:]\n",
    "\n",
    "np.set_printoptions(threshold=np.inf, suppress=True)  # Alles anzeigen, keine Scientific Notation\n",
    "print(np.round(matrix.cpu().numpy(), 2))  # sollte [num_nodes, 7] sein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "16f1ede3-3e87-431b-8f0a-7fc4288736ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gesamt-Statistik:\n",
      "TP = 745\n",
      "FP = 274\n",
      "FN = 140\n"
     ]
    }
   ],
   "source": [
    "total_TP, total_FP, total_FN = 0, 0, 0\n",
    "\n",
    "for test in test_loader:\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(test.x, test.edge_index)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "    pred_nodes = {i for i, p in enumerate(probs) if p > best_threshold}\n",
    "    true_nodes = set((test.y * torch.arange(len(test.y), dtype=test.y.dtype)).nonzero(as_tuple=True)[0].tolist())\n",
    "\n",
    "    TP = len(true_nodes & pred_nodes)\n",
    "    FP = len(pred_nodes - true_nodes)\n",
    "    FN = len(true_nodes - pred_nodes)\n",
    "\n",
    "    total_TP += TP\n",
    "    total_FP += FP\n",
    "    total_FN += FN\n",
    "\n",
    "print(\"Gesamt-Statistik:\")\n",
    "print(f\"TP = {total_TP}\")\n",
    "print(f\"FP = {total_FP}\")\n",
    "print(f\"FN = {total_FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "e66f7b11-dded-472c-a432-b6b5454e015d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaves = [n for n in H.nodes if H.in_degree(n) == 0]\n",
    "\n",
    "# Schritt 2: Rekursiv/Subgraph-basiert Anzahl der Leaves pro Knoten zählen\n",
    "def count_leaves(node, G, gene_presence_matters = False):\n",
    "    # wenn Blatt → 1\n",
    "    if node in leaves:\n",
    "        if not gene_presence_matters or gene_absence_presence_matrix[node] == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    # sonst Summe der Leaves aller Kinder\n",
    "    print(list(count_leaves(child, G) for child in G.predecessors(node)))\n",
    "    return sum(count_leaves(child, G) for child in G.predecessors(node))\n",
    "\n",
    "count_leaves(140, H, gene_presence_matters = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "9655b218-8f80-49ee-a3cf-7f40b6dc2420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "present_nodes = [n for n in G.nodes if n < len(gene_absence_presence_matrix) and gene_absence_presence_matrix[n] == 1]\n",
    "\n",
    "def mrca_of_nodes(G, nodes):\n",
    "    G = G.reverse()\n",
    "    if not nodes:\n",
    "        return None\n",
    "    # Starte mit erstem Knoten\n",
    "    mrca = nodes[0]\n",
    "    for node in nodes[1:]:\n",
    "        mrca = nx.lowest_common_ancestor(G, mrca, node)\n",
    "        if mrca is None:  # kein gemeinsamer Vorfahre\n",
    "            return None\n",
    "    return mrca\n",
    "\n",
    "mrca_node = mrca_of_nodes(G, present_nodes)\n",
    "print(mrca_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65903b79-e97c-488f-99ac-2690a084585f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
