{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bbadd8b-d850-4870-8287-c148ebbe3962",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kippnich/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/numpy/core/getlimits.py:542: UserWarning: Signature b'\\x00\\xd0\\xcc\\xcc\\xcc\\xcc\\xcc\\xcc\\xfb\\xbf\\x00\\x00\\x00\\x00\\x00\\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.\n",
      "This warnings indicates broken support for the dtype!\n",
      "  machar = _get_machar(dtype)\n",
      "/tmp/ipykernel_26369/615299766.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edges = torch.tensor(graph_properties[1], dtype=torch.long)  # [2, num_edges]\n",
      "/tmp/ipykernel_26369/615299766.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coords = torch.tensor(graph_properties[2].T)             # [2, num_nodes]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 Dateien erfolgreich geladen.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import h5py\n",
    "import pickle\n",
    "import torch\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, ReLU, Dropout, BatchNorm1d\n",
    "from typing import List, Tuple, Dict, Any\n",
    "\n",
    "def one_hot_encode(sequences, gene_present, gene_length, alphabet=['A','C','T','G','-']):\n",
    "    \"\"\"\n",
    "    sequences: List of strings (DNA sequences)\n",
    "    gene_present: np.array(bool) oder Torch Tensor, gleiche Länge wie sequences\n",
    "    gene_length: int, fixe Länge für das Hot-Encoding\n",
    "    alphabet: list, Zeichenalphabet\n",
    "    \"\"\"\n",
    "    num_samples = len(sequences)\n",
    "    num_chars = len(alphabet)\n",
    "    char_to_idx = {c:i for i,c in enumerate(alphabet)}\n",
    "    sequences_str = [s.decode('utf-8') for s in sequences]\n",
    "    gene_present = np.array(gene_present, dtype=bool)\n",
    "    \n",
    "    # 1️⃣ Leere Batch-Matrix vorbereiten: (num_samples, gene_length, num_chars)\n",
    "    batch = np.zeros((num_samples, gene_length, num_chars), dtype=np.float32)\n",
    "    \n",
    "    # 2️⃣ Hot-Encode alle Sequenzen\n",
    "    for i, seq in enumerate(sequences_str):\n",
    "        if gene_present[i]:\n",
    "            L = min(len(seq), gene_length)  # abschneiden\n",
    "            for j, c in enumerate(seq[:L]):\n",
    "                if c in char_to_idx:\n",
    "                    batch[i, j, char_to_idx[c]] = 1.0\n",
    "        elif gene_present[i] == 0:\n",
    "            batch[i, :, :] = -1.0\n",
    "            \n",
    "    # 3️⃣ Zufällige, aber konsistente Spaltenpermutation\n",
    "    perm = np.random.permutation(gene_length)\n",
    "    batch = batch[:, perm, :]\n",
    "    \n",
    "    # 4️⃣ Optional: Flatten zu Vektor (num_samples, gene_length*num_chars)\n",
    "    batch_flat = batch.reshape(num_samples, -1)\n",
    "    \n",
    "    return torch.tensor(batch_flat)  # shape: (num_samples, gene_length*num_chars)\n",
    "\n",
    "def load_file(file):\n",
    "\n",
    "    gene_length = 300\n",
    "    #nucleotide_mutation_rate = 0.1\n",
    "    \n",
    "    with h5py.File(file, \"r\") as f:\n",
    "            grp = f[\"results\"]\n",
    "            # Load graph_properties (pickle stored in dataset)\n",
    "            graph_properties = pickle.loads(grp[\"graph_properties\"][()])\n",
    "    \n",
    "            # Unpack graph properties\n",
    "            nodes = torch.tensor(graph_properties[0])                # [num_nodes]\n",
    "            edges = torch.tensor(graph_properties[1], dtype=torch.long)  # [2, num_edges]\n",
    "            coords = torch.tensor(graph_properties[2].T)             # [2, num_nodes]\n",
    "    \n",
    "            # Load datasets instead of attrs\n",
    "            gene_absence_presence_matrix = grp[\"gene_absence_presence_matrix\"][()]\n",
    "            nucleotide_sequences = grp[\"nucleotide_sequences\"][()]\n",
    "            #children_gene_nodes_loss_events = grp[\"children_gene_nodes_loss_events\"][()]\n",
    "        \n",
    "            # Load HGT events (simplified)\n",
    "            hgt_events = {}\n",
    "            hgt_grp_simpl = grp.get(\"nodes_hgt_events_simplified\", None)\n",
    "            if hgt_grp_simpl is not None:\n",
    "                for site_id in hgt_grp_simpl.keys():\n",
    "                    hgt_events[int(site_id)] = hgt_grp_simpl[site_id][()]\n",
    "            else:\n",
    "                hgt_events = {}\n",
    "\n",
    "            hot_encoded_nucleotide_sequences = one_hot_encode(nucleotide_sequences, gene_absence_presence_matrix, gene_length)\n",
    "\n",
    "            # Fill the remaining nodes with zeros.\n",
    "            pad_rows = len(nodes) - len(nucleotide_sequences)\n",
    "            pad = torch.zeros((pad_rows, hot_encoded_nucleotide_sequences.shape[1]), dtype=hot_encoded_nucleotide_sequences.dtype)\n",
    "            hot_encoded_nucleotide_sequences = torch.cat([hot_encoded_nucleotide_sequences, pad], dim=0)\n",
    "        \n",
    "            G = nx.DiGraph()\n",
    "            \n",
    "            # Füge Knoten hinzu (optional mit Koordinaten als Attribut)\n",
    "            node_id_list = nodes.tolist()\n",
    "            for i, node_id in enumerate(node_id_list):\n",
    "                # coords[:, i] exists; using index 5 as in your code for node_time\n",
    "                G.add_node(node_id, node_time = coords[:, i].tolist()[5])\n",
    "            \n",
    "            # Füge Kanten hinzu\n",
    "            edge_list = edges.tolist()\n",
    "            for src, dst in zip(edge_list[0], edge_list[1]):\n",
    "                G.add_edge(src, dst)\n",
    "            \n",
    "            # Collect all recipient_parent_nodes from all sites (if present)\n",
    "            recipient_parent_nodes = set()\n",
    "            if hgt_grp_simpl is not None:\n",
    "                for site_id in hgt_grp_simpl.keys():\n",
    "                    arr = hgt_grp_simpl[site_id][()]  # load dataset as numpy structured array\n",
    "                    # try robust field name for recipient child\n",
    "                    if 'recipient_child_node' in arr.dtype.names:\n",
    "                        recipient_parent_nodes.update(arr[\"recipient_child_node\"].tolist())\n",
    "                    elif 'recipient_child' in arr.dtype.names:\n",
    "                        recipient_parent_nodes.update(arr[\"recipient_child\"].tolist())\n",
    "\n",
    "            # Build theta_gains: 1 if node is in recipient_parent_nodes, else 0\n",
    "            theta_gains = torch.tensor(\n",
    "                [1 if node in recipient_parent_nodes else 0 for node in range(len(G.nodes))],\n",
    "                dtype=torch.long\n",
    "            )\n",
    "\n",
    "            level = {n: 0 for n in G.nodes}  # Leaves haben Level 0\n",
    "            \n",
    "            # 3. Topologische Sortierung (damit Kinder vor Eltern behandelt werden)\n",
    "            for node in reversed(list(nx.topological_sort(G))):\n",
    "                successors = list(G.successors(node))\n",
    "                if successors:\n",
    "                    level[node] = 1 + max(level[s] for s in successors)\n",
    "            \n",
    "            # 4. Level als Attribut setzen\n",
    "            nx.set_node_attributes(G, level, \"level\")\n",
    "\n",
    "            ### Add candidate egdes, i.e. potential hgt edges:\n",
    "        \n",
    "            # Hole alle Knoten und ihre Zeiten\n",
    "            node_times = {n: G.nodes[n]['node_time'] for n in G.nodes}\n",
    "            sorted_nodes = sorted(node_times.keys(), key=lambda n: node_times[n])\n",
    "\n",
    "            # Sort the level and node_times from 0 to max_node_id and not in G.nodes order:\n",
    "            node_times = torch.tensor([node_times[n] for n in sorted_nodes], dtype=torch.float)\n",
    "            node_levels = torch.tensor([level[n] for n in sorted_nodes], dtype=torch.float)\n",
    "\n",
    "            # Füge Kanten hinzu\n",
    "            existing_edges = set(zip(edges[0].tolist(), edges[1].tolist()))\n",
    "            candidate_edges = []\n",
    "            for i, src in enumerate(sorted_nodes):\n",
    "                t_src = node_times[src]\n",
    "                for dst in sorted_nodes[i+1:]:  # nur spätere Knoten\n",
    "                    t_dst = node_times[dst]\n",
    "                    if t_dst > t_src:\n",
    "                        if (dst, src) not in existing_edges:  # vermeidet doppelte\n",
    "                            #G.add_edge(src, dst)\n",
    "                            candidate_edges.append((dst, src))\n",
    "            if candidate_edges:  \n",
    "                candidate_edges = torch.tensor(candidate_edges, dtype=torch.long).t()\n",
    "            else:\n",
    "                candidate_edges = torch.empty((2,0), dtype=torch.long)\n",
    "                    \n",
    "            data = Data(\n",
    "                #nucleotide_sequences = nucleotide_sequences,\n",
    "                hot_encoded_nucleotide_sequences = hot_encoded_nucleotide_sequences,       # Node Features [num_nodes, 2]\n",
    "                edge_index = edges[[1, 0], :],        # Edge Index [2, num_edges]\n",
    "                candidate_edges = candidate_edges[[1, 0], :],\n",
    "                y = theta_gains,            # Labels [num_nodes]\n",
    "                file = file,\n",
    "                G = G,\n",
    "                #recipient_parent_nodes = recipient_parent_nodes,\n",
    "                gene_absence_presence_matrix = gene_absence_presence_matrix,\n",
    "                node_times = node_times,\n",
    "                node_levels = node_levels,\n",
    "                hgt_events = hgt_events,\n",
    "            )\n",
    "\n",
    "            # -----------------------------\n",
    "            # Compute label maps directly here (no external call)\n",
    "            # -----------------------------\n",
    "            recip_label_map = {}\n",
    "            donor_map = {}\n",
    "            # initialize maps for internal nodes with exactly 2 children\n",
    "            for n in G.nodes:\n",
    "                children = list(G.successors(n))\n",
    "                if len(children) == 2:\n",
    "                    recip_label_map[n] = (0, None)\n",
    "                    donor_map[n] = None\n",
    "\n",
    "            # parse hgt_events and fill maps\n",
    "            if hgt_events:\n",
    "                for site_id, arr in hgt_events.items():\n",
    "                    # arr is expected to be a numpy structured array\n",
    "                    try:\n",
    "                        ln = len(arr)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                    for i in range(ln):\n",
    "                        # robust field extraction\n",
    "                        rec_parent = None\n",
    "                        rec_child = None\n",
    "                        donor_parent = None\n",
    "                        if 'recipient_parent_node' in arr.dtype.names:\n",
    "                            rec_parent = int(arr[i]['recipient_parent_node'])\n",
    "                        elif 'recipient_parent' in arr.dtype.names:\n",
    "                            rec_parent = int(arr[i]['recipient_parent'])\n",
    "                        if 'recipient_child_node' in arr.dtype.names:\n",
    "                            rec_child = int(arr[i]['recipient_child_node'])\n",
    "                        elif 'recipient_child' in arr.dtype.names:\n",
    "                            rec_child = int(arr[i]['recipient_child'])\n",
    "                        if 'donor_parent_node' in arr.dtype.names:\n",
    "                            donor_parent = int(arr[i]['donor_parent_node'])\n",
    "                        elif 'donor_parent' in arr.dtype.names:\n",
    "                            donor_parent = int(arr[i]['donor_parent'])\n",
    "\n",
    "                        if rec_parent is None:\n",
    "                            continue\n",
    "                        if rec_parent in recip_label_map:\n",
    "                            recip_label_map[rec_parent] = (1, rec_child)\n",
    "                            donor_map[rec_parent] = donor_parent\n",
    "\n",
    "            # attach precomputed label maps to data\n",
    "            data.recip_label_map = recip_label_map\n",
    "            data.donor_map = donor_map\n",
    "\n",
    "    return data\n",
    "\n",
    "folder = \"/mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks\"\n",
    "\n",
    "files = [os.path.join(folder, f) for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "\n",
    "if len(files) > 100:\n",
    "    files = random.sample(files, 100)\n",
    "\n",
    "list_of_Data = []\n",
    "for f in files:\n",
    "    try:\n",
    "        d = load_file(f)\n",
    "        list_of_Data.append(d)\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Laden von {f}: {e}\")\n",
    "\n",
    "# example = load_file(random.choice(files))\n",
    "\n",
    "print(f\"{len(list_of_Data)} Dateien erfolgreich geladen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3eafd64b-d48d-43eb-bff5-8370e96633c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 90 graphs | Validation on 10 graphs | pos_weight=13.400\n",
      "Epoch 1/4 | time 0.2s\n",
      " TRAIN loss 1.2180 (rec 1.2176, don 0.0036) Prec/Rec/F1 0.135/0.200/0.161 | avg_expected_donor_dist 0.981\n",
      " VAL   loss 1.0983 (rec 1.0983, don 0.0000) Prec/Rec/F1 0.000/0.000/0.000 | avg_expected_donor_dist 0.000\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 2/4 | time 0.2s\n",
      " TRAIN loss 0.8702 (rec 0.8699, don 0.0036) Prec/Rec/F1 0.387/0.480/0.429 | avg_expected_donor_dist 0.555\n",
      " VAL   loss 1.1179 (rec 1.1179, don 0.0000) Prec/Rec/F1 0.000/0.000/0.000 | avg_expected_donor_dist 0.000\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 3/4 | time 0.2s\n",
      " TRAIN loss 0.5224 (rec 0.5222, don 0.0019) Prec/Rec/F1 0.611/0.880/0.721 | avg_expected_donor_dist 0.341\n",
      " VAL   loss 1.4188 (rec 1.4188, don 0.0000) Prec/Rec/F1 0.000/0.000/0.000 | avg_expected_donor_dist 0.000\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "Epoch 4/4 | time 0.2s\n",
      " TRAIN loss 0.3948 (rec 0.3946, don 0.0019) Prec/Rec/F1 0.686/0.960/0.800 | avg_expected_donor_dist 0.317\n",
      " VAL   loss 1.5679 (rec 1.5679, don 0.0000) Prec/Rec/F1 0.000/0.000/0.000 | avg_expected_donor_dist 0.000\n",
      "------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import h5py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import networkx as nx\n",
    "\n",
    "# -------------------------\n",
    "# Model components\n",
    "# -------------------------\n",
    "\n",
    "def make_mlp(input_dim, hidden_dims=[256,128], output_dim=None, dropout=0.1):\n",
    "    \"\"\"\n",
    "    Build an MLP that uses LayerNorm instead of BatchNorm.\n",
    "    LayerNorm is stable for batch size 1.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    dims = [input_dim] + hidden_dims\n",
    "    for i in range(len(hidden_dims)):\n",
    "        layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "        # use LayerNorm over feature dimension\n",
    "        layers.append(nn.LayerNorm(dims[i+1]))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "    if output_dim is not None:\n",
    "        layers.append(nn.Linear(dims[-1], output_dim))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class RecipientFinder(nn.Module):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - hgt_logit: raw logit (no sigmoid) shape [B]\n",
    "      - which_logits: raw logits for left/right shape [B,2]\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_vec_dim, aux_dim=3, hidden=[512,256], dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.seq_vec_dim = seq_vec_dim\n",
    "        self.aux_dim = aux_dim\n",
    "        mlp_input = seq_vec_dim * 4 + aux_dim\n",
    "        self.mlp = make_mlp(mlp_input, hidden_dims=hidden, output_dim=None, dropout=dropout)\n",
    "        self.hgt_head = nn.Linear(hidden[-1], 1)    # logit\n",
    "        self.which_head = nn.Linear(hidden[-1], 2)  # logits\n",
    "\n",
    "    def forward(self, left_vec, right_vec, aux_feats):\n",
    "        # left_vec/right_vec: [B, D]\n",
    "        absdiff = torch.abs(left_vec - right_vec)\n",
    "        prod = left_vec * right_vec\n",
    "        x = torch.cat([left_vec, right_vec, absdiff, prod, aux_feats], dim=1)\n",
    "        h = self.mlp(x)\n",
    "        hgt_logit = self.hgt_head(h).squeeze(-1)         # [B]\n",
    "        which_logits = self.which_head(h)                # [B,2]\n",
    "        return hgt_logit, which_logits\n",
    "\n",
    "class DonorFinder(nn.Module):\n",
    "    \"\"\"\n",
    "    Scores candidate donors relative to a recipient vector.\n",
    "    Returns raw scores (logits) of shape [M]\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_vec_dim, aux_dim=3, hidden=[512,256], dropout=0.1):\n",
    "        super().__init__()\n",
    "        mlp_input = seq_vec_dim * 4 + aux_dim\n",
    "        self.score_mlp = make_mlp(mlp_input, hidden_dims=hidden, output_dim=1, dropout=dropout)\n",
    "\n",
    "    def forward(self, recipient_vec, donor_vecs, aux_feats):\n",
    "        # recipient_vec: [D] or [1,D]\n",
    "        if recipient_vec.dim() == 1:\n",
    "            r = recipient_vec.unsqueeze(0).expand(donor_vecs.shape[0], -1)\n",
    "        else:\n",
    "            r = recipient_vec.expand(donor_vecs.shape[0], -1)\n",
    "        absdiff = torch.abs(r - donor_vecs)\n",
    "        prod = r * donor_vecs\n",
    "        x = torch.cat([r, donor_vecs, absdiff, prod, aux_feats], dim=1)\n",
    "        scores = self.score_mlp(x).squeeze(-1)  # [M]\n",
    "        return scores\n",
    "\n",
    "class HGTDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Orchestrator that contains RecipientFinder and DonorFinder and provides utility compute_aggregates.\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_vec_dim, aux_recipient_dim=3, aux_donor_dim=3,\n",
    "                 rec_hidden=[16], donor_hidden=[16], hgt_threshold=0.5, topk_donors=1):\n",
    "        super().__init__()\n",
    "        self.recipient_finder = RecipientFinder(seq_vec_dim, aux_dim=aux_recipient_dim, hidden=rec_hidden)\n",
    "        self.donor_finder = DonorFinder(seq_vec_dim, aux_dim=aux_donor_dim, hidden=donor_hidden)\n",
    "        self.hgt_threshold = hgt_threshold\n",
    "        self.topk_donors = topk_donors\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_aggregates(G: nx.DiGraph, node_seq_matrix: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Sum one-hot vectors of leaves under each node.\n",
    "        Assumes node_seq_matrix rows correspond to node indices aligned with node ids in G (0..N-1),\n",
    "        otherwise adapt mapping externally.\n",
    "        \"\"\"\n",
    "        nodes = list(G.nodes)\n",
    "        N = len(nodes)\n",
    "        # Build mapping node_id -> index if node ids are not 0..N-1\n",
    "        if nodes == list(range(N)):\n",
    "            node_to_idx = {n: n for n in nodes}\n",
    "        else:\n",
    "            node_to_idx = {n: i for i, n in enumerate(nodes)}\n",
    "        children = {n: list(G.successors(n)) for n in nodes}\n",
    "\n",
    "        # We'll create agg in the same indexing as node_to_idx\n",
    "        D = node_seq_matrix.shape[1]\n",
    "        agg = torch.zeros((N, D), dtype=node_seq_matrix.dtype, device=node_seq_matrix.device)\n",
    "\n",
    "        # If node_seq_matrix ordering matches node_to_idx, copy directly\n",
    "        # Attempt to copy: if node_seq_matrix has N rows we copy by index order 0..N-1\n",
    "        if node_seq_matrix.shape[0] == N:\n",
    "            try:\n",
    "                agg = node_seq_matrix.clone()\n",
    "            except Exception:\n",
    "                agg = node_seq_matrix.clone().to(node_seq_matrix.device)\n",
    "        else:\n",
    "            # fallback: place available rows in order of nodes if possible (rare)\n",
    "            raise RuntimeError(\"node_seq_matrix rows != number of nodes; please supply node-ordered matrix.\")\n",
    "\n",
    "        # children processed before parent: do topological sort\n",
    "        topo = list(nx.topological_sort(G))\n",
    "\n",
    "        for node in reversed(topo):\n",
    "            i = node_to_idx[node]\n",
    "            for c in children[node]:\n",
    "                j = node_to_idx[c]\n",
    "                agg[i] = torch.max(agg[i], agg[j])\n",
    "        return agg\n",
    "\n",
    "# -------------------------\n",
    "# Utilities for training\n",
    "# -------------------------\n",
    "def tree_distance(G: nx.DiGraph, node_a, node_b, max_penalty=None):\n",
    "    \"\"\"\n",
    "    Shortest path length in undirected tree, fallback to max_penalty or len(G.nodes)\n",
    "    \"\"\"\n",
    "    und = G.to_undirected()\n",
    "    try:\n",
    "        return nx.shortest_path_length(und, source=node_a, target=node_b)\n",
    "    except Exception:\n",
    "        return max_penalty if max_penalty is not None else len(G.nodes)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Training loop (fully adjusted)\n",
    "# -------------------------\n",
    "def train_hgt_detector(model: HGTDetector, list_of_Data, epochs=10, val_frac=0.1, lr=1e-3,\n",
    "                       lambda_donor=0.1, hgt_threshold=0.5, max_candidates=300,\n",
    "                       device=None, clip_grad=5.0):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "    N = len(list_of_Data)\n",
    "    idxs = list(range(N))\n",
    "    random.shuffle(idxs)\n",
    "    n_val = max(1, int(val_frac * N))\n",
    "    val_idx = set(idxs[:n_val])\n",
    "    train_idx = idxs[n_val:]\n",
    "\n",
    "    # Estimate pos_weight for BCEWithLogitsLoss on training set (clamped)\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    for i in train_idx:\n",
    "        d = list_of_Data[i]\n",
    "        recip_map = d.recip_label_map\n",
    "        _ = d.donor_map\n",
    "        for p, (lbl, _) in recip_map.items():\n",
    "            if lbl == 1:\n",
    "                pos_count += 1\n",
    "            else:\n",
    "                neg_count += 1\n",
    "    pos_count = max(pos_count, 1)\n",
    "    neg_count = max(neg_count, 1)\n",
    "    ratio = neg_count / pos_count\n",
    "    pos_weight_val = max(1.0, min(ratio, 50.0))  # clamp between 1 and 50\n",
    "    pos_weight = torch.tensor([pos_weight_val], device=device)\n",
    "    bce_loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    print(f\"Training on {len(train_idx)} graphs | Validation on {len(val_idx)} graphs | pos_weight={pos_weight_val:.3f}\")\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_rec_loss = 0.0\n",
    "        train_don_loss = 0.0\n",
    "        train_samples = 0\n",
    "\n",
    "        # metrics\n",
    "        tp = 0; fp = 0; fn = 0\n",
    "        donor_expected_distance_accum = 0.0\n",
    "        donor_expected_count = 0\n",
    "\n",
    "        random.shuffle(train_idx)\n",
    "        for idx in train_idx:\n",
    "            data = list_of_Data[idx]\n",
    "            # move seqs to device once\n",
    "            data.hot_encoded_nucleotide_sequences = data.hot_encoded_nucleotide_sequences.to(device)\n",
    "\n",
    "            recip_map = data.recip_label_map\n",
    "            donor_map = data.donor_map\n",
    "            parents = [n for n in data.G.nodes if len(list(data.G.successors(n))) == 2]\n",
    "            if len(parents) == 0:\n",
    "                continue\n",
    "\n",
    "            # compute aggregates\n",
    "            agg = model.compute_aggregates(data.G, data.hot_encoded_nucleotide_sequences)  # [N,D]\n",
    "\n",
    "            # collect batch lists\n",
    "            L_list = []\n",
    "            R_list = []\n",
    "            aux_list = []\n",
    "            true_label_list = []\n",
    "            true_rec_child_list = []\n",
    "            true_donor_parent_list = []\n",
    "            parent_nodes_list = []\n",
    "\n",
    "            for parent in parents:\n",
    "                children = list(data.G.successors(parent))\n",
    "                left, right = children[0], children[1]\n",
    "                L_list.append(agg[left].unsqueeze(0))\n",
    "                R_list.append(agg[right].unsqueeze(0))\n",
    "                # aux: node_time, level, gene_frac\n",
    "                node_time = float(data.G.nodes[parent].get('node_time', 0.0))\n",
    "                node_level = float(data.G.nodes[parent].get('level', 0.0))\n",
    "                # avoid divide by zero\n",
    "                parent_sum = agg[parent].sum().clamp(min=1.0)\n",
    "                gene_frac = ((agg[left].sum() + agg[right].sum()) / parent_sum).unsqueeze(0).unsqueeze(1)\n",
    "                aux = torch.tensor([[node_time, node_level, float(gene_frac.item())]], device=device, dtype=torch.float32)\n",
    "                aux_list.append(aux)\n",
    "                lbl, true_rec = recip_map.get(parent, (0, None))\n",
    "                true_label_list.append(lbl)\n",
    "                true_rec_child_list.append(true_rec)\n",
    "                true_donor_parent_list.append(donor_map.get(parent, None))\n",
    "                parent_nodes_list.append(parent)\n",
    "\n",
    "            left_vecs = torch.cat(L_list, dim=0)      # [P, D]\n",
    "            right_vecs = torch.cat(R_list, dim=0)     # [P, D]\n",
    "            aux_feats = torch.cat(aux_list, dim=0)    # [P, 3]\n",
    "            true_labels = torch.tensor(true_label_list, dtype=torch.float32, device=device)  # [P]\n",
    "\n",
    "            # Recipient forward\n",
    "            hgt_logits, which_logits = model.recipient_finder(left_vecs, right_vecs, aux_feats)\n",
    "            rec_loss = bce_loss_fn(hgt_logits, true_labels)\n",
    "\n",
    "            # compute predicted probabilities & which\n",
    "            probs = torch.sigmoid(hgt_logits).detach().cpu().numpy()\n",
    "            which_pred = torch.argmax(which_logits.detach().cpu(), dim=1).tolist()\n",
    "\n",
    "            # Donor loss (differentiable expected distance) aggregated for events\n",
    "            donor_loss_total = torch.tensor(0.0, device=device)\n",
    "            donor_events = 0\n",
    "            # For each parent with true_label==1, check whether recipient predicted correctly\n",
    "            for i_parent, parent in enumerate(parent_nodes_list):\n",
    "                lbl = int(true_labels[i_parent].item())\n",
    "                if lbl != 1:\n",
    "                    continue\n",
    "                true_rec_child = true_rec_child_list[i_parent]\n",
    "                true_donor_parent = true_donor_parent_list[i_parent]\n",
    "                # predicted recipient child:\n",
    "                pred_rec_child = list(data.G.successors(parent))[which_pred[i_parent]]\n",
    "                pred_prob = float(probs[i_parent])\n",
    "                # condition: only add donor loss if recipient correctly identified\n",
    "                if (pred_prob >= hgt_threshold) and (true_rec_child is not None) and (pred_rec_child == true_rec_child):\n",
    "                    # build candidate list: nodes with node_time < recipient_time\n",
    "                    rec_node = pred_rec_child\n",
    "                    rec_time = float(data.G.nodes[rec_node].get('node_time', 0.0))\n",
    "                    candidates = [n for n in data.G.nodes if float(data.G.nodes[n].get('node_time',0.0)) < rec_time and n != rec_node]\n",
    "                    # ensure true donor parent is among candidates - if not, append it (keeps learning signal)\n",
    "                    if true_donor_parent is not None and true_donor_parent not in candidates:\n",
    "                        candidates.append(true_donor_parent)\n",
    "                    if len(candidates) == 0:\n",
    "                        # no candidates -> penalize by max distance (normalized)\n",
    "                        max_pen = len(data.G.nodes)\n",
    "                        donor_loss_total = donor_loss_total + (torch.tensor(float(max_pen), device=device) / float(max_pen))\n",
    "                        donor_events += 1\n",
    "                        continue\n",
    "\n",
    "                    # sample candidates if too many\n",
    "                    if len(candidates) > max_candidates:\n",
    "                        random.shuffle(candidates)\n",
    "                        candidates = candidates[:max_candidates]\n",
    "                    cand_idx_tensor = torch.tensor(candidates, dtype=torch.long, device=device)\n",
    "\n",
    "                    donor_vecs = agg[cand_idx_tensor]  # [M, D]\n",
    "                    donor_times = torch.tensor([float(data.G.nodes[n].get('node_time',0.0)) for n in candidates], device=device)\n",
    "                    donor_levels = torch.tensor([float(data.G.nodes[n].get('level',0.0)) for n in candidates], device=device)\n",
    "                    time_diff = (rec_time - donor_times).unsqueeze(1)              # [M,1]\n",
    "                    level_diff = (data.G.nodes[rec_node].get('level',0.0) - donor_levels).unsqueeze(1)\n",
    "                    donor_frac = (donor_vecs.sum(dim=1).unsqueeze(1) / (agg.sum(dim=1).clamp(min=1.0)[cand_idx_tensor].unsqueeze(1))).clamp(0.0,1.0)\n",
    "                    donor_aux = torch.cat([time_diff, level_diff, donor_frac], dim=1)\n",
    "\n",
    "                    # get raw scores (no softmax yet)\n",
    "                    scores = model.donor_finder(agg[rec_node].to(device), donor_vecs, donor_aux)  # [M]\n",
    "\n",
    "                    # compute distances vector to true donor\n",
    "                    # ensure we have a numeric dist for each candidate\n",
    "                    und = data.G.to_undirected()\n",
    "                    max_pen = len(data.G.nodes)\n",
    "                    dists = []\n",
    "                    for cand in candidates:\n",
    "                        if true_donor_parent is None:\n",
    "                            # if no true donor known, give zero distance (no loss)\n",
    "                            dists.append(0.0)\n",
    "                        else:\n",
    "                            try:\n",
    "                                dd = nx.shortest_path_length(und, source=cand, target=true_donor_parent)\n",
    "                                dists.append(float(dd))\n",
    "                            except Exception:\n",
    "                                dists.append(float(max_pen))\n",
    "                    dists = torch.tensor(dists, dtype=torch.float32, device=device)  # [M]\n",
    "\n",
    "                    # Softmax probabilities over scores (temperature can be used)\n",
    "                    probs_soft = torch.softmax(scores, dim=0)  # [M]\n",
    "                    # Expected distance (differentiable)\n",
    "                    expected_dist = torch.dot(probs_soft, dists)  # scalar\n",
    "                    # Normalize by max_pen to keep scale ~[0,1]\n",
    "                    expected_dist = expected_dist / float(max_pen)\n",
    "\n",
    "                    donor_loss_total = donor_loss_total + expected_dist\n",
    "                    donor_events += 1\n",
    "\n",
    "                    # accumulate for reporting expected donor distance (as float)\n",
    "                    donor_expected_distance_accum += float((expected_dist * float(max_pen)).item())\n",
    "                    donor_expected_count += 1\n",
    "\n",
    "            # Normalize donor loss by donor_events (if >0)\n",
    "            if donor_events > 0:\n",
    "                donor_loss = donor_loss_total / donor_events\n",
    "            else:\n",
    "                donor_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "            loss = rec_loss + lambda_donor * donor_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), clip_grad)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_rec_loss += rec_loss.item()\n",
    "            train_don_loss += float(donor_loss.item())\n",
    "            train_samples += 1\n",
    "\n",
    "            # update metrics for recipient detection\n",
    "            for i_parent, parent in enumerate(parent_nodes_list):\n",
    "                true_lbl = int(true_labels[i_parent].item())\n",
    "                pred_prob = float(probs[i_parent])\n",
    "                pred_class = 1 if pred_prob >= hgt_threshold else 0\n",
    "                if pred_class == 1 and true_lbl == 1:\n",
    "                    tp += 1\n",
    "                if pred_class == 1 and true_lbl == 0:\n",
    "                    fp += 1\n",
    "                if pred_class == 0 and true_lbl == 1:\n",
    "                    fn += 1\n",
    "\n",
    "        # End epoch training stats\n",
    "        avg_train_loss = train_loss / max(1, train_samples)\n",
    "        avg_train_rec = train_rec_loss / max(1, train_samples)\n",
    "        avg_train_don = train_don_loss / max(1, train_samples)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        avg_expected_donor_distance = (donor_expected_distance_accum / donor_expected_count) if donor_expected_count > 0 else 0.0\n",
    "\n",
    "        # Validation pass\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            val_rec_loss = 0.0\n",
    "            val_don_loss = 0.0\n",
    "            val_samples = 0\n",
    "            v_tp = v_fp = v_fn = 0\n",
    "            v_donor_expected_dist_acc = 0.0\n",
    "            v_donor_expected_cnt = 0\n",
    "\n",
    "            for idx in val_idx:\n",
    "                data = list_of_Data[idx]\n",
    "                data.hot_encoded_nucleotide_sequences = data.hot_encoded_nucleotide_sequences.to(device)\n",
    "                recip_map = data.recip_label_map\n",
    "                donor_map = data.donor_map\n",
    "                parents = [n for n in data.G.nodes if len(list(data.G.successors(n))) == 2]\n",
    "                if len(parents) == 0:\n",
    "                    continue\n",
    "                agg = model.compute_aggregates(data.G, data.hot_encoded_nucleotide_sequences)\n",
    "                # collect lists\n",
    "                L_list, R_list, aux_list = [], [], []\n",
    "                true_labels = []\n",
    "                true_rec_child_list = []\n",
    "                true_donor_parent_list = []\n",
    "                parent_nodes_list = []\n",
    "                for parent in parents:\n",
    "                    left, right = list(data.G.successors(parent))[0], list(data.G.successors(parent))[1]\n",
    "                    L_list.append(agg[left].unsqueeze(0))\n",
    "                    R_list.append(agg[right].unsqueeze(0))\n",
    "                    node_time = float(data.G.nodes[parent].get('node_time', 0.0))\n",
    "                    node_level = float(data.G.nodes[parent].get('level', 0.0))\n",
    "                    parent_sum = agg[parent].sum().clamp(min=1.0)\n",
    "                    gene_frac = ((agg[left].sum() + agg[right].sum()) / parent_sum).unsqueeze(0).unsqueeze(1)\n",
    "                    aux = torch.tensor([[node_time, node_level, float(gene_frac.item())]], device=device, dtype=torch.float32)\n",
    "                    aux_list.append(aux)\n",
    "                    lbl, true_rec = recip_map.get(parent, (0, None))\n",
    "                    true_labels.append(lbl)\n",
    "                    true_rec_child_list.append(true_rec)\n",
    "                    true_donor_parent_list.append(donor_map.get(parent, None))\n",
    "                    parent_nodes_list.append(parent)\n",
    "                left_vecs = torch.cat(L_list, dim=0)\n",
    "                right_vecs = torch.cat(R_list, dim=0)\n",
    "                aux_feats = torch.cat(aux_list, dim=0)\n",
    "                true_labels = torch.tensor(true_labels, dtype=torch.float32, device=device)\n",
    "\n",
    "                hgt_logits, which_logits = model.recipient_finder(left_vecs, right_vecs, aux_feats)\n",
    "                rec_loss = bce_loss_fn(hgt_logits, true_labels)\n",
    "\n",
    "                # donor loss computation: same expected-distance approach but without gradients\n",
    "                probs = torch.sigmoid(hgt_logits).cpu().numpy()\n",
    "                which_pred = torch.argmax(which_logits, dim=1).tolist()\n",
    "\n",
    "                donor_loss_total = 0.0\n",
    "                donor_events = 0\n",
    "                for i_parent, parent in enumerate(parent_nodes_list):\n",
    "                    lbl = int(true_labels[i_parent].item())\n",
    "                    if lbl != 1:\n",
    "                        continue\n",
    "                    true_rec_child = true_rec_child_list[i_parent]\n",
    "                    true_donor_parent = true_donor_parent_list[i_parent]\n",
    "                    pred_rec_child = list(data.G.successors(parent))[which_pred[i_parent]]\n",
    "                    pred_prob = float(probs[i_parent])\n",
    "                    if (pred_prob >= hgt_threshold) and (true_rec_child is not None) and (pred_rec_child == true_rec_child):\n",
    "                        rec_node = pred_rec_child\n",
    "                        rec_time = float(data.G.nodes[rec_node].get('node_time', 0.0))\n",
    "                        candidates = [n for n in data.G.nodes if float(data.G.nodes[n].get('node_time',0.0)) < rec_time and n != rec_node]\n",
    "                        if true_donor_parent is not None and true_donor_parent not in candidates:\n",
    "                            candidates.append(true_donor_parent)\n",
    "                        if len(candidates) == 0:\n",
    "                            donor_loss_total += 1.0\n",
    "                            donor_events += 1\n",
    "                            continue\n",
    "                        if len(candidates) > max_candidates:\n",
    "                            random.shuffle(candidates)\n",
    "                            candidates = candidates[:max_candidates]\n",
    "                        cand_idx_tensor = torch.tensor(candidates, dtype=torch.long, device=device)\n",
    "                        donor_vecs = agg[cand_idx_tensor]\n",
    "                        donor_times = torch.tensor([float(data.G.nodes[n].get('node_time',0.0)) for n in candidates], device=device)\n",
    "                        donor_levels = torch.tensor([float(data.G.nodes[n].get('level',0.0)) for n in candidates], device=device)\n",
    "                        time_diff = (rec_time - donor_times).unsqueeze(1)\n",
    "                        level_diff = (data.G.nodes[rec_node].get('level',0.0) - donor_levels).unsqueeze(1)\n",
    "                        donor_frac = (donor_vecs.sum(dim=1).unsqueeze(1) / (agg.sum(dim=1).clamp(min=1.0)[cand_idx_tensor].unsqueeze(1))).clamp(0.0,1.0)\n",
    "                        donor_aux = torch.cat([time_diff, level_diff, donor_frac], dim=1)\n",
    "                        scores = model.donor_finder(agg[rec_node].to(device), donor_vecs, donor_aux)\n",
    "                        # distances\n",
    "                        und = data.G.to_undirected()\n",
    "                        max_pen = len(data.G.nodes)\n",
    "                        dists = []\n",
    "                        for cand in candidates:\n",
    "                            if true_donor_parent is None:\n",
    "                                dists.append(0.0)\n",
    "                            else:\n",
    "                                try:\n",
    "                                    dd = nx.shortest_path_length(und, source=cand, target=true_donor_parent)\n",
    "                                    dists.append(float(dd))\n",
    "                                except Exception:\n",
    "                                    dists.append(float(max_pen))\n",
    "                        dists = torch.tensor(dists, dtype=torch.float32, device=device)\n",
    "                        probs_soft = torch.softmax(scores, dim=0)\n",
    "                        expected_dist = float(torch.dot(probs_soft, dists).item()) / float(max_pen)\n",
    "                        donor_loss_total += expected_dist\n",
    "                        donor_events += 1\n",
    "                        v_donor_expected_dist_acc += (expected_dist * float(max_pen))\n",
    "                        v_donor_expected_cnt += 1\n",
    "\n",
    "                donor_loss = (donor_loss_total / donor_events) if donor_events > 0 else 0.0\n",
    "                loss = rec_loss.item() + lambda_donor * donor_loss\n",
    "\n",
    "                val_loss += loss\n",
    "                val_rec_loss += rec_loss.item()\n",
    "                val_don_loss += donor_loss\n",
    "                val_samples += 1\n",
    "\n",
    "                # update val metrics for recipient detection\n",
    "                for i_parent, parent in enumerate(parent_nodes_list):\n",
    "                    true_lbl = int(true_labels[i_parent].item())\n",
    "                    pred_prob = float(probs[i_parent])\n",
    "                    pred_class = 1 if pred_prob >= hgt_threshold else 0\n",
    "                    if pred_class == 1 and true_lbl == 1:\n",
    "                        v_tp += 1\n",
    "                    if pred_class == 1 and true_lbl == 0:\n",
    "                        v_fp += 1\n",
    "                    if pred_class == 0 and true_lbl == 1:\n",
    "                        v_fn += 1\n",
    "\n",
    "            avg_val_loss = val_loss / max(1, val_samples)\n",
    "            avg_val_rec = val_rec_loss / max(1, val_samples)\n",
    "            avg_val_don = val_don_loss / max(1, val_samples)\n",
    "            v_precision = v_tp / (v_tp + v_fp) if (v_tp + v_fp) > 0 else 0.0\n",
    "            v_recall = v_tp / (v_tp + v_fn) if (v_tp + v_fn) > 0 else 0.0\n",
    "            v_f1 = 2 * v_precision * v_recall / (v_precision + v_recall) if (v_precision + v_recall) > 0 else 0.0\n",
    "            avg_v_donor_expected = (v_donor_expected_dist_acc / v_donor_expected_cnt) if v_donor_expected_cnt > 0 else 0.0\n",
    "\n",
    "        # scheduler step (use validation loss)\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        t1 = time.time()\n",
    "        print(f\"Epoch {epoch}/{epochs} | time {t1-t0:.1f}s\")\n",
    "        print(f\" TRAIN loss {avg_train_loss:.4f} (rec {avg_train_rec:.4f}, don {avg_train_don:.4f}) \"\n",
    "              f\"Prec/Rec/F1 {precision:.3f}/{recall:.3f}/{f1:.3f} | avg_expected_donor_dist {avg_expected_donor_distance:.3f}\")\n",
    "        print(f\" VAL   loss {avg_val_loss:.4f} (rec {avg_val_rec:.4f}, don {avg_val_don:.4f}) \"\n",
    "              f\"Prec/Rec/F1 {v_precision:.3f}/{v_recall:.3f}/{v_f1:.3f} | avg_expected_donor_dist {avg_v_donor_expected:.3f}\")\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# choose a sample data to get seq_vec_dim\n",
    "sample_data = random.choice(list_of_Data)\n",
    "seq_vec_dim = sample_data.hot_encoded_nucleotide_sequences.shape[1]\n",
    "model = HGTDetector(seq_vec_dim=seq_vec_dim, hgt_threshold=0.5, topk_donors=1)\n",
    "\n",
    "trained_model = train_hgt_detector(model, list_of_Data,\n",
    "                                  epochs=4,\n",
    "                                  val_frac=0.1,\n",
    "                                  lr=1e-3,\n",
    "                                  lambda_donor=0.1,\n",
    "                                  hgt_threshold=0.5,\n",
    "                                  max_candidates=300,\n",
    "                                  device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                                  clip_grad=5.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b6ca72b-85fa-497c-b1a2-9d2a73dce806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 90 graphs | Validation on 10 graphs | pos_weight=13.400\n",
      "Epoch 1/4 | time 0.3s\n",
      " TRAIN loss 1.845382 (rec 1.845382, don 0.000000) Prec/Rec/F1 0.000/0.000/0.000 | avg_expected_donor_dist 0.000\n",
      " VAL   loss 1.166914 (rec 1.166914, don 0.000000) Prec/Rec/F1 0.000/0.000/0.000 | avg_expected_donor_dist 0.000\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "[0, 1, 2, 3, 4, 8, 7, 6, 5, 9]\n",
      "[(8, 7), (8, 9), (7, 2), (7, 5), (6, 0), (6, 1), (5, 4), (5, 3), (9, 6)]\n",
      "[(5, 3), (5, 4), (8, 0), (8, 9), (7, 6), (7, 9), (6, 2), (6, 5), (9, 1), (9, 7)]\n",
      "[0, 1, 2, 3, 4, 5, 8, 7, 6, 9]\n"
     ]
    },
    {
     "ename": "NetworkXUnfeasible",
     "evalue": "Graph contains a cycle or graph changed during iteration",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNetworkXUnfeasible\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 807\u001b[0m\n\u001b[1;32m    804\u001b[0m seq_vec_dim \u001b[38;5;241m=\u001b[39m sample_data\u001b[38;5;241m.\u001b[39mhot_encoded_nucleotide_sequences\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    805\u001b[0m model \u001b[38;5;241m=\u001b[39m HGTDetector(seq_vec_dim\u001b[38;5;241m=\u001b[39mseq_vec_dim, hgt_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, topk_donors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 807\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_hgt_detector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlist_of_Data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mval_frac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mlambda_donor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mhgt_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mmax_candidates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mclip_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5.0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 512\u001b[0m, in \u001b[0;36mtrain_hgt_detector\u001b[0;34m(model, list_of_Data, epochs, val_frac, lr, lambda_donor, hgt_threshold, max_candidates, device, clip_grad)\u001b[0m\n\u001b[1;32m    509\u001b[0m G_mod\u001b[38;5;241m.\u001b[39madd_edge(new_node, donor_child)\n\u001b[1;32m    511\u001b[0m \u001b[38;5;66;03m# After structural change, recompute aggregates for the modified graph.\u001b[39;00m\n\u001b[0;32m--> 512\u001b[0m agg_current \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_aggregates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG_mod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_seq_matrix_current\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgene_presence_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    513\u001b[0m node_sums_current \u001b[38;5;241m=\u001b[39m agg_current\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# update G_current to modified version for further upward processing\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 207\u001b[0m, in \u001b[0;36mHGTDetector.compute_aggregates\u001b[0;34m(self, G, node_seq_matrix, gene_presence_mask)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28mprint\u001b[39m(G\u001b[38;5;241m.\u001b[39medges)\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mprint\u001b[39m(G\u001b[38;5;241m.\u001b[39mnodes)\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopological_sort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m    208\u001b[0m     children \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(G\u001b[38;5;241m.\u001b[39msuccessors(node))\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(children) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;66;03m# Leaf already initialized\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/networkx/algorithms/dag.py:309\u001b[0m, in \u001b[0;36mtopological_sort\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;129m@nx\u001b[39m\u001b[38;5;241m.\u001b[39m_dispatchable\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtopological_sort\u001b[39m(G):\n\u001b[1;32m    246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a generator of nodes in topologically sorted order.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m    A topological sort is a nonunique permutation of the nodes of a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;03m       *Introduction to Algorithms - A Creative Approach.* Addison-Wesley.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgeneration\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopological_generations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgeneration\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pangenome-hgt-sim/lib/python3.12/site-packages/networkx/algorithms/dag.py:239\u001b[0m, in \u001b[0;36mtopological_generations\u001b[0;34m(G)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m this_generation\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indegree_map:\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mNetworkXUnfeasible(\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph contains a cycle or graph changed during iteration\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    241\u001b[0m     )\n",
      "\u001b[0;31mNetworkXUnfeasible\u001b[0m: Graph contains a cycle or graph changed during iteration"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import h5py\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import networkx as nx\n",
    "\n",
    "# -------------------------\n",
    "# Model components\n",
    "# -------------------------\n",
    "\n",
    "def make_mlp(input_dim, hidden_dims=[32], output_dim=None, dropout=0.1):\n",
    "    \"\"\"\n",
    "    Build an MLP that uses LayerNorm instead of BatchNorm.\n",
    "    LayerNorm is stable for batch size 1.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    dims = [input_dim] + hidden_dims\n",
    "    for i in range(len(hidden_dims)):\n",
    "        layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "        # use LayerNorm over feature dimension\n",
    "        layers.append(nn.LayerNorm(dims[i+1]))\n",
    "        layers.append(nn.ReLU())\n",
    "        layers.append(nn.Dropout(dropout))\n",
    "    if output_dim is not None:\n",
    "        layers.append(nn.Linear(dims[-1], output_dim))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class RecipientFinder(nn.Module):\n",
    "    \"\"\"\n",
    "    Binary detector: for a parent (given aggregated vectors of its two children + aux),\n",
    "    predict whether an HGT event (recipient) occurred at this parent.\n",
    "\n",
    "    Output:\n",
    "      - hgt_logit: raw logit (no sigmoid) shape [B]\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_vec_dim, aux_dim=3, hidden=[32], dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.seq_vec_dim = seq_vec_dim\n",
    "        self.aux_dim = aux_dim\n",
    "        # input: left, right, |left-right|, left*right, aux => 4*D + aux_dim\n",
    "        mlp_input = seq_vec_dim * 4 + aux_dim\n",
    "        self.mlp = make_mlp(mlp_input, hidden_dims=hidden, output_dim=None, dropout=dropout)\n",
    "        self.hgt_head = nn.Linear(hidden[-1], 1)    # logit\n",
    "\n",
    "    def forward(self, left_vec, right_vec, aux_feats):\n",
    "        # left_vec/right_vec: [B, D]\n",
    "        absdiff = torch.abs(left_vec - right_vec)\n",
    "        prod = left_vec * right_vec\n",
    "        x = torch.cat([left_vec, right_vec, absdiff, prod, aux_feats], dim=1)\n",
    "        h = self.mlp(x)\n",
    "        hgt_logit = self.hgt_head(h).squeeze(-1)         # [B]\n",
    "        return hgt_logit\n",
    "\n",
    "\n",
    "class DonorFinder(nn.Module):\n",
    "    \"\"\"\n",
    "    The DonorFinder now:\n",
    "      - takes the aggregated vectors for the two daughters (left_vec, right_vec)\n",
    "      - compares each daughter's aggregated-sum with aggregated sums of candidate nodes\n",
    "        with node_time > daughter's node_time (as per your spec)\n",
    "      - returns:\n",
    "         which_pred: 0 if left is recipient, 1 if right is recipient (the one that best matches)\n",
    "         donor_parent_pred: node id of the best matching donor-parent\n",
    "         donor_score: optional score (float) for the chosen donor\n",
    "    Note: This implementation uses simple scalar-sum comparisons (L1 on sums) for speed/clarity.\n",
    "    You can replace the distance metric with vector L1/L2 if desired.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # DonorFinder here is mostly procedural (no learnable params in this implementation).\n",
    "        # If you want learnable scoring, you can replace the matching with an MLP.\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def find_donor_and_which(agg_tensor, G, parent, left, right, node_seq_sums=None):\n",
    "        \"\"\"\n",
    "        Find which daughter is predicted recipient (0=left,1=right) and the donor_parent node.\n",
    "        - agg_tensor: [N, D] tensor of aggregated leaf-vectors for nodes (consistent with node order)\n",
    "        - G: networkx DiGraph (parent->child edges)\n",
    "        - parent, left, right: node ids (integers, consistent with agg ordering)\n",
    "        - node_seq_sums: optional precomputed 1D tensor [N] with sum over features per node\n",
    "          (if None, computed from agg_tensor as agg_tensor.sum(dim=1))\n",
    "        Returns: which_pred (int 0/1), donor_parent_pred (node id or None), donor_score (float)\n",
    "        \"\"\"\n",
    "        # Compute scalar sums if not provided\n",
    "        if node_seq_sums is None:\n",
    "            node_seq_sums = agg_tensor.sum(dim=1)  # [N]\n",
    "\n",
    "        # times of daughters\n",
    "        left_time = float(G.nodes[left].get('node_time', 0.0))\n",
    "        right_time = float(G.nodes[right].get('node_time', 0.0))\n",
    "\n",
    "        # candidate sets: nodes with node_time > daughter_time\n",
    "        node_times = {n: float(G.nodes[n].get('node_time', 0.0)) for n in G.nodes}\n",
    "        candidates_for_left = [n for n in G.nodes if node_times[n] > left_time and n != left]\n",
    "        candidates_for_right = [n for n in G.nodes if node_times[n] > right_time and n != right]\n",
    "\n",
    "        # handle empties\n",
    "        if len(candidates_for_left) == 0:\n",
    "            best_left = (None, float('inf'))\n",
    "        else:\n",
    "            left_sum = float(node_seq_sums[left].item())\n",
    "            # compute abs diff between candidate sums and left_sum\n",
    "            cand_idx_left = torch.tensor(candidates_for_left, dtype=torch.long)\n",
    "            cand_sums_left = node_seq_sums[cand_idx_left]\n",
    "            diffs_left = torch.abs(cand_sums_left - left_sum)\n",
    "            min_idx = torch.argmin(diffs_left).item()\n",
    "            best_left = (candidates_for_left[min_idx], float(diffs_left[min_idx].item()))\n",
    "\n",
    "        if len(candidates_for_right) == 0:\n",
    "            best_right = (None, float('inf'))\n",
    "        else:\n",
    "            right_sum = float(node_seq_sums[right].item())\n",
    "            cand_idx_right = torch.tensor(candidates_for_right, dtype=torch.long)\n",
    "            cand_sums_right = node_seq_sums[cand_idx_right]\n",
    "            diffs_right = torch.abs(cand_sums_right - right_sum)\n",
    "            min_idx = torch.argmin(diffs_right).item()\n",
    "            best_right = (candidates_for_right[min_idx], float(diffs_right[min_idx].item()))\n",
    "\n",
    "        # Decide which daughter yields smaller minimal diff -> that daughter considered recipient\n",
    "        if best_left[1] <= best_right[1]:\n",
    "            which = 0\n",
    "            donor_parent = best_left[0]\n",
    "            donor_score = best_left[1]\n",
    "        else:\n",
    "            which = 1\n",
    "            donor_parent = best_right[0]\n",
    "            donor_score = best_right[1]\n",
    "\n",
    "        return which, donor_parent, donor_score\n",
    "\n",
    "\n",
    "class HGTDetector(nn.Module):\n",
    "    \"\"\"\n",
    "    Orchestrator that contains RecipientFinder and DonorFinder and provides utility compute_aggregates.\n",
    "    Note: DonorFinder in this design is procedural (no learnable params). If you want learnable\n",
    "    scoring, convert DonorFinder to use an MLP and return differentiable scores.\n",
    "    \"\"\"\n",
    "    def __init__(self, seq_vec_dim, aux_recipient_dim=3, hgt_threshold=0.5, topk_donors=1):\n",
    "        super().__init__()\n",
    "        self.recipient_finder = RecipientFinder(seq_vec_dim, aux_dim=aux_recipient_dim, hidden=[32])\n",
    "        self.hgt_threshold = hgt_threshold\n",
    "        self.topk_donors = topk_donors\n",
    "\n",
    "    def compute_aggregates(self, G: nx.DiGraph, node_seq_matrix: torch.Tensor, gene_presence_mask: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Compute subtree aggregates for every node in G.\n",
    "\n",
    "        - G: networkx.DiGraph with nodes that have integer ids. We assume that the original node ids\n",
    "             correspond to the row indices of node_seq_matrix for nodes 0..N-1. If your node ids\n",
    "             are different, you must remap externally.\n",
    "        - node_seq_matrix: torch.Tensor [N_orig, D] containing leaf one-hot / feature vectors aligned\n",
    "             with original node ids (0..N_orig-1). If new internal nodes are appended later, extend this\n",
    "             matrix with zero rows before calling compute_aggregates for the expanded graph.\n",
    "        - gene_presence_mask: optional 1D torch tensor length N_orig with 1 for leaves that should be counted,\n",
    "             0 otherwise. If provided, it will zero out rows where mask==0 before aggregation.\n",
    "\n",
    "        Returns:\n",
    "          agg: torch.Tensor [N_mod, D] where N_mod == number of nodes in G (list(G.nodes) order).\n",
    "        Important assumptions / notes:\n",
    "          - If node_seq_matrix.shape[0] == N_nodes and node ids are 0..N-1 in order, we copy directly.\n",
    "          - If a new node is added (id == original N), the caller should have appended a zero-row\n",
    "            to node_seq_matrix so that the total rows match list(G.nodes) order.\n",
    "        \"\"\"\n",
    "        \n",
    "        N_nodes = len(list(G.nodes))\n",
    "        N, D = node_seq_matrix.shape\n",
    "        device = node_seq_matrix.device\n",
    "    \n",
    "        node_list = list(G.nodes)\n",
    "        node_to_idx = {n: i for i, n in enumerate(node_list)}\n",
    "    \n",
    "        # Initialize agg in node_list order\n",
    "        if node_seq_matrix.shape[0] == N_nodes:\n",
    "            agg = node_seq_matrix.clone().to(device)\n",
    "        elif node_seq_matrix.shape[0] >= N_nodes:\n",
    "            # if node_seq_matrix has at least as many rows, try to copy by node id where possible\n",
    "            agg = torch.zeros((N_nodes, D), dtype=node_seq_matrix.dtype, device=device)\n",
    "            for n, i in node_to_idx.items():\n",
    "                if isinstance(n, int) and n < node_seq_matrix.shape[0]:\n",
    "                    agg[i] = node_seq_matrix[n].to(device)\n",
    "                else:\n",
    "                    agg[i] = torch.zeros(D, dtype=node_seq_matrix.dtype, device=device)\n",
    "        else:\n",
    "            raise RuntimeError(\"node_seq_matrix rows != number of nodes; please supply node-ordered matrix or extend it.\")\n",
    "    \n",
    "        # optional gene presence mask: expects a mask aligned to leaves; map to node_list indices if needed\n",
    "        if gene_presence_mask is not None:\n",
    "            # if mask length equals number of nodes, use directly\n",
    "            if gene_presence_mask.shape[0] == N_nodes:\n",
    "                full_mask = gene_presence_mask.to(device).to(dtype=torch.float32)\n",
    "            else:\n",
    "                # best-effort: assume gene_presence_mask is aligned to some leaf ordering => leave as ones\n",
    "                full_mask = torch.ones(N_nodes, dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            full_mask = torch.ones(N_nodes, dtype=torch.float32, device=device)\n",
    "    \n",
    "        agg = agg * full_mask.unsqueeze(1)\n",
    "    \n",
    "        # bottom-up: children before parent\n",
    "        topo = list(nx.topological_sort(G))\n",
    "        for node in reversed(topo):\n",
    "            children = list(G.successors(node))\n",
    "            if len(children) == 0:\n",
    "                continue\n",
    "            child_vecs = [agg[node_to_idx[c]].unsqueeze(0) for c in children]\n",
    "            child_stack = torch.cat(child_vecs, dim=0)  # [C, D]\n",
    "            agg_val, _ = torch.max(child_stack, dim=0)\n",
    "            agg[node_to_idx[node]] = agg_val\n",
    "    \n",
    "        return agg, node_to_idx\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Utilities for training\n",
    "# -------------------------\n",
    "def tree_distance(G: nx.DiGraph, node_a, node_b, max_penalty=None):\n",
    "    \"\"\"\n",
    "    Compute time-based distance between node_a and node_b in a tree:\n",
    "      dist(a,b) = 2 * t_MRCA(a,b) - t(a) - t(b)\n",
    "\n",
    "    We find MRCA by intersecting ancestor sets (including node itself) and selecting\n",
    "    the one with maximum node_time. If no MRCA or times missing, fallback to max_penalty or len(G.nodes).\n",
    "    \"\"\"\n",
    "    #try:\n",
    "        # ancestors returns nodes that have a path to the given node (excluding the node itself),\n",
    "        # so include the node itself.\n",
    "    anc_a = set(nx.ancestors(G, node_a))\n",
    "    anc_a.add(node_a)\n",
    "    anc_b = set(nx.ancestors(G, node_b))\n",
    "    anc_b.add(node_b)\n",
    "    common = anc_a.intersection(anc_b)\n",
    "    if len(common) == 0:\n",
    "        raise RuntimeError(\"no common ancestor\")\n",
    "    # pick MRCA as the common ancestor with maximum node_time\n",
    "    def node_time(n):\n",
    "        return float(G.nodes[n].get('node_time', float('-inf')))\n",
    "    mrca = max(common, key=node_time)\n",
    "    t_mrca = node_time(mrca)\n",
    "    t_a = node_time(node_a)\n",
    "    t_b = node_time(node_b)\n",
    "    # if times are -inf or missing, fallback\n",
    "    if any([not math.isfinite(t) for t in (t_mrca, t_a, t_b)]):\n",
    "        raise RuntimeError(\"invalid times\")\n",
    "    dist = 2.0 * t_mrca - t_a - t_b\n",
    "    # ensure non-negative\n",
    "    return max(0.0, float(dist))\n",
    "    #except Exception:\n",
    "    #    return float(max_penalty) if max_penalty is not None else float(len(G.nodes))\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Training loop (modified to apply structure corrections on a copy)\n",
    "# -------------------------\n",
    "def train_hgt_detector(model: HGTDetector, list_of_Data, epochs=10, val_frac=0.1, lr=1e-3,\n",
    "                       lambda_donor=0.1, hgt_threshold=0.5, max_candidates=300,\n",
    "                       device=None, clip_grad=5.0):\n",
    "    \"\"\"\n",
    "    Training loop that:\n",
    "      - processes each internal parent node in time order (bottom-up)\n",
    "      - runs RecipientFinder (binary)\n",
    "      - if recipient predicted (p >= threshold) -> run DonorFinder to get which and donor_parent\n",
    "      - if recipient prediction matches ground truth and donor_parent matches ground truth:\n",
    "          * perform local structure correction on a deepcopy of G\n",
    "          * recompute aggregates for the modified graph\n",
    "          * continue processing from the parent node (the loop continues sequentially)\n",
    "      - computes losses:\n",
    "          - rec_loss: BCEWithLogitsLoss over recipient predictions (batched per graph)\n",
    "          - if recipient correctly predicted:\n",
    "              + add penalty if daughter-edge (which) is wrong (0 if correct, 1 if wrong)\n",
    "              + add (normalized) distance to true donor (time-based)\n",
    "        The donor components are scaled by lambda_donor (same as before).\n",
    "    \"\"\"\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "    N = len(list_of_Data)\n",
    "    idxs = list(range(N))\n",
    "    random.shuffle(idxs)\n",
    "    n_val = max(1, int(val_frac * N))\n",
    "    val_idx = set(idxs[:n_val])\n",
    "    train_idx = idxs[n_val:]\n",
    "\n",
    "    # Estimate pos_weight for BCEWithLogitsLoss on training set (clamped)\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    for i in train_idx:\n",
    "        d = list_of_Data[i]\n",
    "        recip_map = d.recip_label_map\n",
    "        for p, (lbl, _) in recip_map.items():\n",
    "            if lbl == 1:\n",
    "                pos_count += 1\n",
    "            else:\n",
    "                neg_count += 1\n",
    "    pos_count = max(pos_count, 1)\n",
    "    neg_count = max(neg_count, 1)\n",
    "    ratio = neg_count / pos_count\n",
    "    pos_weight_val = max(1.0, min(ratio, 50.0))  # clamp between 1 and 50\n",
    "    pos_weight = torch.tensor([pos_weight_val], device=device)\n",
    "    bce_loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    print(f\"Training on {len(train_idx)} graphs | Validation on {len(val_idx)} graphs | pos_weight={pos_weight_val:.3f}\")\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        t0 = time.time()\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_rec_loss = 0.0\n",
    "        train_don_loss = 0.0\n",
    "        train_samples = 0\n",
    "\n",
    "        # metrics\n",
    "        tp = 0; fp = 0; fn = 0\n",
    "        donor_expected_distance_accum = 0.0\n",
    "        donor_expected_count = 0\n",
    "\n",
    "        random.shuffle(train_idx)\n",
    "        for idx in train_idx:\n",
    "            data = list_of_Data[idx]\n",
    "            # move seqs to device once\n",
    "            # Assumption: data.hot_encoded_nucleotide_sequences rows correspond to node ids 0..M-1\n",
    "            data.hot_encoded_nucleotide_sequences = data.hot_encoded_nucleotide_sequences.to(device)\n",
    "\n",
    "            # optional gene presence mask (1D length M) that indicates which leaves count\n",
    "            gene_presence_mask = getattr(data, 'gene_absence_presence_matrix', None)\n",
    "            if gene_presence_mask is not None:\n",
    "                # ensure it's a tensor on device\n",
    "                if not isinstance(gene_presence_mask, torch.Tensor):\n",
    "                    gene_presence_mask = torch.tensor(gene_presence_mask, dtype=torch.float32, device=device)\n",
    "                else:\n",
    "                    gene_presence_mask = gene_presence_mask.to(device)\n",
    "\n",
    "            recip_map = data.recip_label_map\n",
    "            donor_map = data.donor_map\n",
    "\n",
    "            # We will iterate parents in increasing node_time order (bottom-up)\n",
    "            parents = [n for n in data.G.nodes if len(list(data.G.successors(n))) == 2]\n",
    "            if len(parents) == 0:\n",
    "                continue\n",
    "\n",
    "            # sort parents by node_time ascending (so we process earliest events first)\n",
    "            parents_sorted = sorted(parents, key=lambda n: float(data.G.nodes[n].get('node_time', 0.0)))\n",
    "\n",
    "            # compute aggregates for the original graph (may be recomputed later if we modify structure)\n",
    "            agg = model.compute_aggregates(data.G, data.hot_encoded_nucleotide_sequences, gene_presence_mask)  # [N,D]\n",
    "            # precompute scalar sums per node for DonorFinder\n",
    "            node_seq_sums = agg.sum(dim=1)  # [N]\n",
    "\n",
    "            # We'll process parents sequentially; collect losses per graph\n",
    "            graph_rec_logits = []\n",
    "            graph_rec_labels = []\n",
    "            graph_donor_penalties = []  # penalties for daughter-edge miss (0/1)\n",
    "            graph_donor_distances = []  # normalized distances\n",
    "\n",
    "            # For reproducibility of structure edits, we won't mutate original G. When needed we create a modified copy.\n",
    "            G_current = data.G  # original graph object (not mutated)\n",
    "            agg_current = agg\n",
    "            node_sums_current = node_seq_sums\n",
    "            node_seq_matrix_current = data.hot_encoded_nucleotide_sequences  # refer to original (will extend when editing)\n",
    "\n",
    "            # We'll keep an index map telling how many nodes exist currently (to append new nodes if needed)\n",
    "            current_max_node_id = max([n for n in data.G.nodes]) if len(data.G.nodes) > 0 else -1\n",
    "\n",
    "            # iterate parents in time order\n",
    "            for parent in parents_sorted:\n",
    "                # children (two)\n",
    "                children = list(G_current.successors(parent))\n",
    "                if len(children) != 2:\n",
    "                    # structure may have been altered previously; skip if not binary anymore\n",
    "                    continue\n",
    "                left, right = children[0], children[1]\n",
    "\n",
    "                # build aux features for this parent\n",
    "                node_time = float(G_current.nodes[parent].get('node_time', 0.0))\n",
    "                node_level = float(G_current.nodes[parent].get('level', 0.0))\n",
    "                parent_sum = agg_current[parent].sum().clamp(min=1.0)\n",
    "                gene_frac = ((agg_current[left].sum() + agg_current[right].sum()) / parent_sum).unsqueeze(0).unsqueeze(1)\n",
    "                aux = torch.tensor([[node_time, node_level, float(gene_frac.item())]], device=device, dtype=torch.float32)\n",
    "\n",
    "                # left/right vectors\n",
    "                left_vec = agg_current[left].unsqueeze(0).to(device)   # [1, D]\n",
    "                right_vec = agg_current[right].unsqueeze(0).to(device) # [1, D]\n",
    "\n",
    "                # Recipient forward (single sample)\n",
    "                hgt_logit = model.recipient_finder(left_vec, right_vec, aux)  # [1]\n",
    "                graph_rec_logits.append(hgt_logit.squeeze(0))\n",
    "                lbl, true_rec_child = recip_map.get(parent, (0, None))\n",
    "                graph_rec_labels.append(float(lbl))\n",
    "\n",
    "                prob = torch.sigmoid(hgt_logit).item()\n",
    "                rec_pred = 1 if prob >= hgt_threshold else 0\n",
    "\n",
    "                # only attempt donor-finding & potential structure correction if model predicts HGT\n",
    "                # and ground truth label indicates HGT (we will still compute donors to compute loss terms)\n",
    "                if rec_pred == 1:\n",
    "                    # DonorFinder decides which daughter is recipient and predicts donor_parent\n",
    "                    which_pred, donor_parent_pred, donor_score = DonorFinder.find_donor_and_which(\n",
    "                        agg_current, G_current, parent, left, right, node_seq_sums=node_sums_current\n",
    "                    )\n",
    "                    # map which_pred to predicted recipient child id\n",
    "                    pred_rec_child = children[which_pred]\n",
    "\n",
    "                    # Now compute donor-related losses/metrics based on ground truth\n",
    "                    if lbl == 1:\n",
    "                        # ground truth recipient child and donor parent\n",
    "                        true_rec_child = true_rec_child  # from recip_map\n",
    "                        true_donor_parent = donor_map.get(parent, None)  # may be None\n",
    "\n",
    "                        # 1) daughter-edge correctness penalty: 0 if predicted recipient child == true_rec_child else 1\n",
    "                        daughter_edge_penalty = 0.0\n",
    "                        if true_rec_child is not None:\n",
    "                            if pred_rec_child != true_rec_child:\n",
    "                                daughter_edge_penalty = 1.0\n",
    "                        else:\n",
    "                            # if no true_rec_child provided, we don't penalize (set penalty 0)\n",
    "                            daughter_edge_penalty = 0.0\n",
    "                        graph_donor_penalties.append(daughter_edge_penalty)\n",
    "\n",
    "                        # 2) donor distance: compute time-based distance between predicted donor_parent and true_donor_parent\n",
    "                        if (true_donor_parent is None) or (donor_parent_pred is None):\n",
    "                            normalized_dist = 1.0  # maximal penalty if missing\n",
    "                        else:\n",
    "                            # time-based distance using tree_distance but using G_current\n",
    "                            max_pen = len(G_current.nodes)\n",
    "                            raw_dist = tree_distance(G_current, donor_parent_pred, true_donor_parent, max_penalty=max_pen)\n",
    "                            # normalize by max_pen to keep in [0,1]\n",
    "                            normalized_dist = float(raw_dist) / float(max_pen) if max_pen > 0 else float(raw_dist)\n",
    "                        graph_donor_distances.append(normalized_dist)\n",
    "\n",
    "                        # Accumulate reporting\n",
    "                        donor_expected_distance_accum += normalized_dist * float(len(G_current.nodes))\n",
    "                        donor_expected_count += 1\n",
    "\n",
    "                        # If both recipient daughter and donor parent are correctly identified,\n",
    "                        # perform structure correction on a deepcopy of G (so original G remains unchanged for next epoch)\n",
    "                        if (pred_rec_child == true_rec_child) and (donor_parent_pred == true_donor_parent) and (donor_parent_pred is not None):\n",
    "                            # perform structure correction on a COPY of the graph\n",
    "                            G_mod = copy.deepcopy(G_current)\n",
    "                            # extend node_seq_matrix_current by a zero-row to represent the new internal node\n",
    "                            M_old = node_seq_matrix_current.shape[0]\n",
    "                            D = node_seq_matrix_current.shape[1]\n",
    "                            new_row = torch.zeros((1, D), dtype=node_seq_matrix_current.dtype, device=node_seq_matrix_current.device)\n",
    "                            node_seq_matrix_current = torch.cat([node_seq_matrix_current, new_row], dim=0)\n",
    "                            # new node id\n",
    "                            current_max_node_id += 1\n",
    "                            new_node = current_max_node_id\n",
    "                            # compute new_node time as per your formula:\n",
    "                            rec_child_time = float(G_mod.nodes[pred_rec_child].get('node_time', 0.0))\n",
    "                            donor_parent_time = float(G_mod.nodes[donor_parent_pred].get('node_time', 0.0))\n",
    "                            # donor child of the donor edge: pick the child of donor_parent that is on the donor edge to be split.\n",
    "                            donor_children = list(G_mod.successors(donor_parent_pred))\n",
    "                            # if donor_parent_pred had multiple children, we need to choose the child that is on the donor edge.\n",
    "                            # For safety, pick the child with smallest difference in aggregate sums to the recipient sum.\n",
    "                            if len(donor_children) == 0:\n",
    "                                # cannot split; skip structural edit\n",
    "                                pass\n",
    "                            else:\n",
    "                                # pick donor_child as the one whose subtree sum is closest to pred_rec_child subtree sum\n",
    "                                rec_sum = float(agg_current[pred_rec_child].sum().item())\n",
    "                                cand_sums = [(c, float(agg_current[c].sum().item())) for c in donor_children if c < node_seq_sums.shape[0]]\n",
    "                                if len(cand_sums) == 0:\n",
    "                                    donor_child = donor_children[0]\n",
    "                                else:\n",
    "                                    donor_child = min(cand_sums, key=lambda x: abs(x[1] - rec_sum))[0]\n",
    "\n",
    "                                donor_child_time = float(G_mod.nodes[donor_child].get('node_time', 0.0))\n",
    "\n",
    "                                t1 = rec_child_time + donor_parent_time\n",
    "                                t2 = donor_child_time + donor_parent_time\n",
    "                                new_time = 0.5 * max(t1, t2) * 0.5  # as per your \"half of maximum of sums\" (two halves)\n",
    "                                # Note: you've described \"half of the maximum of 1) sum(rec_child, donor_parent) OR 2) sum(donor_child, donor_parent)\".\n",
    "                                # Implementation sets new_time = 0.5 * max(t1, t2) * 0.5 => effectively 0.25 * max(...).\n",
    "                                # To be consistent with the verbal spec (half of the max), set:\n",
    "                                # new_time = 0.5 * max(t1, t2)\n",
    "                                # Use that instead. (I'll set to the literal half)\n",
    "                                new_time = 0.5 * max(t1, t2)\n",
    "\n",
    "                                # add new node and set attributes\n",
    "                                G_mod.add_node(new_node)\n",
    "                                G_mod.nodes[new_node]['node_time'] = new_time\n",
    "                                G_mod.nodes[new_node]['level'] = (G_mod.nodes[pred_rec_child].get('level', 0.0) + G_mod.nodes[donor_parent_pred].get('level', 0.0))/2.0\n",
    "\n",
    "                                # Rewire recipient edge: remove (recipient_parent -> pred_rec_child), insert (recipient_parent -> new_node) and (new_node -> pred_rec_child)\n",
    "                                try:\n",
    "                                    G_mod.remove_edge(parent, pred_rec_child)\n",
    "                                except Exception:\n",
    "                                    # maybe edge already changed; continue gracefully\n",
    "                                    pass\n",
    "                                G_mod.add_edge(parent, new_node)\n",
    "                                G_mod.add_edge(new_node, pred_rec_child)\n",
    "\n",
    "                                # Split donor edge: remove (donor_parent_pred -> donor_child) and add (donor_parent_pred -> new_node) and (new_node -> donor_child)\n",
    "                                try:\n",
    "                                    G_mod.remove_edge(donor_parent_pred, donor_child)\n",
    "                                except Exception:\n",
    "                                    pass\n",
    "                                G_mod.add_edge(donor_parent_pred, new_node)\n",
    "                                G_mod.add_edge(new_node, donor_child)\n",
    "\n",
    "                                # After structural change, recompute aggregates for the modified graph.\n",
    "                                agg_current = model.compute_aggregates(G_mod, node_seq_matrix_current, gene_presence_mask)\n",
    "                                node_sums_current = agg_current.sum(dim=1)\n",
    "                                # update G_current to modified version for further upward processing\n",
    "                                G_current = G_mod\n",
    "                                G_test = G_mod\n",
    "                                print(G_mod.nodes)\n",
    "                                print(G_mod.edges)\n",
    "                                # continue loop (the parent list was computed from original G; we continue on)\n",
    "                                # Note: parents_sorted may not reflect the new internal node; but per spec we continue from current place.\n",
    "                                # No extra insertion into parents_sorted needed.\n",
    "\n",
    "                    else:\n",
    "                        # ground truth label is 0 but model predicted 1: we can still compute donor penalties as neutral (no add)\n",
    "                        # append neutral penalties to keep lists aligned\n",
    "                        graph_donor_penalties.append(0.0)\n",
    "                        graph_donor_distances.append(0.0)\n",
    "                else:\n",
    "                    # model did not predict HGT at this parent; no donor penalties\n",
    "                    graph_donor_penalties.append(0.0)\n",
    "                    graph_donor_distances.append(0.0)\n",
    "\n",
    "            # Now we have per-parent lists (graph_rec_logits, graph_rec_labels, graph_donor_penalties, graph_donor_distances)\n",
    "            if len(graph_rec_logits) == 0:\n",
    "                continue\n",
    "\n",
    "            rec_logits_tensor = torch.stack(graph_rec_logits, dim=0)  # [P]\n",
    "            rec_labels_tensor = torch.tensor(graph_rec_labels, dtype=torch.float32, device=device)  # [P]\n",
    "\n",
    "            rec_loss = bce_loss_fn(rec_logits_tensor, rec_labels_tensor)\n",
    "\n",
    "            # donor losses: average over events where rec label ==1\n",
    "            donor_penalty_tensor = torch.tensor(graph_donor_penalties, dtype=torch.float32, device=device)\n",
    "            donor_dist_tensor = torch.tensor(graph_donor_distances, dtype=torch.float32, device=device)\n",
    "\n",
    "            # Only include donor penalties where ground-truth recipient label == 1\n",
    "            gt_mask = (rec_labels_tensor == 1.0).to(dtype=torch.float32)\n",
    "            if gt_mask.sum() > 0:\n",
    "                avg_daughter_penalty = (donor_penalty_tensor * gt_mask).sum() / gt_mask.sum()\n",
    "                avg_donor_distance = (donor_dist_tensor * gt_mask).sum() / gt_mask.sum()\n",
    "                donor_loss = avg_daughter_penalty + avg_donor_distance\n",
    "            else:\n",
    "                avg_daughter_penalty = torch.tensor(0.0, device=device)\n",
    "                avg_donor_distance = torch.tensor(0.0, device=device)\n",
    "                donor_loss = torch.tensor(0.0, device=device)\n",
    "\n",
    "            loss = rec_loss + lambda_donor * donor_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), clip_grad)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += float(loss.item())\n",
    "            train_rec_loss += float(rec_loss.item())\n",
    "            train_don_loss += float(donor_loss.item())\n",
    "            train_samples += 1\n",
    "\n",
    "            # update metrics for recipient detection (on original per-parent predictions)\n",
    "            probs = torch.sigmoid(rec_logits_tensor).detach().cpu().numpy().tolist()\n",
    "            for i_parent, lbl in enumerate(graph_rec_labels):\n",
    "                true_lbl = int(lbl)\n",
    "                pred_prob = float(probs[i_parent])\n",
    "                pred_class = 1 if pred_prob >= hgt_threshold else 0\n",
    "                if pred_class == 1 and true_lbl == 1:\n",
    "                    tp += 1\n",
    "                if pred_class == 1 and true_lbl == 0:\n",
    "                    fp += 1\n",
    "                if pred_class == 0 and true_lbl == 1:\n",
    "                    fn += 1\n",
    "\n",
    "        # End epoch training stats\n",
    "        avg_train_loss = train_loss / max(1, train_samples)\n",
    "        avg_train_rec = train_rec_loss / max(1, train_samples)\n",
    "        avg_train_don = train_don_loss / max(1, train_samples)\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        avg_expected_donor_distance = (donor_expected_distance_accum / donor_expected_count) if donor_expected_count > 0 else 0.0\n",
    "\n",
    "        # Validation pass (keeps previous logic but adapted to new RecipientFinder)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            val_rec_loss = 0.0\n",
    "            val_don_loss = 0.0\n",
    "            val_samples = 0\n",
    "            v_tp = v_fp = v_fn = 0\n",
    "            v_donor_expected_dist_acc = 0.0\n",
    "            v_donor_expected_cnt = 0\n",
    "\n",
    "            for idx in val_idx:\n",
    "                data = list_of_Data[idx]\n",
    "                data.hot_encoded_nucleotide_sequences = data.hot_encoded_nucleotide_sequences.to(device)\n",
    "                gene_presence_mask = getattr(data, 'gene_absence_presence_matrix', None)\n",
    "                if gene_presence_mask is not None:\n",
    "                    if not isinstance(gene_presence_mask, torch.Tensor):\n",
    "                        gene_presence_mask = torch.tensor(gene_presence_mask, dtype=torch.float32, device=device)\n",
    "                    else:\n",
    "                        gene_presence_mask = gene_presence_mask.to(device)\n",
    "\n",
    "                recip_map = data.recip_label_map\n",
    "                donor_map = data.donor_map\n",
    "                parents = [n for n in data.G.nodes if len(list(data.G.successors(n))) == 2]\n",
    "                if len(parents) == 0:\n",
    "                    continue\n",
    "                parents_sorted = sorted(parents, key=lambda n: float(data.G.nodes[n].get('node_time', 0.0)))\n",
    "                agg, node_to_idx = model.compute_aggregates(data.G, data.hot_encoded_nucleotide_sequences, gene_presence_mask)\n",
    "                node_seq_sums = agg.sum(dim=1)\n",
    "\n",
    "                graph_rec_logits = []\n",
    "                graph_rec_labels = []\n",
    "                graph_donor_penalties = []\n",
    "                graph_donor_distances = []\n",
    "\n",
    "                G_current = data.G\n",
    "                agg_current = agg\n",
    "                node_sums_current = node_sums\n",
    "                node_seq_matrix_current = data.hot_encoded_nucleotide_sequences\n",
    "                current_max_node_id = max([n for n in data.G.nodes]) if len(data.G.nodes) > 0 else -1\n",
    "\n",
    "                for parent in parents_sorted:\n",
    "                    children = list(G_current.successors(parent))\n",
    "                    if len(children) != 2:\n",
    "                        continue\n",
    "                    left, right = children[0], children[1]\n",
    "                    node_time = float(G_current.nodes[parent].get('node_time', 0.0))\n",
    "                    node_level = float(G_current.nodes[parent].get('level', 0.0))\n",
    "                    parent_sum = agg_current[parent].sum().clamp(min=1.0)\n",
    "                    gene_frac = ((agg_current[left].sum() + agg_current[right].sum()) / parent_sum).unsqueeze(0).unsqueeze(1)\n",
    "                    aux = torch.tensor([[node_time, node_level, float(gene_frac.item())]], device=device, dtype=torch.float32)\n",
    "                    left_vec = agg_current[left].unsqueeze(0).to(device)\n",
    "                    right_vec = agg_current[right].unsqueeze(0).to(device)\n",
    "\n",
    "                    hgt_logit = model.recipient_finder(left_vec, right_vec, aux)\n",
    "                    graph_rec_logits.append(hgt_logit.squeeze(0))\n",
    "                    lbl, true_rec_child = recip_map.get(parent, (0, None))\n",
    "                    graph_rec_labels.append(float(lbl))\n",
    "                    prob = torch.sigmoid(hgt_logit).item()\n",
    "                    rec_pred = 1 if prob >= hgt_threshold else 0\n",
    "\n",
    "                    if rec_pred == 1:\n",
    "                        which_pred, donor_parent_pred, donor_score = DonorFinder.find_donor_and_which(\n",
    "                            agg_current, G_current, parent, left, right, node_seq_sums=node_sums_current\n",
    "                        )\n",
    "                        pred_rec_child = children[which_pred]\n",
    "                        if lbl == 1:\n",
    "                            true_rec_child = true_rec_child\n",
    "                            true_donor_parent = donor_map.get(parent, None)\n",
    "                            daughter_edge_penalty = 0.0\n",
    "                            if true_rec_child is not None:\n",
    "                                if pred_rec_child != true_rec_child:\n",
    "                                    daughter_edge_penalty = 1.0\n",
    "                            graph_donor_penalties.append(daughter_edge_penalty)\n",
    "                            if (true_donor_parent is None) or (donor_parent_pred is None):\n",
    "                                normalized_dist = 1.0\n",
    "                            else:\n",
    "                                max_pen = len(G_current.nodes)\n",
    "                                raw_dist = tree_distance(G_current, donor_parent_pred, true_donor_parent, max_penalty=max_pen)\n",
    "                                normalized_dist = float(raw_dist) / float(max_pen) if max_pen > 0 else float(raw_dist)\n",
    "                            graph_donor_distances.append(normalized_dist)\n",
    "                            v_donor_expected_dist_acc += normalized_dist * float(len(G_current.nodes))\n",
    "                            v_donor_expected_cnt += 1\n",
    "                            if (pred_rec_child == true_rec_child) and (donor_parent_pred == true_donor_parent) and (donor_parent_pred is not None):\n",
    "                                # apply same structural edit on a deepcopy (kept local to validation)\n",
    "                                G_mod = copy.deepcopy(G_current)\n",
    "                                M_old = node_seq_matrix_current.shape[0]\n",
    "                                D = node_seq_matrix_current.shape[1]\n",
    "                                new_row = torch.zeros((1, D), dtype=node_seq_matrix_current.dtype, device=node_seq_matrix_current.device)\n",
    "                                node_seq_matrix_current = torch.cat([node_seq_matrix_current, new_row], dim=0)\n",
    "                                current_max_node_id += 1\n",
    "                                new_node = current_max_node_id\n",
    "                                rec_child_time = float(G_mod.nodes[pred_rec_child].get('node_time', 0.0))\n",
    "                                donor_parent_time = float(G_mod.nodes[donor_parent_pred].get('node_time', 0.0))\n",
    "                                donor_children = list(G_mod.successors(donor_parent_pred))\n",
    "                                if len(donor_children) == 0:\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    rec_sum = float(agg_current[pred_rec_child].sum().item())\n",
    "                                    cand_sums = [(c, float(agg_current[c].sum().item())) for c in donor_children if c < node_sums.shape[0]]\n",
    "                                    if len(cand_sums) == 0:\n",
    "                                        donor_child = donor_children[0]\n",
    "                                    else:\n",
    "                                        donor_child = min(cand_sums, key=lambda x: abs(x[1] - rec_sum))[0]\n",
    "                                    donor_child_time = float(G_mod.nodes[donor_child].get('node_time', 0.0))\n",
    "                                    t1 = rec_child_time + donor_parent_time\n",
    "                                    t2 = donor_child_time + donor_parent_time\n",
    "                                    new_time = 0.5 * max(t1, t2)\n",
    "                                    G_mod.add_node(new_node)\n",
    "                                    G_mod.nodes[new_node]['node_time'] = new_time\n",
    "                                    G_mod.nodes[new_node]['level'] = (G_mod.nodes[pred_rec_child].get('level', 0.0) + G_mod.nodes[donor_parent_pred].get('level', 0.0))/2.0\n",
    "                                    try:\n",
    "                                        G_mod.remove_edge(parent, pred_rec_child)\n",
    "                                    except Exception:\n",
    "                                        pass\n",
    "                                    G_mod.add_edge(parent, new_node)\n",
    "                                    G_mod.add_edge(new_node, pred_rec_child)\n",
    "                                    try:\n",
    "                                        G_mod.remove_edge(donor_parent_pred, donor_child)\n",
    "                                    except Exception:\n",
    "                                        pass\n",
    "                                    G_mod.add_edge(donor_parent_pred, new_node)\n",
    "                                    G_mod.add_edge(new_node, donor_child)\n",
    "                                    agg_current = model.compute_aggregates(G_mod, node_seq_matrix_current, gene_presence_mask)\n",
    "                                    node_sums_current = agg_current.sum(dim=1)\n",
    "                                    G_current = G_mod\n",
    "                        else:\n",
    "                            graph_donor_penalties.append(0.0)\n",
    "                            graph_donor_distances.append(0.0)\n",
    "                    else:\n",
    "                        graph_donor_penalties.append(0.0)\n",
    "                        graph_donor_distances.append(0.0)\n",
    "\n",
    "                if len(graph_rec_logits) == 0:\n",
    "                    continue\n",
    "\n",
    "                rec_logits_tensor = torch.stack(graph_rec_logits, dim=0)\n",
    "                rec_labels_tensor = torch.tensor(graph_rec_labels, dtype=torch.float32, device=device)\n",
    "                rec_loss = bce_loss_fn(rec_logits_tensor, rec_labels_tensor)\n",
    "\n",
    "                donor_penalty_tensor = torch.tensor(graph_donor_penalties, dtype=torch.float32, device=device)\n",
    "                donor_dist_tensor = torch.tensor(graph_donor_distances, dtype=torch.float32, device=device)\n",
    "\n",
    "                gt_mask = (rec_labels_tensor == 1.0).to(dtype=torch.float32)\n",
    "                if gt_mask.sum() > 0:\n",
    "                    avg_daughter_penalty = (donor_penalty_tensor * gt_mask).sum() / gt_mask.sum()\n",
    "                    avg_donor_distance = (donor_dist_tensor * gt_mask).sum() / gt_mask.sum()\n",
    "                    donor_loss = avg_daughter_penalty + avg_donor_distance\n",
    "                else:\n",
    "                    avg_daughter_penalty = 0.0\n",
    "                    avg_donor_distance = 0.0\n",
    "                    donor_loss = 0.0\n",
    "\n",
    "                loss = float(rec_loss.item()) + lambda_donor * float(donor_loss)\n",
    "\n",
    "                val_loss += loss\n",
    "                val_rec_loss += float(rec_loss.item())\n",
    "                val_don_loss += float(donor_loss)\n",
    "                val_samples += 1\n",
    "\n",
    "                probs = torch.sigmoid(rec_logits_tensor).cpu().numpy().tolist()\n",
    "                for i_parent, lbl in enumerate(graph_rec_labels):\n",
    "                    true_lbl = int(lbl)\n",
    "                    pred_prob = float(probs[i_parent])\n",
    "                    pred_class = 1 if pred_prob >= hgt_threshold else 0\n",
    "                    if pred_class == 1 and true_lbl == 1:\n",
    "                        v_tp += 1\n",
    "                    if pred_class == 1 and true_lbl == 0:\n",
    "                        v_fp += 1\n",
    "                    if pred_class == 0 and true_lbl == 1:\n",
    "                        v_fn += 1\n",
    "\n",
    "            avg_val_loss = val_loss / max(1, val_samples)\n",
    "            avg_val_rec = val_rec_loss / max(1, val_samples)\n",
    "            avg_val_don = val_don_loss / max(1, val_samples)\n",
    "            v_precision = v_tp / (v_tp + v_fp) if (v_tp + v_fp) > 0 else 0.0\n",
    "            v_recall = v_tp / (v_tp + v_fn) if (v_tp + v_fn) > 0 else 0.0\n",
    "            v_f1 = 2 * v_precision * v_recall / (v_precision + v_recall) if (v_precision + v_recall) > 0 else 0.0\n",
    "            avg_v_donor_expected = (v_donor_expected_dist_acc / v_donor_expected_cnt) if v_donor_expected_cnt > 0 else 0.0\n",
    "\n",
    "        # scheduler step (use validation loss)\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        t1 = time.time()\n",
    "        print(f\"Epoch {epoch}/{epochs} | time {t1-t0:.1f}s\")\n",
    "        print(f\" TRAIN loss {avg_train_loss:.6f} (rec {avg_train_rec:.6f}, don {avg_train_don:.6f}) \"\n",
    "              f\"Prec/Rec/F1 {precision:.3f}/{recall:.3f}/{f1:.3f} | avg_expected_donor_dist {avg_expected_donor_distance:.3f}\")\n",
    "        print(f\" VAL   loss {avg_val_loss:.6f} (rec {avg_val_rec:.6f}, don {avg_val_don:.6f}) \"\n",
    "              f\"Prec/Rec/F1 {v_precision:.3f}/{v_recall:.3f}/{v_f1:.3f} | avg_expected_donor_dist {avg_v_donor_expected:.3f}\")\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Example usage snippet (unchanged seeds + instantiation)\n",
    "# -------------------------\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# NOTE: the following assumes you have variable `list_of_Data` already defined,\n",
    "# and each data object has attributes:\n",
    "#   - G: networkx.DiGraph\n",
    "#   - hot_encoded_nucleotide_sequences: torch.Tensor [M_orig, D]\n",
    "#   - recip_label_map: dict parent -> (label:int, true_rec_child:int or None)\n",
    "#   - donor_map: dict parent -> true_donor_parent (or None)\n",
    "#   - optional: gene_absence_presence_matrix: 1D mask aligned with hot_encoded_nucleotide_sequences rows\n",
    "#\n",
    "# Please ensure node ids in G for the original (unmodified) tree are integers 0..M_orig-1\n",
    "# and hot_encoded_nucleotide_sequences rows are ordered accordingly. If not, adapt mapping.\n",
    "\n",
    "# choose a sample data to get seq_vec_dim\n",
    "sample_data = random.choice(list_of_Data)\n",
    "seq_vec_dim = sample_data.hot_encoded_nucleotide_sequences.shape[1]\n",
    "model = HGTDetector(seq_vec_dim=seq_vec_dim, hgt_threshold=0.5, topk_donors=1)\n",
    "\n",
    "trained_model = train_hgt_detector(model, list_of_Data,\n",
    "                                  epochs=4,\n",
    "                                  val_frac=0.1,\n",
    "                                  lr=1e-3,\n",
    "                                  lambda_donor=0.1,\n",
    "                                  hgt_threshold=0.5,\n",
    "                                  max_candidates=300,\n",
    "                                  device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "                                  clip_grad=5.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07269f3f-4a37-4d4c-b0bb-31e583216cd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G_mod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mG_mod\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'G_mod' is not defined"
     ]
    }
   ],
   "source": [
    "G_mod"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
