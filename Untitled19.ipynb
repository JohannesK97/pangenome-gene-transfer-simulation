{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "8b6cbb7a-53d6-4b37-a7b4-fd7c2b536f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'-------------------------------------------------------------------------------------------------------------------------------------'\n",
      " b'CCGC---C-CAGC-G-CGG-CAATTC-ACGGGTG-AG-CCT-C-GGATT-GTTG-CGGTTTGCG-CATTAATAAGC--TC--CGATGGTTGTG--AAT--GACCATG-TACTCC-CTCTGCC-GTCTGACTAT'\n",
      " b'CCGACT---TCGACGC--GAGAACTCTACG-ACGTGGACTA-C-G-AGTACTTA-GGATTTGCC-CCTC-A-AAAC--TCC-CGTT-CGTGACC-AATG--TGGC-C-TAAT-CT--CTGCCTGTCTGATCTA'\n",
      " b'CACC----GCA-CGGCC-CAGAACGATTCCGACC-AA-GTTACCGGGGGAG-TG-GGAATTGCGATCCC-ATAAACATCC-GACACGCGTGTGCAATTG-GTCCTTC-TTCTCC-C-CGGAC-AACTACTCTT'\n",
      " b'CACC-----CA-CGGCC-CAGAACGATACGGACC-AA-GTT---AGGGGAG-TGGGTAATTGCGATCCC-ATAAACATCC-GCCACGCGCATGCATTCG-GTCCTTCA-TCCCA-C-CCAAT-AAGTGCTCTT']\n",
      "tensor([[-1., -1., -1.,  ..., -1., -1., -1.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1157/1735096251.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edges = torch.tensor(graph_properties[1], dtype=torch.long)  # [2, num_edges]\n",
      "/tmp/ipykernel_1157/1735096251.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coords = torch.tensor(graph_properties[2].T)             # [2, num_nodes]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import h5py\n",
    "import pickle\n",
    "import torch\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "\n",
    "def one_hot_encode(sequences, gene_present, gene_length, alphabet=['A','C','T','G','-']):\n",
    "    \"\"\"\n",
    "    sequences: List of strings (DNA sequences)\n",
    "    gene_present: np.array(bool) oder Torch Tensor, gleiche Länge wie sequences\n",
    "    gene_length: int, fixe Länge für das Hot-Encoding\n",
    "    alphabet: list, Zeichenalphabet\n",
    "    \"\"\"\n",
    "    num_samples = len(sequences)\n",
    "    num_chars = len(alphabet)\n",
    "    char_to_idx = {c:i for i,c in enumerate(alphabet)}\n",
    "    sequences_str = [s.decode('utf-8') for s in sequences]\n",
    "    gene_present = np.array(gene_present, dtype=bool)\n",
    "    \n",
    "    # 1️⃣ Leere Batch-Matrix vorbereiten: (num_samples, gene_length, num_chars)\n",
    "    batch = np.zeros((num_samples, gene_length, num_chars), dtype=np.float32)\n",
    "    \n",
    "    # 2️⃣ Hot-Encode alle Sequenzen\n",
    "    for i, seq in enumerate(sequences_str):\n",
    "        if gene_present[i]:\n",
    "            L = min(len(seq), gene_length)  # abschneiden\n",
    "            for j, c in enumerate(seq[:L]):\n",
    "                if c in char_to_idx:\n",
    "                    batch[i, j, char_to_idx[c]] = 1.0\n",
    "        elif gene_present[i] == 0:\n",
    "            batch[i, :, :] = -1.0\n",
    "            \n",
    "    # 3️⃣ Zufällige, aber konsistente Spaltenpermutation\n",
    "    perm = np.random.permutation(gene_length)\n",
    "    batch = batch[:, perm, :]\n",
    "    \n",
    "    # 4️⃣ Optional: Flatten zu Vektor (num_samples, gene_length*num_chars)\n",
    "    batch_flat = batch.reshape(num_samples, -1)\n",
    "    \n",
    "    return torch.tensor(batch_flat)  # shape: (num_samples, gene_length*num_chars)\n",
    "\n",
    "def load_file(file):\n",
    "\n",
    "    gene_length = 300\n",
    "    #nucleotide_mutation_rate = 0.1\n",
    "    \n",
    "    with h5py.File(file, \"r\") as f:\n",
    "            grp = f[\"results\"]\n",
    "            # Load graph_properties (pickle stored in dataset)\n",
    "            graph_properties = pickle.loads(grp[\"graph_properties\"][()])\n",
    "    \n",
    "            # Unpack graph properties\n",
    "            nodes = torch.tensor(graph_properties[0])                # [num_nodes]\n",
    "            edges = torch.tensor(graph_properties[1], dtype=torch.long)  # [2, num_edges]\n",
    "            coords = torch.tensor(graph_properties[2].T)             # [2, num_nodes]\n",
    "    \n",
    "            # Load datasets instead of attrs\n",
    "            gene_absence_presence_matrix = grp[\"gene_absence_presence_matrix\"][()]\n",
    "            nucleotide_sequences = grp[\"nucleotide_sequences\"][()]\n",
    "            #children_gene_nodes_loss_events = grp[\"children_gene_nodes_loss_events\"][()]\n",
    "        \n",
    "            # Load HGT events (simplified)\n",
    "            hgt_events = {}\n",
    "            hgt_grp_simpl = grp[\"nodes_hgt_events_simplified\"]\n",
    "            for site_id in hgt_grp_simpl.keys():\n",
    "                hgt_events[int(site_id)] = hgt_grp_simpl[site_id][()]\n",
    "\n",
    "            hot_encoded_nucleotide_sequences = one_hot_encode(nucleotide_sequences, gene_absence_presence_matrix, gene_length)\n",
    "\n",
    "            # Fill the remaining nodes with zeros.\n",
    "            pad_rows = len(nodes) - len(nucleotide_sequences)\n",
    "            pad = torch.zeros((pad_rows, hot_encoded_nucleotide_sequences.shape[1]), dtype=hot_encoded_nucleotide_sequences.dtype)\n",
    "            hot_encoded_nucleotide_sequences = torch.cat([hot_encoded_nucleotide_sequences, pad], dim=0)\n",
    "        \n",
    "            G = nx.DiGraph()\n",
    "            \n",
    "            # Füge Knoten hinzu (optional mit Koordinaten als Attribut)\n",
    "            for i, node_id in enumerate(nodes.tolist()):\n",
    "                G.add_node(node_id, node_time = coords[:, i].tolist()[5])\n",
    "            \n",
    "            # Füge Kanten hinzu\n",
    "            edge_list = edges.tolist()\n",
    "            for src, dst in zip(edge_list[0], edge_list[1]):\n",
    "                G.add_edge(src, dst)\n",
    "            \n",
    "            # Collect all recipient_parent_nodes from all sites\n",
    "            recipient_parent_nodes = set()\n",
    "            for site_id in hgt_grp_simpl.keys():\n",
    "                arr = hgt_grp_simpl[site_id][()]  # load dataset as numpy structured array\n",
    "                recipient_parent_nodes.update(arr[\"recipient_child_node\"].tolist())\n",
    "            \n",
    "            # Build theta_gains: 1 if node is in recipient_parent_nodes, else 0\n",
    "            theta_gains = torch.tensor(\n",
    "                [1 if node in recipient_parent_nodes else 0 for node in range(len(G.nodes))],\n",
    "                dtype=torch.long\n",
    "            )\n",
    "\n",
    "            level = {n: 0 for n in G.nodes}  # Leaves haben Level 0\n",
    "            \n",
    "            # 3. Topologische Sortierung (damit Kinder vor Eltern behandelt werden)\n",
    "            for node in reversed(list(nx.topological_sort(G))):\n",
    "                successors = list(G.successors(node))\n",
    "                if successors:\n",
    "                    level[node] = 1 + max(level[s] for s in successors)\n",
    "            \n",
    "            # 4. Level als Attribut setzen\n",
    "            nx.set_node_attributes(G, level, \"level\")\n",
    "\n",
    "            ### Add candidate egdes, i.e. potential hgt edges:\n",
    "        \n",
    "            # Hole alle Knoten und ihre Zeiten\n",
    "            node_times = {n: G.nodes[n]['node_time'] for n in G.nodes}\n",
    "            sorted_nodes = sorted(node_times.keys(), key=lambda n: node_times[n])\n",
    "            \n",
    "            # Füge Kanten hinzu\n",
    "            existing_edges = set(zip(edges[0].tolist(), edges[1].tolist()))\n",
    "            candidate_edges = []\n",
    "            for i, src in enumerate(sorted_nodes):\n",
    "                t_src = node_times[src]\n",
    "                for dst in sorted_nodes[i+1:]:  # nur spätere Knoten\n",
    "                    t_dst = node_times[dst]\n",
    "                    if t_dst > t_src:\n",
    "                        if (dst, src) not in existing_edges:  # vermeidet doppelte\n",
    "                            #G.add_edge(src, dst)\n",
    "                            candidate_edges.append((dst, src))\n",
    "            if candidate_edges:  \n",
    "                candidate_edges = torch.tensor(candidate_edges, dtype=torch.long).t()\n",
    "            else:\n",
    "                candidate_edges = torch.empty((2,0), dtype=torch.long)\n",
    "                    \n",
    "            data = Data(\n",
    "                nucleotide_sequences = nucleotide_sequences,\n",
    "                hot_encoded_nucleotide_sequences = hot_encoded_nucleotide_sequences,       # Node Features [num_nodes, 2]\n",
    "                edge_index = edges[[1, 0], :],        # Edge Index [2, num_edges]\n",
    "                candidate_edges = candidate_edges[[1, 0], :],\n",
    "                y = theta_gains,            # Labels [num_nodes]\n",
    "                file = file,\n",
    "                G = G,\n",
    "                recipient_parent_nodes = recipient_parent_nodes,\n",
    "                gene_absence_presence_matrix = gene_absence_presence_matrix\n",
    "            )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Pfad zum Windows-Ordner (aus Linux/WSL-Sicht)\n",
    "folder = \"/mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks\"\n",
    "\n",
    "# Alle Dateien im Ordner (nur reguläre Dateien, keine Unterordner)\n",
    "files = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "\n",
    "file = random.choice(files)\n",
    "data = load_file(os.path.join(folder, file))\n",
    "\n",
    "\n",
    "print(data.nucleotide_sequences)\n",
    "print(data.hot_encoded_nucleotide_sequences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "edceebc1-0945-450a-86b4-3ba7865fa9cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1 if node in {4, 6} else 0 for node in range(10)],\n",
    "                dtype=torch.long\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "de63e9be-6c41-4c84-adad-5283143321bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7, 3, 3, 7, 5)]\n",
      "[3]\n",
      "[0, 1, 2, 3, 4, 8, 7, 6, 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1157/550435347.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edges = torch.tensor(graph_properties[1], dtype=torch.long)  # [2, num_edges]\n",
      "/tmp/ipykernel_1157/550435347.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coords = torch.tensor(graph_properties[2].T)             # [2, num_nodes]\n"
     ]
    }
   ],
   "source": [
    "file = random.choice(files)\n",
    "data = load_file(os.path.join(folder, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "9d25915a-93d6-4316-b39f-9d26dedf792b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1157/1735096251.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  edges = torch.tensor(graph_properties[1], dtype=torch.long)  # [2, num_edges]\n",
      "/tmp/ipykernel_1157/1735096251.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  coords = torch.tensor(graph_properties[2].T)             # [2, num_nodes]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000 | Avg Loss: 1.1823\n",
      "Epoch 001 | Avg Loss: 1.1815\n",
      "Epoch 002 | Avg Loss: 1.1807\n",
      "Epoch 003 | Avg Loss: 1.1800\n",
      "Epoch 004 | Avg Loss: 1.1792\n",
      "Epoch 005 | Avg Loss: 1.1784\n",
      "Epoch 006 | Avg Loss: 1.1776\n",
      "Epoch 007 | Avg Loss: 1.1768\n",
      "Epoch 008 | Avg Loss: 1.1760\n",
      "Epoch 009 | Avg Loss: 1.1752\n",
      "Epoch 010 | Avg Loss: 1.1744\n",
      "Epoch 011 | Avg Loss: 1.1736\n",
      "Epoch 012 | Avg Loss: 1.1728\n",
      "Epoch 013 | Avg Loss: 1.1720\n",
      "Epoch 014 | Avg Loss: 1.1712\n",
      "Epoch 015 | Avg Loss: 1.1704\n",
      "Epoch 016 | Avg Loss: 1.1696\n",
      "Epoch 017 | Avg Loss: 1.1688\n",
      "Epoch 018 | Avg Loss: 1.1680\n",
      "Epoch 019 | Avg Loss: 1.1672\n",
      "Epoch 020 | Avg Loss: 1.1664\n",
      "Epoch 021 | Avg Loss: 1.1656\n",
      "Epoch 022 | Avg Loss: 1.1648\n",
      "Epoch 023 | Avg Loss: 1.1641\n",
      "Epoch 024 | Avg Loss: 1.1633\n",
      "Epoch 025 | Avg Loss: 1.1625\n",
      "Epoch 026 | Avg Loss: 1.1617\n",
      "Epoch 027 | Avg Loss: 1.1609\n",
      "Epoch 028 | Avg Loss: 1.1601\n",
      "Epoch 029 | Avg Loss: 1.1593\n",
      "Epoch 030 | Avg Loss: 1.1585\n",
      "Epoch 031 | Avg Loss: 1.1577\n",
      "Epoch 032 | Avg Loss: 1.1569\n",
      "Epoch 033 | Avg Loss: 1.1562\n",
      "Epoch 034 | Avg Loss: 1.1554\n",
      "Epoch 035 | Avg Loss: 1.1546\n",
      "Epoch 036 | Avg Loss: 1.1538\n",
      "Epoch 037 | Avg Loss: 1.1530\n",
      "Epoch 038 | Avg Loss: 1.1522\n",
      "Epoch 039 | Avg Loss: 1.1514\n",
      "Epoch 040 | Avg Loss: 1.1506\n",
      "Epoch 041 | Avg Loss: 1.1499\n",
      "Epoch 042 | Avg Loss: 1.1491\n",
      "Epoch 043 | Avg Loss: 1.1483\n",
      "Epoch 044 | Avg Loss: 1.1475\n",
      "Epoch 045 | Avg Loss: 1.1467\n",
      "Epoch 046 | Avg Loss: 1.1459\n",
      "Epoch 047 | Avg Loss: 1.1451\n",
      "Epoch 048 | Avg Loss: 1.1444\n",
      "Epoch 049 | Avg Loss: 1.1436\n",
      "Epoch 050 | Avg Loss: 1.1428\n",
      "Epoch 051 | Avg Loss: 1.1420\n",
      "Epoch 052 | Avg Loss: 1.1412\n",
      "Epoch 053 | Avg Loss: 1.1404\n",
      "Epoch 054 | Avg Loss: 1.1397\n",
      "Epoch 055 | Avg Loss: 1.1389\n",
      "Epoch 056 | Avg Loss: 1.1381\n",
      "Epoch 057 | Avg Loss: 1.1373\n",
      "Epoch 058 | Avg Loss: 1.1365\n",
      "Epoch 059 | Avg Loss: 1.1358\n",
      "Epoch 060 | Avg Loss: 1.1350\n",
      "Epoch 061 | Avg Loss: 1.1342\n",
      "Epoch 062 | Avg Loss: 1.1334\n",
      "Epoch 063 | Avg Loss: 1.1326\n",
      "Epoch 064 | Avg Loss: 1.1319\n",
      "Epoch 065 | Avg Loss: 1.1311\n",
      "Epoch 066 | Avg Loss: 1.1303\n",
      "Epoch 067 | Avg Loss: 1.1295\n",
      "Epoch 068 | Avg Loss: 1.1287\n",
      "Epoch 069 | Avg Loss: 1.1280\n",
      "Epoch 070 | Avg Loss: 1.1272\n",
      "Epoch 071 | Avg Loss: 1.1264\n",
      "Epoch 072 | Avg Loss: 1.1256\n",
      "Epoch 073 | Avg Loss: 1.1249\n",
      "Epoch 074 | Avg Loss: 1.1241\n",
      "Epoch 075 | Avg Loss: 1.1233\n",
      "Epoch 076 | Avg Loss: 1.1225\n",
      "Epoch 077 | Avg Loss: 1.1218\n",
      "Epoch 078 | Avg Loss: 1.1210\n",
      "Epoch 079 | Avg Loss: 1.1202\n",
      "Epoch 080 | Avg Loss: 1.1194\n",
      "Epoch 081 | Avg Loss: 1.1187\n",
      "Epoch 082 | Avg Loss: 1.1179\n",
      "Epoch 083 | Avg Loss: 1.1171\n",
      "Epoch 084 | Avg Loss: 1.1163\n",
      "Epoch 085 | Avg Loss: 1.1156\n",
      "Epoch 086 | Avg Loss: 1.1148\n",
      "Epoch 087 | Avg Loss: 1.1140\n",
      "Epoch 088 | Avg Loss: 1.1133\n",
      "Epoch 089 | Avg Loss: 1.1125\n",
      "Epoch 090 | Avg Loss: 1.1117\n",
      "Epoch 091 | Avg Loss: 1.1109\n",
      "Epoch 092 | Avg Loss: 1.1102\n",
      "Epoch 093 | Avg Loss: 1.1094\n",
      "Epoch 094 | Avg Loss: 1.1086\n",
      "Epoch 095 | Avg Loss: 1.1079\n",
      "Epoch 096 | Avg Loss: 1.1071\n",
      "Epoch 097 | Avg Loss: 1.1063\n",
      "Epoch 098 | Avg Loss: 1.1056\n",
      "Epoch 099 | Avg Loss: 1.1048\n",
      "File simulation_10.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_12.h5 | Learned candidate edge probabilities (top 20): [0.05730538 0.05730538 0.05730538 0.05730538 0.05730538 0.05730538\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_13.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_14.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_18.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_19.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_2.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_20.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_22.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_23.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_24.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_25.h5 | Learned candidate edge probabilities (top 20): [0.05730541 0.05730541 0.05730541 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_26.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.05730541 0.05730541 0.03940838]\n",
      "File simulation_29.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_3.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_31.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_35.h5 | Learned candidate edge probabilities (top 20): [0.05730541 0.05730541 0.05730541 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_37.h5 | Learned candidate edge probabilities (top 20): [0.05730541 0.05730541 0.05730541 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_4.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_42.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_43.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_45.h5 | Learned candidate edge probabilities (top 20): [0.05730538 0.05730538 0.05730538 0.05730538 0.05730538 0.05730538\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_48.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_50.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_51.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_52.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.05730541 0.05730541 0.05730541 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_53.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_54.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.05730538 0.05730538 0.05730538 0.03940838 0.03940838 0.05730538]\n",
      "File simulation_55.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.05730541 0.05730541 0.05730541\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_56.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_57.h5 | Learned candidate edge probabilities (top 20): [0.05730537 0.05730537 0.05730537 0.05730537 0.05730537 0.05730537\n",
      " 0.05730537 0.05730537 0.05730537 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_58.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_59.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_6.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_60.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.05730541 0.05730541 0.05730541\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_62.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_65.h5 | Learned candidate edge probabilities (top 20): [0.05730538 0.05730538 0.05730538 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.05730538]\n",
      "File simulation_68.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_72.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.05730541 0.05730541 0.05730541\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_77.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_78.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.05730537 0.05730537 0.05730537\n",
      " 0.03940838 0.03940838 0.03940838 0.05730537 0.05730537 0.05730537\n",
      " 0.05730537 0.05730537 0.05730537 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_79.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_80.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.05730541 0.05730541 0.05730541 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_81.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_82.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_84.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_86.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_87.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_90.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.05730537 0.05730537 0.05730537\n",
      " 0.05730537 0.05730537 0.05730537 0.03940838 0.03940838 0.03940838\n",
      " 0.05730537 0.05730537 0.05730537 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_93.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_94.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_96.h5 | Learned candidate edge probabilities (top 20): [0.05730541 0.05730541 0.05730541 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n",
      "File simulation_99.h5 | Learned candidate edge probabilities (top 20): [0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838\n",
      " 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838 0.03940838]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import softmax\n",
    "import math\n",
    "\n",
    "# ======================\n",
    "\n",
    "# 1. DAG Positional Encoding\n",
    "\n",
    "# ======================\n",
    "\n",
    "class DAGPositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding based on node level (depth).\"\"\"\n",
    "    def __init__(self, d_model, max_depth=200):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_depth = max_depth\n",
    "        pe = torch.zeros(max_depth, d_model)\n",
    "        position = torch.arange(0, max_depth, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, node_levels):\n",
    "        \"\"\"Return positional encodings for node levels (shape: [num_nodes, d_model]).\"\"\"\n",
    "        node_levels = node_levels.clamp(0, self.max_depth - 1)\n",
    "        return self.pe[node_levels]\n",
    "\n",
    "\n",
    "# ======================\n",
    "\n",
    "# 2. Graph Attention Layer with learnable edge weights\n",
    "\n",
    "# ======================\n",
    "\n",
    "class WeightedGraphAttention(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels, heads=1):\n",
    "        super().__init__(aggr='add')\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.heads = heads\n",
    "\n",
    "        self.W = nn.Linear(in_channels, heads * out_channels, bias=False)\n",
    "        self.attn_src = nn.Parameter(torch.Tensor(heads * out_channels))\n",
    "        self.attn_dst = nn.Parameter(torch.Tensor(heads * out_channels))\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.W.weight)\n",
    "        nn.init.xavier_uniform_(self.attn_src.unsqueeze(0))\n",
    "        nn.init.xavier_uniform_(self.attn_dst.unsqueeze(0))\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_probs):\n",
    "        # x: [N, in_channels]\n",
    "        x = self.W(x)  # [N, H*C]\n",
    "        return self.propagate(edge_index, x=x, edge_probs=edge_probs)\n",
    "\n",
    "    def message(self, x_j, x_i, edge_probs, index, ptr, size_i):\n",
    "        # Compute attention scores\n",
    "        alpha = (x_i * self.attn_dst + x_j * self.attn_src).sum(dim=-1)\n",
    "        alpha = alpha / math.sqrt(self.out_channels)\n",
    "        \n",
    "        # Add edge bias (log-prob)\n",
    "        alpha = alpha + torch.log(edge_probs + 1e-8)\n",
    "        alpha = F.leaky_relu(alpha, 0.2)\n",
    "        alpha = softmax(alpha, index, ptr, num_nodes=size_i)\n",
    "        \n",
    "        # Scale message directly by edge probability (stronger influence)\n",
    "        msg = x_j * alpha.unsqueeze(-1) * edge_probs.unsqueeze(-1)\n",
    "        return msg\n",
    "\n",
    "\n",
    "\n",
    "# ======================\n",
    "\n",
    "# 3. DAG Transformer Network\n",
    "\n",
    "# ======================\n",
    "\n",
    "class DAGTransformer(nn.Module):\n",
    "    \"\"\"Full DAG-based network with learnable edge weights for candidate HGTs.\"\"\"\n",
    "    def __init__(self, in_dim, hidden_dim=128, out_dim=1, heads=4, num_layers=2, max_depth=200):\n",
    "        super().__init__()\n",
    "        self.pos_enc = DAGPositionalEncoding(hidden_dim, max_depth)\n",
    "        self.input_proj = nn.Linear(in_dim, hidden_dim)\n",
    "        self.heads = heads\n",
    "        self.out_channels = hidden_dim // heads\n",
    "        self.layers = nn.ModuleList([\n",
    "            WeightedGraphAttention(hidden_dim, self.out_channels, heads=heads)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(hidden_dim, out_dim)\n",
    "    \n",
    "    def forward(self, data, edge_logits):\n",
    "        x = data.hot_encoded_nucleotide_sequences\n",
    "        base_edges = data.edge_index\n",
    "        candidate_edges = data.candidate_edges\n",
    "        G = data.G\n",
    "\n",
    "        # Combine base edges and candidate edges\n",
    "        if candidate_edges.numel() > 0:\n",
    "            combined_edges = torch.cat([base_edges, candidate_edges], dim=1)\n",
    "        else:\n",
    "            combined_edges = base_edges\n",
    "\n",
    "        # Convert logits to probabilities\n",
    "        if candidate_edges.numel() > 0:\n",
    "            edge_probs_candidates = torch.sigmoid(edge_logits)\n",
    "            edge_probs = torch.cat([\n",
    "                torch.ones(base_edges.shape[1], device=x.device),\n",
    "                edge_probs_candidates\n",
    "            ])\n",
    "        else:\n",
    "            edge_probs = torch.ones(base_edges.shape[1], device=x.device)\n",
    "\n",
    "        # Project input features\n",
    "        x = self.input_proj(x)\n",
    "\n",
    "        # Add positional encoding\n",
    "        node_levels = torch.tensor([G.nodes[n]['level'] for n in G.nodes], dtype=torch.long, device=x.device)\n",
    "        x = x + self.pos_enc(node_levels)\n",
    "\n",
    "        # Multi-layer attention propagation\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, combined_edges, edge_probs)\n",
    "            x = F.relu(x)\n",
    "\n",
    "        # Output prediction per node\n",
    "        out = self.fc_out(x).squeeze(-1)\n",
    "        return out, edge_probs\n",
    "    \n",
    "\n",
    "# ======================\n",
    "\n",
    "# 4. Training Example\n",
    "\n",
    "# ======================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 1️⃣ Modell nur einmal definieren\n",
    "in_dim = None\n",
    "for file in files:\n",
    "    data = load_file(os.path.join(folder, file))\n",
    "    in_dim = data.hot_encoded_nucleotide_sequences.shape[1]\n",
    "    break\n",
    "\n",
    "model = DAGTransformer(\n",
    "    in_dim=in_dim,\n",
    "    hidden_dim=128,\n",
    "    out_dim=1,\n",
    "    heads=4,\n",
    "    num_layers=2,\n",
    "    max_depth=200\n",
    ").to(device)\n",
    "\n",
    "# 2️⃣ Candidate-Edge-Logits global für alle Bäume persistent speichern\n",
    "global_edge_logits = {}\n",
    "for file in files:\n",
    "    path = os.path.join(folder, file)\n",
    "    data = load_file(path)\n",
    "    if data.candidate_edges.numel() > 0:\n",
    "        global_edge_logits[file] = nn.Parameter(\n",
    "            torch.full((data.candidate_edges.shape[1],), -3.0, device=device)\n",
    "        )\n",
    "    else:\n",
    "        global_edge_logits[file] = None\n",
    "\n",
    "# 3️⃣ Optimizer für Modell + alle Edge-Logits\n",
    "params = list(model.parameters()) + [p for p in global_edge_logits.values() if p is not None]\n",
    "optimizer = torch.optim.Adam(params, lr=2e-3, weight_decay=1e-5)\n",
    "\n",
    "# 4️⃣ Training auf allen Bäumen\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for file in files:\n",
    "        path = os.path.join(folder, file)\n",
    "        data = load_file(path)\n",
    "        data = data.to(device)\n",
    "\n",
    "        edge_logits = global_edge_logits[file]\n",
    "\n",
    "        y_true = data.y.float().to(device)\n",
    "\n",
    "        num_pos = (y_true == 1).sum()\n",
    "        num_neg = (y_true == 0).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out, edge_probs = model(data, edge_logits)\n",
    "\n",
    "        if edge_logits is not None and data.candidate_edges.numel() > 0:\n",
    "            candidate_scores = edge_probs[data.edge_index.shape[1]:]\n",
    "\n",
    "            # ✅ Ziel: Source-Knotenlabel (nicht Zielknoten)\n",
    "            target_scores = y_true[data.candidate_edges[0]]\n",
    "\n",
    "            # pos_weight für HGT-Knoten (damit sie stärker gewichtet werden)\n",
    "            weight = torch.ones_like(target_scores)\n",
    "            weight[target_scores == 1] = num_neg / (num_pos + 1e-6)\n",
    "            \n",
    "            loss_edge = F.binary_cross_entropy(candidate_scores, target_scores, weight=weight)\n",
    "\n",
    "            # Optional: Node-based Stabilisierung\n",
    "            loss_node = F.binary_cross_entropy_with_logits(out, y_true)\n",
    "\n",
    "            # Sparsity-Term: Bestraft zu viele aktive Edges\n",
    "            loss_sparsity = 0.01 * candidate_scores.sum()\n",
    "\n",
    "            # Gesamtverlust\n",
    "            #loss = loss_edge + 0.1 * loss_node + loss_sparsity\n",
    "            loss = loss_edge\n",
    "        else:\n",
    "            loss = F.binary_cross_entropy_with_logits(out, y_true)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | Avg Loss: {epoch_loss / len(files):.4f}\")\n",
    "    \n",
    "# 5️⃣ Nach Training: Candidate-Edge-Wahrscheinlichkeiten pro Baum ausgeben\n",
    "for file in files:\n",
    "    edge_logits = global_edge_logits[file]\n",
    "    if edge_logits is not None:\n",
    "        learned_p = torch.sigmoid(edge_logits).detach().cpu().numpy()\n",
    "        print(f\"File {file} | Learned candidate edge probabilities (top 20): {learned_p[:20]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "04b573e8-ff24-49c8-82c4-24c2cf0cec94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 8], y=[9], nucleotide_sequences=[5], hot_encoded_nucleotide_sequences=[9, 5000], candidate_edges=[2, 18], file='/mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks/simulation_10.h5', G=DiGraph with 9 nodes and 8 edges, recipient_parent_nodes=[0], gene_absence_presence_matrix=[5])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "37171acc-f6ac-4c7e-9bb7-dec0274981c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "635"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(G.nucleotide_sequences[4])*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "594a7b5c-904b-4a6b-ae12-442c3968d399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(G.gene_absence_presence_matrix)\n",
    "sum(G.hot_encoded_nucleotide_sequences[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "22e3fc45-52e7-49e8-a5f0-02a55bd25f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/uhewm/Desktop/ProjectHGT/simulation_chunks/simulation_10.h5'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
