{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16caa069-096a-4169-8965-8b22ae102a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import advanced_simulator_h5\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sbi import utils\n",
    "from sbi.inference import SNPE\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import h5py\n",
    "import glob\n",
    "import numpy as np\n",
    "from advanced_simulator_h5 import reconstruct_distance_matrix\n",
    "from sbi.neural_nets import posterior_nn\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "output_dir = r\"C:\\Users\\uhewm\\Desktop\\SBI_Data\\simulation_chunks(1)\"\n",
    "\n",
    "#simulations = advanced_simulator_h5.load_simulation_results_h5(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8deda9f-88a9-4ee2-ad3b-8942eadca2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion: Flatten und concat Beobachtungen zu 2D Tensor [B, FeatureDim]\n",
    "def flatten_and_concat(gene_tensor, allele_tensor, distances_tensor, fitch_tensor):\n",
    "    B = gene_tensor.shape[0]\n",
    "    gene_flat = gene_tensor.view(B, -1)                    # [B, 1000]\n",
    "    allele_flat = allele_tensor.view(B, -1)                # [B, 1000,1000 = 1,000,000]\n",
    "    distances_flat = distances_tensor.view(B, -1)          # [B, 1,000,000]\n",
    "    fitch_flat = fitch_tensor.view(B, -1)                  # [B, 1]\n",
    "    combined = torch.cat([gene_flat, allele_flat, distances_flat, fitch_flat], dim=1)  # [B, 2,001,001]\n",
    "    return combined\n",
    "\n",
    "\n",
    "def unflatten_and_split(x: torch.Tensor, num_samples: int, multidimensional_scaling_dimensions: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Zerlegt einen flach kombinierten Tensor wieder in seine Einzelbestandteile:\n",
    "    - gene presence/absence matrix\n",
    "    - allele PCA matrix\n",
    "    - distance matrix\n",
    "    - fitch score\n",
    "\n",
    "    Args:\n",
    "        x: Tensor der Form (B, N), der alle Komponenten enthält.\n",
    "        alleles_shape: Tuple (A0, A1) — Form der Allele und Distance-Matrix.\n",
    "\n",
    "    Returns:\n",
    "        Tuple aus:\n",
    "        - gene_tensor: (B, 1, A1)\n",
    "        - allele_tensor: (B, 1, A0, A1)\n",
    "        - distance_tensor: (B, 1, A0, A1)\n",
    "        - fitch_tensor: (B, 1)\n",
    "    \"\"\"\n",
    "    B = x.shape[0]\n",
    "    G = num_samples\n",
    "    A0, A1 = num_samples, multidimensional_scaling_dimensions\n",
    "    D0, D1 = num_samples, multidimensional_scaling_dimensions\n",
    "\n",
    "    gene_len = G\n",
    "    allele_len = A0 * A1\n",
    "    distance_len = D0 * D1\n",
    "\n",
    "    gene_flat = x[:, :gene_len].view(B, 1, G)\n",
    "\n",
    "    allele_start = gene_len\n",
    "    allele_end = allele_start + allele_len\n",
    "    allele_flat = x[:, allele_start:allele_end].view(B, 1, A0, A1)\n",
    "\n",
    "    distance_start = allele_end\n",
    "    distance_end = distance_start + distance_len\n",
    "    distance_flat = x[:, distance_start:distance_end].view(B, 1, D0, D1)\n",
    "\n",
    "    fitch_flat = x[:, distance_end:].view(B, 1)\n",
    "\n",
    "    return gene_flat, allele_flat, distance_flat, fitch_flat\n",
    "    \n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_samples, multidimensional_scaling_dimensions):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_samples = num_samples\n",
    "        self.multidimensional_scaling_dimensions = multidimensional_scaling_dimensions\n",
    "\n",
    "        self.Softplus = nn.Softplus()\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=3, padding=0)\n",
    "        \n",
    "        dummy_input = torch.zeros(1, 1, num_samples)  # B=1, C=1, L=num_samples\n",
    "        conv1d_out_dim = self.conv1d(dummy_input).view(1, -1).shape[1]\n",
    "\n",
    "        self.pointwise_combine_allele_distance = nn.Conv2d(in_channels=2*3, out_channels=8, kernel_size=1)\n",
    "        \n",
    "        dummy_input = torch.zeros(1, 1, num_samples, num_samples)\n",
    "        out = self.min_max_avg_conv2d(dummy_input)\n",
    "        out_combined = torch.cat([out, out], dim=1)\n",
    "        conv2d_out_dim = self.pointwise_combine_allele_distance(out_combined).view(1, -1).shape[1]\n",
    "\n",
    "        total_input_dim = conv1d_out_dim + conv2d_out_dim + 1 \n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(total_input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 3),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        \n",
    "    def min_max_avg_conv2d(self, x, kernel_size=5, stride=3, padding=0):\n",
    "        \"\"\"\n",
    "        x: Tensor mit Shape [B, C, H, W]\n",
    "        Kernel: z.B. 3x3\n",
    "        Output: Tensor mit Shape [B, C*3, H_out, W_out], wobei für jeden Kanal\n",
    "                Minimum, Maximum, Average über das jeweilige Fenster berechnet werden.\n",
    "        \"\"\"\n",
    "    \n",
    "        B, C, H, W = x.shape\n",
    "    \n",
    "        # 1. Unfold: extrahiere alle Fenster als Spalten\n",
    "        # Ergebnis: [B, C * kernel_size*kernel_size, L], L = Anzahl Fenster = H_out * W_out\n",
    "        x_unfold = F.unfold(x, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        # Shape: [B, C * k*k, L]\n",
    "    \n",
    "        k = kernel_size * kernel_size\n",
    "        L = x_unfold.shape[-1]\n",
    "    \n",
    "        # 2. reshape zu [B, C, k*k, L]\n",
    "        x_unfold = x_unfold.view(B, C, k, L)\n",
    "    \n",
    "        # 3. min, max, avg entlang Dimension k (Fenstergröße)\n",
    "        min_vals = x_unfold.min(dim=2)[0]   # [B, C, L]\n",
    "        max_vals = x_unfold.max(dim=2)[0]   # [B, C, L]\n",
    "        avg_vals = x_unfold.mean(dim=2)     # [B, C, L]\n",
    "    \n",
    "        # 4. Concatenate diese 3 Werte pro Kanal\n",
    "        out = torch.cat([min_vals, max_vals, avg_vals], dim=1)  # [B, C*3, L]\n",
    "    \n",
    "        # 5. Fold zurück zu Bild: Größe (H_out, W_out)\n",
    "        H_out = (H + 2*padding - kernel_size)//stride + 1\n",
    "        W_out = (W + 2*padding - kernel_size)//stride + 1\n",
    "    \n",
    "        out = out.view(B, C*3, H_out, W_out)\n",
    "        return self.Softplus(out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: shape [B, total_features]\n",
    "        \"\"\"\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        gene_flat, allele_flat, distance_flat, fitch_flat = unflatten_and_split(x, self.num_samples, self.multidimensional_scaling_dimensions)\n",
    "\n",
    "        allele_flat = allele_flat.squeeze(1)     # [B, 100, 50]\n",
    "        distance_flat = distance_flat.squeeze(1) # [B, 100, 50]\n",
    "    \n",
    "        allele_processed_list = []\n",
    "        distance_processed_list = []\n",
    "    \n",
    "        for i in range(B):\n",
    "            allele_processed = reconstruct_distance_matrix(\n",
    "                allele_flat[i],  # shape [100, 50]\n",
    "                gene_absence_presence=gene_flat[i].squeeze()  # ggf. anpassen\n",
    "            )\n",
    "            distance_processed = reconstruct_distance_matrix(\n",
    "                distance_flat[i],  # shape [100, 50]\n",
    "                gene_absence_presence=None\n",
    "            )\n",
    "    \n",
    "            allele_processed_list.append(torch.tensor(allele_processed, dtype=torch.float32))\n",
    "            distance_processed_list.append(torch.tensor(distance_processed, dtype=torch.float32))\n",
    "    \n",
    "        # Stapeln zu Tensor zurück\n",
    "        allele_flat_processed = torch.stack(allele_processed_list)    # [B, H, W]\n",
    "        distance_flat_processed = torch.stack(distance_processed_list) # [B, H, W]\n",
    "\n",
    "        allele_flat_processed = allele_flat_processed.unsqueeze(1)    \n",
    "        distance_flat_processed = distance_flat_processed.unsqueeze(1)\n",
    "    \n",
    "        # Jetzt kannst du Conv2d Layer darauf anwenden\n",
    "        allele_flat_conv_out = self.min_max_avg_conv2d(allele_flat_processed)\n",
    "        distance_flat_conv_out = self.min_max_avg_conv2d(distance_flat_processed)\n",
    "    \n",
    "        gene_flat_conv_out = self.conv1d(gene_flat)\n",
    "\n",
    "        allele_distance_combined = torch.cat([allele_flat_conv_out, distance_flat_conv_out], dim=1)\n",
    "        \n",
    "        allele_distance_combined_conv_out = self.pointwise_combine_allele_distance(allele_distance_combined)\n",
    "        \n",
    "        # --- Kombination\n",
    "\n",
    "        parts = [gene_flat_conv_out, allele_distance_combined_conv_out, fitch_flat]\n",
    "\n",
    "        parts = [p.view(p.size(0), -1) for p in parts]\n",
    "        \n",
    "        x = torch.cat(parts, dim=-1)\n",
    "        \n",
    "        return self.fc(x)\n",
    "\n",
    "class H5SimulationDataset(Dataset):\n",
    "    def __init__(self, files_or_dir):\n",
    "        if isinstance(files_or_dir, str):  # ein Pfad\n",
    "            self.files = sorted(glob.glob(f\"{files_or_dir}/*.h5\"))\n",
    "        elif isinstance(files_or_dir, list):  # eine Liste von Dateipfaden\n",
    "            self.files = sorted(files_or_dir)\n",
    "        else:\n",
    "            raise ValueError(\"Expected str (directory) or list (files) for files_or_dir.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.files[idx]\n",
    "        with h5py.File(file, \"r\") as f:\n",
    "            grp = f[\"results\"]\n",
    "            hgt_rate = grp.attrs[\"hgt_rate\"]\n",
    "            rho = grp.attrs[\"rho\"]\n",
    "            gene_number_hgt_events_passed = grp.attrs[\"gene_number_hgt_events_passed\"]\n",
    "            fitch_score = grp.attrs[\"fitch_score\"]\n",
    "            \n",
    "            gene_absence_presence = grp[\"matrix\"][:]\n",
    "            distance_matrix = grp[\"distance_matrix\"][:]\n",
    "            #reconstructed_distance_matrix = reconstruct_distance_matrix(distance_matrix, gene_absence_presence=None)\n",
    "            reconstructed_distance_matrix = distance_matrix\n",
    "            \n",
    "            # PCA-Matrizen\n",
    "            alleles_list_pca = []\n",
    "            pca_keys = [key for key in grp.keys() if key.startswith(\"alleles_list_pca_\")]\n",
    "            pca_keys.sort()\n",
    "            for key in pca_keys:\n",
    "                alleles_pca_raw = grp[key][:]\n",
    "                idx = int(key.split(\"_\")[-1])\n",
    "                \"\"\"\n",
    "                alleles_pca_reconstructed = reconstruct_distance_matrix(\n",
    "                        alleles_pca_raw,\n",
    "                        gene_absence_presence = gene_absence_presence[idx]\n",
    "                    )\n",
    "                \"\"\"\n",
    "                alleles_pca_reconstructed = alleles_pca_raw\n",
    "                alleles_list_pca.append(alleles_pca_reconstructed)\n",
    "\n",
    "        # Als Torch-Tensoren\n",
    "        gene_absence_presence = torch.tensor(gene_absence_presence, dtype=torch.float32)\n",
    "        distance_matrix = torch.tensor(reconstructed_distance_matrix, dtype=torch.float32)\n",
    "        alleles_list_pca = torch.tensor(np.stack(alleles_list_pca), dtype=torch.float32)\n",
    "        fitch_score = torch.tensor(fitch_score, dtype=torch.float32)\n",
    "        gene_number_hgt_events_passed = float(gene_number_hgt_events_passed)\n",
    "\n",
    "        theta = torch.tensor([hgt_rate, rho, gene_number_hgt_events_passed], dtype=torch.float32)\n",
    "\n",
    "        return {\n",
    "            \"gene_absence_presence\": gene_absence_presence.float(),\n",
    "            \"alleles_list_pca\": alleles_list_pca.float(),\n",
    "            \"distance_matrix\": distance_matrix.float(),\n",
    "            \"fitch_score\": fitch_score.clone().detach().float(),\n",
    "            \"theta\": theta\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56bb0ec5-7c74-44d4-90f5-74a6989e1d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Von 27898 Dateien werden 13949 verwendet.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\uhewm\\AppData\\Local\\Temp\\ipykernel_9972\\2088379684.py:223: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  gene_number_hgt_events_passed = float(gene_number_hgt_events_passed)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 5635451796 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m theta_all \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(theta_buffer, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Simulationen anhängen\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m \u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend_simulations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta_all\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_all\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# === TRAINIEREN UND POSTERIOR ERSTELLEN ===\u001b[39;00m\n\u001b[0;32m     70\u001b[0m density_estimator \u001b[38;5;241m=\u001b[39m inference\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pangenome-hgt-sim\\Lib\\site-packages\\sbi\\inference\\trainers\\npe\\npe_base.py:169\u001b[0m, in \u001b[0;36mPosteriorEstimator.append_simulations\u001b[1;34m(self, theta, x, proposal, exclude_invalid_x, data_device)\u001b[0m\n\u001b[0;32m    166\u001b[0m theta \u001b[38;5;241m=\u001b[39m theta[is_valid_x]\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Check for problematic z-scoring\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m \u001b[43mwarn_if_zscoring_changes_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSNPE_C\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m current_round \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_non_atomic_loss\n\u001b[0;32m    174\u001b[0m ):\n\u001b[0;32m    175\u001b[0m     nle_nre_apt_msg_on_invalid_x(\n\u001b[0;32m    176\u001b[0m         num_nans,\n\u001b[0;32m    177\u001b[0m         num_infs,\n\u001b[0;32m    178\u001b[0m         exclude_invalid_x,\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiround SNPE-C (atomic)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    180\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pangenome-hgt-sim\\Lib\\site-packages\\sbi\\utils\\sbiutils.py:40\u001b[0m, in \u001b[0;36mwarn_if_zscoring_changes_data\u001b[1;34m(x, duplicate_tolerance)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Raise warning if z-scoring would create duplicate data points.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    x: Simulated data.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    duplicate_tolerance: Tolerated proportion of duplicates after z-scoring.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Count unique xs.\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m num_unique \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Check we do have different data in the batch\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_unique \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pangenome-hgt-sim\\Lib\\site-packages\\torch\\_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pangenome-hgt-sim\\Lib\\site-packages\\torch\\_jit_internal.py:624\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pangenome-hgt-sim\\Lib\\site-packages\\torch\\functional.py:1075\u001b[0m, in \u001b[0;36m_return_output\u001b[1;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m   1073\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unique_impl(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[1;32m-> 1075\u001b[0m output, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_unique_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pangenome-hgt-sim\\Lib\\site-packages\\torch\\functional.py:960\u001b[0m, in \u001b[0;36m_unique_impl\u001b[1;34m(input, sorted, return_inverse, return_counts, dim)\u001b[0m\n\u001b[0;32m    949\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    950\u001b[0m         unique,\n\u001b[0;32m    951\u001b[0m         (\u001b[38;5;28minput\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m         dim\u001b[38;5;241m=\u001b[39mdim,\n\u001b[0;32m    957\u001b[0m     )\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 960\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique_dim\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    968\u001b[0m     output, inverse_indices, counts \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_unique2(\n\u001b[0;32m    969\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    970\u001b[0m         \u001b[38;5;28msorted\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28msorted\u001b[39m,\n\u001b[0;32m    971\u001b[0m         return_inverse\u001b[38;5;241m=\u001b[39mreturn_inverse,\n\u001b[0;32m    972\u001b[0m         return_counts\u001b[38;5;241m=\u001b[39mreturn_counts,\n\u001b[0;32m    973\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 5635451796 bytes."
     ]
    }
   ],
   "source": [
    "# === Parameter ===\n",
    "output_dir = r\"C:\\Users\\uhewm\\Desktop\\SBI_Data\\simulation_chunks(1)\"\n",
    "\n",
    "# alle Dateien laden\n",
    "all_files = sorted(glob.glob(os.path.join(output_dir, \"*.h5\")))\n",
    "sampling_fraction = 0.5   # z.B. 10% der Daten\n",
    "n_total = len(all_files)\n",
    "n_files = max(1, int(n_total * sampling_fraction))\n",
    "sampled_files = random.sample(all_files, n_files)\n",
    "\n",
    "print(f\"Von {n_total} Dateien werden {n_files} verwendet.\")\n",
    "\n",
    "hgt_rate_max = 20 # Maximum hgt rate\n",
    "hgt_rate_min = 0 # Minimum hgt rate\n",
    "\n",
    "rho_max = 3 # Maximum gene loss rate\n",
    "rho_min = 0 # Minimum gene loss rate\n",
    "\n",
    "num_samples = 100\n",
    "multidimensional_scaling_dimensions = 50\n",
    "\n",
    "# Dein Prior\n",
    "prior = utils.BoxUniform(\n",
    "    low=torch.tensor([hgt_rate_min, rho_min, 0.0]),\n",
    "    high=torch.tensor([hgt_rate_max, rho_max, 1500.0])  # je nach Parameter\n",
    ")\n",
    "\n",
    "# === DEIN EMBEDDING ===\n",
    "embedding_net = NeuralNetwork(num_samples=num_samples, multidimensional_scaling_dimensions=multidimensional_scaling_dimensions)\n",
    "\n",
    "# === INITIALISIERE SNPE ===\n",
    "neural_posterior = posterior_nn(\n",
    "    model=\"maf\",\n",
    "    embedding_net=embedding_net,\n",
    "    hidden_features=64,\n",
    "    num_transforms=4\n",
    ")\n",
    "inference = SNPE(prior=prior, density_estimator=neural_posterior)\n",
    "\n",
    "dataset = H5SimulationDataset(sampled_files)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "x_buffer, theta_buffer = [], []\n",
    "\n",
    "for batch in loader:\n",
    "    gene_absence_presence = batch[\"gene_absence_presence\"]\n",
    "    alleles_list_pca = batch[\"alleles_list_pca\"]\n",
    "    distance_matrix = batch[\"distance_matrix\"]\n",
    "    fitch_score = batch[\"fitch_score\"]\n",
    "    theta = batch[\"theta\"]\n",
    "\n",
    "    x = flatten_and_concat(\n",
    "        gene_absence_presence,\n",
    "        alleles_list_pca,\n",
    "        distance_matrix,\n",
    "        fitch_score.unsqueeze(1)\n",
    "    )\n",
    "\n",
    "    x_buffer.append(x)\n",
    "    theta_buffer.append(theta)\n",
    "\n",
    "# Stapelweise zusammenführen\n",
    "x_all = torch.cat(x_buffer, dim=0)\n",
    "theta_all = torch.cat(theta_buffer, dim=0)\n",
    "\n",
    "# Simulationen anhängen\n",
    "inference.append_simulations(theta_all, x_all)\n",
    "\n",
    "# === TRAINIEREN UND POSTERIOR ERSTELLEN ===\n",
    "density_estimator = inference.train()\n",
    "posterior = inference.build_posterior(density_estimator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfccf721-59b5-4729-9bea-80db320f656c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ergebnisse sammeln\n",
    "true_hgt = []\n",
    "true_rho = []\n",
    "true_hgt_events = []\n",
    "\n",
    "pred_mean_hgt = []\n",
    "pred_mean_rho = []\n",
    "pred_mean_hgt_events = []\n",
    "\n",
    "abs_diff_hgt = []\n",
    "abs_diff_rho = []\n",
    "abs_diff_hgt_events = []\n",
    "\n",
    "gene_freq_colors = []\n",
    "\n",
    "# wenn du aus dem gelernten Posterior abfragen willst:\n",
    "# typischerweise in einem separaten Notebook/Skript\n",
    "# nach dem Training\n",
    "\n",
    "# 5000 Stichproben indices zufällig auswählen\n",
    "indices = random.sample(range(len(dataset)), k=min(5000, len(dataset)))\n",
    "\n",
    "for idx in indices:\n",
    "    # hole einen einzelnen Eintrag\n",
    "    item = dataset[idx]\n",
    "    \n",
    "    # flatten & concat\n",
    "    x_test = flatten_and_concat(\n",
    "        item[\"gene_absence_presence\"].unsqueeze(0),   # B=1\n",
    "        item[\"alleles_list_pca\"].unsqueeze(0),\n",
    "        item[\"distance_matrix\"].unsqueeze(0).unsqueeze(1),\n",
    "        item[\"fitch_score\"].view(1, 1)\n",
    "    )\n",
    "    \n",
    "    # ground truth\n",
    "    true_hgt.append(item[\"theta\"][0].item())\n",
    "    true_rho.append(item[\"theta\"][1].item())\n",
    "    true_hgt_events.append(item[\"theta\"][2].item())\n",
    "    \n",
    "    # sample aus posterior\n",
    "    samples = posterior.sample((20,), x=x_test)\n",
    "    \n",
    "    # mittelwert pro Parameter\n",
    "    mean_hgt = samples[:, 0].mean().item()\n",
    "    mean_rho = samples[:, 1].mean().item()\n",
    "    mean_hgt_events = samples[:, 2].mean().item()\n",
    "    \n",
    "    pred_mean_hgt.append(mean_hgt)\n",
    "    pred_mean_rho.append(mean_rho)\n",
    "    pred_mean_hgt_events.append(mean_hgt_events)\n",
    "    \n",
    "    abs_diff_hgt.append(abs(mean_hgt - true_hgt[-1]))\n",
    "    abs_diff_rho.append(abs(mean_rho - true_rho[-1]))\n",
    "    abs_diff_hgt_events.append(abs(mean_hgt_events - true_hgt_events[-1]))\n",
    "\n",
    "    gene_freq = item[\"gene_absence_presence\"].sum() / item[\"gene_absence_presence\"].numel()\n",
    "    gene_freq_colors.append(gene_freq.item())\n",
    "\n",
    "# Konvertiere Listen für sauberes Plotting\n",
    "true_hgt = torch.tensor(true_hgt)\n",
    "true_rho = torch.tensor(true_rho)\n",
    "true_hgt_events = torch.tensor(true_hgt_events)\n",
    "\n",
    "pred_mean_hgt = torch.tensor(pred_mean_hgt)\n",
    "pred_mean_rho = torch.tensor(pred_mean_rho)\n",
    "pred_mean_hgt_events = torch.tensor(pred_mean_hgt_events)\n",
    "\n",
    "colors = torch.tensor(gene_freq_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aae054-3f5a-44a9-9ac2-a5564eda49f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Plotten ===\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# 1. hgt_rate\n",
    "sc0 = axs[0].scatter(\n",
    "    true_hgt,\n",
    "    pred_mean_hgt,\n",
    "    c=colors,\n",
    "    cmap=\"RdYlGn\",\n",
    "    alpha=0.7,\n",
    "    s=4\n",
    ")\n",
    "axs[0].plot(\n",
    "    [hgt_rate_min, hgt_rate_max],\n",
    "    [hgt_rate_min, hgt_rate_max],\n",
    "    color=\"red\", linestyle=\"--\"\n",
    ")\n",
    "axs[0].set_xlabel(\"Real hgt_rate\")\n",
    "axs[0].set_ylabel(\"Predicted hgt_rate\")\n",
    "axs[0].set_title(\"Predicted vs Real hgt_rate\")\n",
    "cbar0 = fig.colorbar(sc0, ax=axs[0], orientation='vertical', pad=0.01)\n",
    "cbar0.set_label(\"Gene frequency\", fontsize=9)\n",
    "\n",
    "# 2. rho\n",
    "sc1 = axs[1].scatter(\n",
    "    true_rho,\n",
    "    pred_mean_rho,\n",
    "    c=colors,\n",
    "    cmap=\"RdYlGn\",\n",
    "    alpha=0.7,\n",
    "    s=4\n",
    ")\n",
    "axs[1].plot(\n",
    "    [rho_min, rho_max],\n",
    "    [rho_min, rho_max],\n",
    "    color=\"red\", linestyle=\"--\"\n",
    ")\n",
    "axs[1].set_xlabel(\"Real rho\")\n",
    "axs[1].set_ylabel(\"Predicted rho\")\n",
    "axs[1].set_title(\"Predicted vs Real rho\")\n",
    "cbar1 = fig.colorbar(sc1, ax=axs[1], orientation='vertical', pad=0.01)\n",
    "cbar1.set_label(\"Gene frequency\", fontsize=9)\n",
    "\n",
    "# 3. number_of_hgt_events\n",
    "sc2 = axs[2].scatter(\n",
    "    true_hgt_events,\n",
    "    pred_mean_hgt_events,\n",
    "    c=colors,\n",
    "    cmap=\"RdYlGn\",\n",
    "    alpha=0.7,\n",
    "    s=4\n",
    ")\n",
    "axs[2].plot(\n",
    "    [true_hgt_events.min(), true_hgt_events.max()],\n",
    "    [true_hgt_events.min(), true_hgt_events.max()],\n",
    "    color=\"red\", linestyle=\"--\"\n",
    ")\n",
    "axs[2].set_xlabel(\"Real number_of_hgt_events\")\n",
    "axs[2].set_ylabel(\"Predicted number_of_hgt_events\")\n",
    "axs[2].set_title(\"Predicted vs Real number_of_hgt_events\")\n",
    "cbar2 = fig.colorbar(sc2, ax=axs[2], orientation='vertical', pad=0.01)\n",
    "cbar2.set_label(\"Gene frequency\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "save_dir = r\"C:\\Users\\uhewm\\Desktop\\SBI_Data\\plots\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "filename = f\"posterior_plot_{timestamp}.png\"\n",
    "save_path = os.path.join(save_dir, filename)\n",
    "\n",
    "# Speichern\n",
    "plt.savefig(save_path, dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cad1ca-b408-4602-a660-196f532d206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Beobachtung vorbereiten\n",
    "first_x = flatten_and_concat(\n",
    "    dataset[0][\"gene_absence_presence\"],\n",
    "    dataset[0][\"alleles_list_pca\"],\n",
    "    dataset[0][\"distance_matrix\"],\n",
    "    dataset[0][\"fitch_score\"].unsqueeze(0)\n",
    ")\n",
    "\n",
    "# 2. Posterior auf diese Beobachtung setzen\n",
    "posterior.set_default_x(first_x)\n",
    "\n",
    "# 3. Sampling aus dem posterior\n",
    "samples = posterior.sample((1000,))\n",
    "\n",
    "print(samples.shape)  # (1000, dim_theta)\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "samples_np = samples.detach().cpu().numpy()\n",
    "\n",
    "df = pd.DataFrame(samples_np, columns=[\"theta_1\", \"theta_2\", \"theta_3\"])\n",
    "sns.pairplot(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5952f6d1-988e-441a-8793-01141e41a838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangenome-gene-transfer-simulation",
   "language": "python",
   "name": "pangenome-gene-transfer-simulation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
